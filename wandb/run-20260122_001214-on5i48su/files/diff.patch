diff --git a/configs/config.yaml b/configs/config.yaml
index 157b097..c4fcc72 100644
--- a/configs/config.yaml
+++ b/configs/config.yaml
@@ -31,7 +31,8 @@ logging:
 # Hydra configuration
 hydra:
   run:
-    dir: outputs/${experiment.name}/${now:%Y-%m-%d_%H-%M-%S}
+    # Output structure: outputs/{experiment}/{dataset}/{timestamp}/
+    dir: outputs/${experiment.name}/${data.dataset}/${now:%Y-%m-%d_%H-%M-%S}
   sweep:
-    dir: outputs/${experiment.name}/multirun/${now:%Y-%m-%d_%H-%M-%S}
+    dir: outputs/${experiment.name}/${data.dataset}/multirun/${now:%Y-%m-%d_%H-%M-%S}
     subdir: ${hydra.job.num}
diff --git a/configs/data/default.yaml b/configs/data/default.yaml
index c12357a..85e595c 100644
--- a/configs/data/default.yaml
+++ b/configs/data/default.yaml
@@ -1,5 +1,13 @@
 # Data configuration
 # EEG preprocessing and loading settings
+#
+# This is the default configuration. For dataset-specific configs, use:
+#   - data=greek_resting   (Greek HD-EEG: HC/MCI/AD)
+#   - data=meditation_bids (OpenNeuro ds001787: expert/novice)
+
+# Dataset type - determines file discovery and loading logic
+# Options: "greek_resting", "meditation_bids", or custom
+dataset: "greek_resting"
 
 # Data paths (relative to paths.data_dir)
 groups:
diff --git a/pyproject.toml b/pyproject.toml
index a0cb9f0..9ba7e65 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -125,4 +125,5 @@ ignore_missing_imports = true
 [tool.pytest.ini_options]
 testpaths = ["tests"]
 python_files = ["test_*.py"]
+pythonpath = ["src"]
 addopts = "-v --cov=eeg_biomarkers --cov-report=term-missing"
diff --git a/scripts/local_analysis/config.py b/scripts/local_analysis/config.py
index 279f11b..2157ea2 100644
--- a/scripts/local_analysis/config.py
+++ b/scripts/local_analysis/config.py
@@ -2,9 +2,21 @@
 Local Analysis Configuration
 
 Edit these paths to match your local setup.
+Supports multiple datasets through DatasetConfig system.
 """
 
 from pathlib import Path
+import sys
+
+# Add src to path for imports
+sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
+
+# =============================================================================
+# DATASET SELECTION - Choose which dataset to use
+# =============================================================================
+
+# Available datasets: "greek_resting", "meditation_bids"
+DATASET = "greek_resting"
 
 # =============================================================================
 # PATHS - Edit these to match your local setup
@@ -13,11 +25,16 @@ from pathlib import Path
 # Model checkpoint (from MatrixAutoEncoder repo or RunPod)
 CHECKPOINT_PATH = Path("/Users/luki/Documents/GitHub/eeg-state-biomarkers/models/best2.pt")
 
-# Data directories - point to your local .fif files
-# Full dataset on mounted NVMe drive
-DATA_DIR = Path("/Volumes/Nvme_Data/GreekData")
+# Data directories for different datasets
+DATA_PATHS = {
+    "greek_resting": Path("/Volumes/Nvme_Data/GreekData"),
+    "meditation_bids": Path("/Volumes/Nvme_Data/ds001787"),
+}
+
+# Legacy support: DATA_DIR for backwards compatibility
+DATA_DIR = DATA_PATHS.get(DATASET, DATA_PATHS["greek_resting"])
 
-# Subdirectory mapping for different conditions
+# Subdirectory mapping for different conditions (Greek dataset only)
 # AD -> AD-RAW/FILT, MCI -> MCI-RAW/FILT, HC/HID -> HC-RAW/FILT
 CONDITION_SUBDIRS = {
     "AD": "AD-RAW/FILT",
@@ -192,3 +209,71 @@ def get_subjects_by_group(fif_files: list) -> dict:
 def get_label_name(label: int) -> str:
     """Get human-readable name for label."""
     return {0: "HC", 1: "MCI", 2: "AD"}.get(label, "Unknown")
+
+
+# =============================================================================
+# DATASET CONFIG INTEGRATION
+# =============================================================================
+
+def get_dataset_config():
+    """
+    Get the DatasetConfig for the currently selected dataset.
+
+    Returns:
+        DatasetConfig instance for the current dataset
+    """
+    try:
+        from eeg_biomarkers.data.dataset_config import get_dataset_config as _get_config
+        return _get_config(DATASET)
+    except ImportError:
+        print("Warning: eeg_biomarkers not installed. Using legacy config.")
+        return None
+
+
+def get_data_files_via_config(groups=None):
+    """
+    Get data files using the DatasetConfig system.
+
+    This is the preferred method for multi-dataset support.
+
+    Args:
+        groups: List of group names to include (e.g., ["HC", "MCI"])
+                If None, includes all groups in the dataset config.
+
+    Returns:
+        List of (file_path, label, group_name) tuples
+    """
+    config = get_dataset_config()
+    if config is None:
+        # Fallback to legacy method
+        return get_fif_files(groups)
+
+    data_dir = DATA_PATHS.get(DATASET, DATA_DIR)
+
+    files = []
+    for group in config.groups:
+        if groups is not None and group.name not in groups:
+            continue
+
+        group_files = config.get_files_for_group(data_dir, group)
+        for file_path in group_files:
+            files.append((file_path, group.label, group.name))
+
+    return files
+
+
+def get_subject_id_via_config(file_path: Path) -> str:
+    """
+    Extract subject ID using the DatasetConfig system.
+
+    Args:
+        file_path: Path to data file
+
+    Returns:
+        Subject ID string
+    """
+    config = get_dataset_config()
+    if config is None:
+        return get_subject_id(file_path)
+
+    return config.get_subject_id(file_path)
diff --git a/src/eeg_biomarkers/data/__init__.py b/src/eeg_biomarkers/data/__init__.py
index 46ef841..67bfd2f 100644
--- a/src/eeg_biomarkers/data/__init__.py
+++ b/src/eeg_biomarkers/data/__init__.py
@@ -5,12 +5,37 @@ from eeg_biomarkers.data.preprocessing import (
     extract_phase_circular,
     chunk_signal,
     load_eeg_file,
+    preprocess_raw,
+)
+from eeg_biomarkers.data.dataset_config import (
+    DatasetConfig,
+    GroupConfig,
+    PreprocessingConfig,
+    GreekRestingConfig,
+    MeditationBIDSConfig,
+    get_dataset_config,
+    register_dataset,
+    list_datasets,
+    dataset_from_hydra_config,
 )
 
 __all__ = [
+    # Dataset classes
     "EEGDataset",
     "EEGDataModule",
+    # Preprocessing
     "extract_phase_circular",
     "chunk_signal",
     "load_eeg_file",
+    "preprocess_raw",
+    # Dataset configuration
+    "DatasetConfig",
+    "GroupConfig",
+    "PreprocessingConfig",
+    "GreekRestingConfig",
+    "MeditationBIDSConfig",
+    "get_dataset_config",
+    "register_dataset",
+    "list_datasets",
+    "dataset_from_hydra_config",
 ]
diff --git a/src/eeg_biomarkers/data/dataset.py b/src/eeg_biomarkers/data/dataset.py
index 94c377a..9ee67c9 100644
--- a/src/eeg_biomarkers/data/dataset.py
+++ b/src/eeg_biomarkers/data/dataset.py
@@ -15,16 +15,22 @@ from omegaconf import DictConfig
 from tqdm import tqdm
 
 from eeg_biomarkers.data.preprocessing import (
-    load_eeg_file,
+    preprocess_raw,
     prepare_phase_chunks,
 )
+from eeg_biomarkers.data.dataset_config import (
+    DatasetConfig,
+    dataset_from_hydra_config,
+)
 
 logger = logging.getLogger(__name__)
 
 
 def get_cache_key(cfg: DictConfig) -> str:
-    """Generate a unique cache key based on preprocessing config."""
+    """Generate a unique cache key based on dataset and preprocessing config."""
+    dataset_name = cfg.data.get("dataset", "greek_resting")
     key_parts = [
+        f"ds_{dataset_name}",
         f"filter_{cfg.data.preprocessing.filter_low}_{cfg.data.preprocessing.filter_high}",
         f"ref_{cfg.data.preprocessing.reference}",
         f"notch_{cfg.data.preprocessing.notch_freq}",
@@ -40,31 +46,129 @@ class CachedFileDataset(Dataset):
     Dataset that loads preprocessed chunks from a single cached .pt file.
 
     Each file is preprocessed once and saved as a tensor file.
-    Loading tensors is extremely fast compared to MNE preprocessing.
+    Uses lazy loading to avoid loading all data into memory at once.
     """
 
-    def __init__(self, cache_path: Path, label: int, subject_id: str):
+    def __init__(self, cache_path: Path, label: int, subject_id: str, lazy: bool = True):
         self.cache_path = cache_path
         self.label = label
         self.subject_id = subject_id
-
-        # Load cached data (fast - just tensor loading)
-        data = torch.load(cache_path, weights_only=True)
-        self.chunks = data["chunks"]  # (n_chunks, features, time)
-        self.masks = data["masks"]    # (n_chunks, time)
+        self.lazy = lazy
+
+        if lazy:
+            # Only load metadata - actual data loaded on demand
+            data = torch.load(cache_path, weights_only=True)
+            self._len = len(data["chunks"])
+            self._chunks = None
+            self._masks = None
+            del data  # Free memory immediately
+        else:
+            # Load cached data fully (original behavior)
+            data = torch.load(cache_path, weights_only=True)
+            self._chunks = data["chunks"]  # (n_chunks, features, time)
+            self._masks = data["masks"]    # (n_chunks, time)
+            self._len = len(self._chunks)
+
+    def _load_data(self) -> None:
+        """Lazily load data when first accessed."""
+        if self._chunks is None:
+            data = torch.load(self.cache_path, weights_only=True)
+            self._chunks = data["chunks"]
+            self._masks = data["masks"]
 
     def __len__(self) -> int:
-        return len(self.chunks)
+        return self._len
 
     def __getitem__(self, idx: int) -> dict:
+        if self.lazy and self._chunks is None:
+            self._load_data()
         return {
-            "data": self.chunks[idx],
-            "mask": self.masks[idx],
+            "data": self._chunks[idx],
+            "mask": self._masks[idx],
             "label": torch.tensor(self.label, dtype=torch.long),
             "subject_id": self.subject_id,
         }
 
 
+class MemoryEfficientConcatDataset(Dataset):
+    """
+    Memory-efficient dataset that loads files on-demand.
+
+    Unlike ConcatDataset which loads all sub-datasets into memory,
+    this keeps only an index and loads data from disk as needed.
+    Better for large datasets on memory-constrained systems (M1 Mac).
+
+    Trade-off: Slower iteration (disk I/O per batch) but much lower memory.
+    """
+
+    def __init__(self, file_infos: list[tuple[Path, int, str]]):
+        """
+        Args:
+            file_infos: List of (cache_path, label, subject_id) tuples
+        """
+        self.file_infos = file_infos
+
+        # Build index: (file_idx, chunk_idx) for each sample
+        self._index: list[tuple[int, int]] = []
+        self._file_lengths: list[int] = []
+
+        logger.info(f"Building index for {len(file_infos)} files...")
+        for file_idx, (cache_path, label, subject_id) in enumerate(file_infos):
+            try:
+                data = torch.load(cache_path, weights_only=True)
+                n_chunks = len(data["chunks"])
+                self._file_lengths.append(n_chunks)
+                for chunk_idx in range(n_chunks):
+                    self._index.append((file_idx, chunk_idx))
+                del data
+            except Exception as e:
+                logger.warning(f"Failed to index {cache_path}: {e}")
+                self._file_lengths.append(0)
+
+        logger.info(f"Indexed {len(self._index)} total chunks from {len(file_infos)} files")
+
+        # LRU cache for recently loaded files (keep a few in memory)
+        self._cache: dict[int, dict] = {}
+        self._cache_order: list[int] = []
+        self._max_cache_size = 3  # Keep last 3 files in memory
+
+    def _get_file_data(self, file_idx: int) -> dict:
+        """Load file data with simple LRU caching."""
+        if file_idx in self._cache:
+            return self._cache[file_idx]
+
+        # Load from disk
+        cache_path, _, _ = self.file_infos[file_idx]
+        data = torch.load(cache_path, weights_only=True)
+
+        # Add to cache
+        self._cache[file_idx] = data
+        self._cache_order.append(file_idx)
+
+        # Evict old entries if cache is full
+        while len(self._cache_order) > self._max_cache_size:
+            old_idx = self._cache_order.pop(0)
+            if old_idx in self._cache:
+                del self._cache[old_idx]
+
+        return data
+
+    def __len__(self) -> int:
+        return len(self._index)
+
+    def __getitem__(self, idx: int) -> dict:
+        file_idx, chunk_idx = self._index[idx]
+        cache_path, label, subject_id = self.file_infos[file_idx]
+
+        data = self._get_file_data(file_idx)
+        return {
+            "data": data["chunks"][chunk_idx],
+            "mask": data["masks"][chunk_idx],
+            "label": torch.tensor(label, dtype=torch.long),
+            "subject_id": subject_id,
+        }
+
+
 class EEGDataset(Dataset):
     """
     In-memory PyTorch Dataset for EEG phase chunks.
@@ -110,7 +214,7 @@ class EEGDataModule:
     Data module for loading and preparing EEG datasets.
 
     Handles:
-    - Loading data from multiple groups/conditions
+    - Loading data from multiple groups/conditions (via DatasetConfig)
     - Preprocessing and phase extraction with disk caching
     - Train/test splitting with proper subject-level separation
     - DataLoader creation
@@ -118,15 +222,27 @@ class EEGDataModule:
     The preprocessing cache ensures that expensive CSD/filtering operations
     are only done once per file. Subsequent runs load fast tensor files.
 
+    Supports multiple datasets through the DatasetConfig abstraction:
+    - Greek resting-state (HC/MCI/AD)
+    - OpenNeuro meditation BIDS (expert/novice)
+    - Custom datasets via register_dataset decorator
+
     Args:
         cfg: Hydra configuration
-        data_dir: Root directory containing group folders
+        data_dir: Root directory containing data
     """
 
     def __init__(self, cfg: DictConfig, data_dir: str | Path):
         self.cfg = cfg
         self.data_dir = Path(data_dir)
 
+        # Load dataset configuration based on cfg.data.dataset
+        self.dataset_config: DatasetConfig = dataset_from_hydra_config(cfg)
+        logger.info(f"Using dataset config: {self.dataset_config.name}")
+
+        # Validate dataset exists
+        self.dataset_config.validate(self.data_dir)
+
         # Cache directory for preprocessed data
         cache_dir = cfg.data.get("caching", {}).get("cache_dir", "preprocessed_cache")
         self.cache_dir = self.data_dir / cache_dir
@@ -156,21 +272,29 @@ class EEGDataModule:
 
     def _preprocess_and_cache_file(
         self,
-        fif_file: Path,
+        data_file: Path,
         cache_path: Path
     ) -> bool:
         """
         Preprocess a single file and save to cache.
 
+        Uses the dataset config to load the raw data (supporting FIF, BDF, etc.)
+        and then applies standard preprocessing pipeline.
+
         Returns True if successful, False otherwise.
         """
         try:
-            raw = load_eeg_file(
-                fif_file,
+            # Load raw data using dataset-specific loader
+            raw = self.dataset_config.load_raw(data_file)
+
+            # Apply preprocessing (filtering, referencing)
+            raw = preprocess_raw(
+                raw,
                 filter_low=self.cfg.data.preprocessing.filter_low,
                 filter_high=self.cfg.data.preprocessing.filter_high,
                 reference=self.cfg.data.preprocessing.reference,
                 notch_freq=self.cfg.data.preprocessing.notch_freq,
+                resample_freq=self.cfg.data.preprocessing.get("resample_freq"),
             )
 
             chunks, mask, info = prepare_phase_chunks(
@@ -196,7 +320,7 @@ class EEGDataModule:
             return True
 
         except Exception as e:
-            logger.warning(f"Failed to preprocess {fif_file.name}: {e}")
+            logger.warning(f"Failed to preprocess {data_file.name}: {e}")
             return False
 
     def _setup_cached_datasets(self) -> None:
@@ -204,43 +328,62 @@ class EEGDataModule:
         Set up datasets with disk caching.
 
         Strategy:
-        1. Discover all subjects and their files
+        1. Discover all subjects and their files (via DatasetConfig)
         2. Split subjects into train/val/test
         3. For each split, preprocess files (if not cached) and create dataset
         """
-        # Step 1: Discover all subjects and their files
+        # Step 1: Discover all subjects and their files using DatasetConfig
         subject_files: dict[str, list[Path]] = {}
         subject_labels: dict[str, int] = {}
 
-        for group_idx, group in enumerate(self.cfg.data.groups):
-            group_name = group.name
-            group_path = self.data_dir / group.path
-            self._label_map[group_name] = group_idx
+        # Get label map from dataset config
+        self._label_map = self.dataset_config.get_label_map()
 
-            if not group_path.exists():
-                logger.warning(f"Group path does not exist: {group_path}")
+        for group in self.dataset_config.groups:
+            # Use DatasetConfig to discover files for this group
+            group_files = self.dataset_config.get_files_for_group(self.data_dir, group)
+
+            if not group_files:
+                logger.warning(f"No files found for group {group.name}")
                 continue
 
-            subject_dirs = sorted([d for d in group_path.iterdir() if d.is_dir()])
-
-            # Sampling
-            n_subjects = self.cfg.data.sampling.n_subjects_per_group
-            if n_subjects is not None and n_subjects < len(subject_dirs):
-                rng = np.random.RandomState(self.cfg.data.sampling.random_seed)
-                indices = rng.permutation(len(subject_dirs))[:n_subjects]
-                subject_dirs = [subject_dirs[i] for i in sorted(indices)]
-            elif n_subjects is not None:
-                subject_dirs = subject_dirs[:n_subjects]
-
-            for subject_dir in subject_dirs:
-                subject_id = subject_dir.name
-                fif_files = [
-                    f for f in subject_dir.rglob("*_eeg.fif")
-                    if not f.name.startswith("._")
+            # Group files by subject using DatasetConfig's subject ID extraction
+            for data_file in group_files:
+                subject_id = self.dataset_config.get_subject_id(data_file)
+
+                if subject_id not in subject_files:
+                    subject_files[subject_id] = []
+                    subject_labels[subject_id] = group.label
+
+                subject_files[subject_id].append(data_file)
+
+            logger.info(f"Group {group.name}: {len(group_files)} files")
+
+        # Apply sampling if requested
+        n_subjects = self.cfg.data.sampling.n_subjects_per_group
+        if n_subjects is not None:
+            # Sample within each group
+            sampled_subject_files = {}
+            sampled_subject_labels = {}
+
+            for group in self.dataset_config.groups:
+                # Get subjects for this group
+                group_subjects = [
+                    s for s, label in subject_labels.items()
+                    if label == group.label
                 ]
-                if fif_files:
-                    subject_files[subject_id] = fif_files
-                    subject_labels[subject_id] = group_idx
+
+                if n_subjects < len(group_subjects):
+                    rng = np.random.RandomState(self.cfg.data.sampling.random_seed)
+                    indices = rng.permutation(len(group_subjects))[:n_subjects]
+                    group_subjects = [group_subjects[i] for i in sorted(indices)]
+
+                for subj in group_subjects:
+                    sampled_subject_files[subj] = subject_files[subj]
+                    sampled_subject_labels[subj] = subject_labels[subj]
+
+            subject_files = sampled_subject_files
+            subject_labels = sampled_subject_labels
 
         all_subjects = list(subject_files.keys())
         total_files = sum(len(files) for files in subject_files.values())
@@ -270,16 +413,26 @@ class EEGDataModule:
         )
 
         # Step 3: Preprocess and cache files, then create datasets
-        def get_cache_path(fif_file: Path) -> Path:
-            """Get cache path for a .fif file."""
+        def get_cache_path(data_file: Path) -> Path:
+            """Get cache path for a data file (FIF, BDF, etc.)."""
             # Use relative path from data_dir to create unique cache path
-            rel_path = fif_file.relative_to(self.data_dir)
+            try:
+                rel_path = data_file.relative_to(self.data_dir)
+            except ValueError:
+                # File is not under data_dir, use absolute path hash
+                rel_path = Path(hashlib.md5(str(data_file).encode()).hexdigest()[:16])
             cache_name = f"{rel_path.stem}_{self.cache_key}.pt"
             return self.cache_dir / rel_path.parent / cache_name
 
+        # Check if we should use memory-efficient loading (MPS/low memory)
+        use_memory_efficient = torch.backends.mps.is_available()
+        if use_memory_efficient:
+            logger.info("Using memory-efficient dataset (MPS detected)")
+
         def build_dataset(subjects: list[str], desc: str) -> Dataset:
             """Build dataset for a set of subjects, using cache."""
-            datasets = []
+            file_infos: list[tuple[Path, int, str]] = []  # For memory-efficient mode
+            datasets = []  # For standard mode
             n_cached = 0
             n_processed = 0
             n_failed = 0
@@ -287,12 +440,12 @@ class EEGDataModule:
             # Collect all files for these subjects
             all_files = []
             for subj in subjects:
-                for fif_file in subject_files[subj]:
-                    all_files.append((fif_file, subj, subject_labels[subj]))
+                for data_file in subject_files[subj]:
+                    all_files.append((data_file, subj, subject_labels[subj]))
 
             with tqdm(all_files, desc=f"Preparing {desc}", unit="file") as pbar:
-                for fif_file, subject_id, label in pbar:
-                    cache_path = get_cache_path(fif_file)
+                for data_file, subject_id, label in pbar:
+                    cache_path = get_cache_path(data_file)
                     pbar.set_postfix(subj=subject_id[:10])
 
                     # Check if cached
@@ -304,33 +457,41 @@ class EEGDataModule:
                                 cached_data = torch.load(cache_path, weights_only=False)
                                 if "info" in cached_data and "n_channels" in cached_data["info"]:
                                     self._n_channels = cached_data["info"]["n_channels"]
+                                del cached_data
                             except Exception:
                                 pass  # Will be set during preprocessing if needed
                     else:
                         # Preprocess and cache
-                        if self._preprocess_and_cache_file(fif_file, cache_path):
+                        if self._preprocess_and_cache_file(data_file, cache_path):
                             n_processed += 1
                         else:
                             n_failed += 1
                             continue
 
-                    # Create dataset from cached file
-                    try:
-                        ds = CachedFileDataset(cache_path, label, subject_id)
-                        datasets.append(ds)
-                    except Exception as e:
-                        logger.warning(f"Failed to load cache {cache_path}: {e}")
-                        n_failed += 1
+                    # Add to appropriate structure
+                    if use_memory_efficient:
+                        file_infos.append((cache_path, label, subject_id))
+                    else:
+                        try:
+                            ds = CachedFileDataset(cache_path, label, subject_id)
+                            datasets.append(ds)
+                        except Exception as e:
+                            logger.warning(f"Failed to load cache {cache_path}: {e}")
+                            n_failed += 1
 
             logger.info(
-                f"{desc}: {n_cached} cached, {n_processed} processed, "
-                f"{n_failed} failed, {sum(len(d) for d in datasets)} total chunks"
+                f"{desc}: {n_cached} cached, {n_processed} processed, {n_failed} failed"
             )
 
-            if not datasets:
-                raise RuntimeError(f"No data loaded for {desc}!")
-
-            return ConcatDataset(datasets)
+            if use_memory_efficient:
+                if not file_infos:
+                    raise RuntimeError(f"No data loaded for {desc}!")
+                return MemoryEfficientConcatDataset(file_infos)
+            else:
+                if not datasets:
+                    raise RuntimeError(f"No data loaded for {desc}!")
+                logger.info(f"{desc}: {sum(len(d) for d in datasets)} total chunks")
+                return ConcatDataset(datasets)
 
         self.train_dataset = build_dataset(train_subjects, "train")
         self.val_dataset = build_dataset(val_subjects, "val")
@@ -341,19 +502,46 @@ class EEGDataModule:
             f"{len(self.val_dataset)} val, {len(self.test_dataset)} test chunks"
         )
 
+    def _get_dataloader_kwargs(self) -> dict:
+        """Get optimal DataLoader kwargs based on device/platform."""
+        # Check if MPS (Apple Silicon) - pin_memory not supported
+        use_mps = torch.backends.mps.is_available()
+        use_cuda = torch.cuda.is_available()
+
+        if use_mps:
+            # MPS: No pin_memory, fewer workers to avoid memory duplication
+            return {
+                "num_workers": 0,  # Main process only - avoids memory copies
+                "pin_memory": False,
+                "persistent_workers": False,
+            }
+        elif use_cuda:
+            # CUDA: Full optimization
+            return {
+                "num_workers": 4,
+                "pin_memory": True,
+                "persistent_workers": True,
+            }
+        else:
+            # CPU: Moderate workers
+            return {
+                "num_workers": 2,
+                "pin_memory": False,
+                "persistent_workers": True,
+            }
+
     def train_dataloader(self, batch_size: int | None = None) -> DataLoader:
         """Get training DataLoader."""
         if self.train_dataset is None:
             raise RuntimeError("Call setup('fit') first")
 
         bs = batch_size or self.cfg.training.batch_size
+        kwargs = self._get_dataloader_kwargs()
         return DataLoader(
             self.train_dataset,
             batch_size=bs,
             shuffle=True,
-            num_workers=4,  # Can use workers now - loading cached tensors is fast
-            pin_memory=True,
-            persistent_workers=True,
+            **kwargs,
         )
 
     def val_dataloader(self, batch_size: int | None = None) -> DataLoader:
@@ -362,13 +550,12 @@ class EEGDataModule:
             raise RuntimeError("Call setup('fit') first")
 
         bs = batch_size or self.cfg.training.batch_size
+        kwargs = self._get_dataloader_kwargs()
         return DataLoader(
             self.val_dataset,
             batch_size=bs,
             shuffle=False,
-            num_workers=4,
-            pin_memory=True,
-            persistent_workers=True,
+            **kwargs,
         )
 
     def test_dataloader(self, batch_size: int | None = None) -> DataLoader:
@@ -377,13 +564,12 @@ class EEGDataModule:
             raise RuntimeError("Call setup('test') first")
 
         bs = batch_size or self.cfg.training.batch_size
+        kwargs = self._get_dataloader_kwargs()
         return DataLoader(
             self.test_dataset,
             batch_size=bs,
             shuffle=False,
-            num_workers=4,
-            pin_memory=True,
-            persistent_workers=True,
+            **kwargs,
         )
 
     @property
diff --git a/src/eeg_biomarkers/data/preprocessing.py b/src/eeg_biomarkers/data/preprocessing.py
index 364ec2e..e18f21f 100644
--- a/src/eeg_biomarkers/data/preprocessing.py
+++ b/src/eeg_biomarkers/data/preprocessing.py
@@ -10,30 +10,33 @@ import mne
 from scipy.signal import hilbert
 
 
-def load_eeg_file(
-    filepath: str | Path,
+def preprocess_raw(
+    raw: mne.io.Raw,
     filter_low: float = 3.0,
     filter_high: float = 48.0,
     reference: Literal["csd", "average"] = "csd",
     notch_freq: float | None = None,
+    resample_freq: float | None = None,
     verbose: bool = False,
 ) -> mne.io.Raw:
     """
-    Load and preprocess an EEG file.
+    Preprocess an already-loaded MNE Raw object.
 
     Args:
-        filepath: Path to .fif file
+        raw: MNE Raw object (already loaded with preload=True)
         filter_low: Low cutoff frequency (Hz)
         filter_high: High cutoff frequency (Hz)
         reference: Referencing method ("csd" or "average")
         notch_freq: Notch filter frequency for line noise (50/60 Hz), or None
+        resample_freq: Target sampling rate, or None to keep original
         verbose: Whether to print MNE output
 
     Returns:
         Preprocessed Raw object
     """
-    # Load raw data
-    raw = mne.io.read_raw_fif(filepath, preload=True, verbose=verbose)
+    # Resample if requested (do this first to speed up filtering)
+    if resample_freq is not None and raw.info["sfreq"] != resample_freq:
+        raw.resample(resample_freq, verbose=verbose)
 
     # Apply notch filter if specified
     if notch_freq is not None:
@@ -51,6 +54,44 @@ def load_eeg_file(
     return raw
 
 
+def load_eeg_file(
+    filepath: str | Path,
+    filter_low: float = 3.0,
+    filter_high: float = 48.0,
+    reference: Literal["csd", "average"] = "csd",
+    notch_freq: float | None = None,
+    verbose: bool = False,
+) -> mne.io.Raw:
+    """
+    Load and preprocess an EEG file (FIF format).
+
+    This is a convenience function for loading .fif files. For other formats
+    or more control, use preprocess_raw() directly.
+
+    Args:
+        filepath: Path to .fif file
+        filter_low: Low cutoff frequency (Hz)
+        filter_high: High cutoff frequency (Hz)
+        reference: Referencing method ("csd" or "average")
+        notch_freq: Notch filter frequency for line noise (50/60 Hz), or None
+        verbose: Whether to print MNE output
+
+    Returns:
+        Preprocessed Raw object
+    """
+    # Load raw data
+    raw = mne.io.read_raw_fif(filepath, preload=True, verbose=verbose)
+
+    return preprocess_raw(
+        raw,
+        filter_low=filter_low,
+        filter_high=filter_high,
+        reference=reference,
+        notch_freq=notch_freq,
+        verbose=verbose,
+    )
+
+
 def extract_phase_circular(
     signal: np.ndarray,
     include_amplitude: bool = False,
