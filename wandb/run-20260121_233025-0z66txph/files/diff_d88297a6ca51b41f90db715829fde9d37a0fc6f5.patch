diff --git a/configs/config.yaml b/configs/config.yaml
index 157b097..c4fcc72 100644
--- a/configs/config.yaml
+++ b/configs/config.yaml
@@ -31,7 +31,8 @@ logging:
 # Hydra configuration
 hydra:
   run:
-    dir: outputs/${experiment.name}/${now:%Y-%m-%d_%H-%M-%S}
+    # Output structure: outputs/{experiment}/{dataset}/{timestamp}/
+    dir: outputs/${experiment.name}/${data.dataset}/${now:%Y-%m-%d_%H-%M-%S}
   sweep:
-    dir: outputs/${experiment.name}/multirun/${now:%Y-%m-%d_%H-%M-%S}
+    dir: outputs/${experiment.name}/${data.dataset}/multirun/${now:%Y-%m-%d_%H-%M-%S}
     subdir: ${hydra.job.num}
diff --git a/configs/data/default.yaml b/configs/data/default.yaml
index c12357a..85e595c 100644
--- a/configs/data/default.yaml
+++ b/configs/data/default.yaml
@@ -1,5 +1,13 @@
 # Data configuration
 # EEG preprocessing and loading settings
+#
+# This is the default configuration. For dataset-specific configs, use:
+#   - data=greek_resting   (Greek HD-EEG: HC/MCI/AD)
+#   - data=meditation_bids (OpenNeuro ds001787: expert/novice)
+
+# Dataset type - determines file discovery and loading logic
+# Options: "greek_resting", "meditation_bids", or custom
+dataset: "greek_resting"
 
 # Data paths (relative to paths.data_dir)
 groups:
diff --git "a/pdfs/Ph D Eeg Mci Biomarkers \342\200\224 Systems Neuroscience Pivot Notes & Plan (jan 2026).pdf" "b/pdfs/Ph D Eeg Mci Biomarkers \342\200\224 Systems Neuroscience Pivot Notes & Plan (jan 2026).pdf"
new file mode 100644
index 0000000..657b8ad
Binary files /dev/null and "b/pdfs/Ph D Eeg Mci Biomarkers \342\200\224 Systems Neuroscience Pivot Notes & Plan (jan 2026).pdf" differ
diff --git a/pyproject.toml b/pyproject.toml
index a0cb9f0..9ba7e65 100644
--- a/pyproject.toml
+++ b/pyproject.toml
@@ -125,4 +125,5 @@ ignore_missing_imports = true
 [tool.pytest.ini_options]
 testpaths = ["tests"]
 python_files = ["test_*.py"]
+pythonpath = ["src"]
 addopts = "-v --cov=eeg_biomarkers --cov-report=term-missing"
diff --git a/reports/state_space_dynamics_progress.md b/reports/state_space_dynamics_progress.md
new file mode 100644
index 0000000..d98a009
--- /dev/null
+++ b/reports/state_space_dynamics_progress.md
@@ -0,0 +1,252 @@
+# State-Space Dynamics Analysis: Progress Report
+
+**Project:** EEG State Biomarkers for MCI Detection
+**Date:** January 2026
+**Status:** Local Analysis Complete, Awaiting Full Dataset Training
+
+---
+
+## 1. Executive Summary
+
+This report documents progress on analyzing EEG latent-space dynamics using a trained transformer autoencoder. We have developed and validated a multi-embedding analysis pipeline that:
+
+1. **Computes flow geometry metrics** (speed, occupancy entropy, tortuosity) across 5 embedding methods
+2. **Validates cross-embedding consistency** to ensure findings are not method artifacts
+3. **Compares HC vs MCI groups** with pooled embedding and per-class density aggregation
+
+Key preliminary finding: **Occupancy entropy shows consistent HC > MCI trend across embeddings**, suggesting MCI trajectories are more "trapped" in restricted regions of state space.
+
+---
+
+## 2. Single-Subject Trajectory Analysis
+
+### 2.1 Rabinovich-Style Flow Visualization
+
+The trajectory analysis reveals the flow structure in the learned latent space.
+
+**Flow Field (Quiver Plot):**
+![Trajectory Flow](../results/local_analysis/trajectory_flow.png)
+
+The flow field shows local displacement vectors, revealing "channels" and preferred flow directions in the latent space.
+
+**Density Heatmap:**
+![Trajectory Density](../results/local_analysis/trajectory_density.png)
+
+Density shows where the system spends most time. Concentrated regions suggest metastable states or attractors.
+
+**Speed-Colored Trajectory:**
+![Trajectory Speed](../results/local_analysis/trajectory_speed.png)
+
+Speed coloring identifies slow (metastable) vs fast (transition) regions. Low-speed regions (blue) indicate potential attractor-like states.
+
+**3D Trajectory:**
+![Trajectory 3D](../results/local_analysis/trajectory_3d.png)
+
+The 3D view reveals the overall trajectory structure and temporal evolution through the latent space.
+
+---
+
+## 3. Multi-Embedding Analysis
+
+### 3.1 Embedding Methods Compared
+
+| Method | Description | Key Parameters |
+|--------|-------------|----------------|
+| PCA | Standard linear projection | n_components=2 |
+| Time-lagged PCA | Captures temporal structure via lagged covariance | tau=5 samples |
+| Diffusion Maps | Nonlinear, respects manifold geometry | k=15 neighbors |
+| UMAP | Preserves local + global structure | n_neighbors=15, min_dist=0.1 |
+| Delay Embedding | Takens reconstruction of dynamics | tau=5, dim=3 |
+
+### 3.2 HC Subject Example (S017)
+
+![Multi-Embedding HC](../results/local_analysis/multi_embedding_S017.png)
+
+**Embedding Consistency (HC):**
+![Embedding Consistency HC](../results/local_analysis/embedding_consistency_S017.png)
+
+### 3.3 MCI Subject Example (i002)
+
+![Multi-Embedding MCI](../results/local_analysis/multi_embedding_i002.png)
+
+**Embedding Consistency (MCI):**
+![Embedding Consistency MCI](../results/local_analysis/embedding_consistency_i002.png)
+
+### 3.4 Cross-Embedding Consistency Interpretation
+
+High consistency (Spearman r > 0.7) across embedding methods suggests:
+- The underlying dynamics are captured reliably
+- Findings are not artifacts of a particular embedding choice
+- Flow geometry metrics reflect genuine dynamical structure
+
+---
+
+## 4. Group Comparison: HC vs MCI vs AD
+
+### 4.1 Pooled Embedding Density Maps
+
+The `--compare-all` mode fits each embedding on pooled data from all subjects, then computes per-class density aggregation.
+
+**PCA Density Comparison:**
+![PCA Density](../results/local_analysis/compare_all_pca_density.png)
+
+**Time-lagged PCA Density:**
+![tPCA Density](../results/local_analysis/compare_all_tpca_density.png)
+
+**Diffusion Maps Density:**
+![Diffusion Density](../results/local_analysis/compare_all_diffusion_density.png)
+
+**Delay Embedding Density:**
+![Delay Density](../results/local_analysis/compare_all_delay_density.png)
+
+**UMAP Density:**
+![UMAP Density](../results/local_analysis/compare_all_umap_density.png)
+
+### 4.2 Flow Metrics by Group
+
+![Flow Metrics Comparison](../results/local_analysis/compare_all_flow_metrics.png)
+
+### 4.3 Key Metric: Occupancy Entropy
+
+| Group | Occupancy Entropy (mean ± std) | Interpretation |
+|-------|-------------------------------|----------------|
+| HC | Higher | More uniform exploration of state space |
+| MCI | Lower | More "trapped" in restricted regions |
+| AD | Lowest | Most restricted dynamics |
+
+**Theoretical basis:** Reduced occupancy entropy in MCI/AD suggests:
+- Fewer accessible metastable states
+- More rigid, less flexible dynamics
+- Consistent with "supercritical" brain state hypothesis
+
+---
+
+## 5. RQA-Based Classification
+
+### 5.1 ROC Curve
+
+![ROC Curve](../results/local_analysis/roc_curve.png)
+
+### 5.2 Feature Importance
+
+![Feature Importance](../results/local_analysis/feature_importance.png)
+
+### 5.3 Confusion Matrix
+
+![Confusion Matrix](../results/local_analysis/confusion_matrix.png)
+
+---
+
+## 6. UMAP Visualizations
+
+### 6.1 3D UMAP of Mean Latents
+
+![UMAP Mean Latents](../results/local_analysis/umap_mean_latents_3d.png)
+
+### 6.2 3D UMAP Trajectories
+
+![UMAP Trajectories](../results/local_analysis/umap_trajectories_3d.png)
+
+### 6.3 Per-Subject Coloring
+
+![UMAP Per Subject](../results/local_analysis/umap_per_subject_3d.png)
+
+---
+
+## 7. RQA Metrics Comparison
+
+### 7.1 Violin Plots
+
+![RQA Violin](../results/local_analysis/rqa_violin_comparison.png)
+
+### 7.2 Group Comparison
+
+![RQA Group Comparison](../results/local_analysis/group_comparison_rr2.png)
+
+### 7.3 Expected Trends (Brain Criticality Framework)
+
+| Feature | Description | Expected in MCI |
+|---------|-------------|-----------------|
+| DET | Determinism (diagonal lines) | ↑ higher |
+| LAM | Laminarity (vertical lines) | ↑ higher |
+| TT | Trapping Time | ↑ higher |
+| L_mean | Mean diagonal length | ↑ higher |
+| ENTR | Entropy of diagonals | ↓ lower |
+| DIV | Divergence (1/L_max) | ↓ lower |
+
+---
+
+## 8. Recurrence Matrix Examples
+
+![Recurrence Matrices](../results/local_analysis/i007_20150115_1321_recurrence_matrices.png)
+
+---
+
+## 9. Next Steps
+
+### 9.1 Immediate (RunPod GPU)
+
+1. **Train on full dataset** (78 MCI + 31 HC) with modern architecture
+2. **Run integration experiment** with all 12 guardrails
+3. **Validate occupancy entropy** as primary discriminative metric
+
+### 9.2 Analysis Extensions
+
+1. **Bootstrap confidence intervals** for flow metrics
+2. **Permutation testing** for HC vs MCI differences
+3. **Cross-validation** of density-based features
+
+### 9.3 Theoretical Development
+
+1. **Quantify "criticality shift"** using RQA + flow metrics jointly
+2. **Relate occupancy entropy to attractor dimensionality**
+3. **Connect to Rabinovich's IFPS framework** for cognitive dynamics
+
+---
+
+## 10. Technical Notes
+
+### 10.1 Scripts Used
+
+All analysis scripts are in `scripts/local_analysis/`:
+
+```bash
+# Single-subject trajectory
+python scripts/local_analysis/plot_trajectory.py --subject S017
+
+# Multi-embedding comparison
+python scripts/local_analysis/multi_embedding.py --conditions HID MCI
+
+# Pooled group comparison
+python scripts/local_analysis/multi_embedding.py --compare-all --conditions HID MCI AD
+
+# RQA classification
+python scripts/local_analysis/classify_rqa.py --conditions HID MCI
+```
+
+### 10.2 Key Parameters
+
+| Parameter | Value | Rationale |
+|-----------|-------|-----------|
+| tau (time lag) | 5 samples | ~20ms at 250Hz, captures fast dynamics |
+| delay_dim | 3 | Minimal embedding dimension |
+| n_neighbors (UMAP) | 15 | Balance local/global structure |
+| RR target | 2% | Standard for EEG RQA |
+
+### 10.3 Reproducibility
+
+- UMAP random_state fixed to 42
+- All figures saved to `results/local_analysis/`
+- Model checkpoint: `models/best.pt`
+
+---
+
+## References
+
+1. Rabinovich, M.I. et al. (2008). Transient Cognitive Dynamics, Metastability, and Decision Making. *PLoS Computational Biology*.
+2. Webber, C.L. & Zbilut, J.P. (2005). Recurrence Quantification Analysis of Nonlinear Dynamical Systems. *Tutorials in Contemporary Nonlinear Methods*.
+3. Moon, K.R. et al. (2019). Visualizing Structure and Transitions in High-Dimensional Biological Data. *Nature Biotechnology*.
+
+---
+
+*Report generated from local analysis on M1 MacBook. Full dataset training pending on RunPod GPU.*
diff --git a/scripts/local_analysis/README.md b/scripts/local_analysis/README.md
new file mode 100644
index 0000000..57b0cc0
--- /dev/null
+++ b/scripts/local_analysis/README.md
@@ -0,0 +1,423 @@
+# Local Analysis Scripts
+
+Scripts for analyzing EEG autoencoder results on your local M1 MacBook.
+
+## Setup
+
+1. **Edit `config.py`** to point to your local paths:
+   ```python
+   CHECKPOINT_PATH = Path("/Users/luki/Documents/GitHub/MatrixAutoEncoder/models/best.pt")
+   DATA_DIR = Path("/Users/luki/Documents/GitHub/MatrixAutoEncoder/data")
+   ```
+
+2. **Install dependencies** (if not already installed):
+   ```bash
+   cd /Users/luki/Documents/GitHub/eeg-state-biomarkers
+   uv sync
+   ```
+
+3. **Activate environment**:
+   ```bash
+   source .venv/bin/activate
+   ```
+
+## Available Scripts
+
+### 1. Plot Recurrence Matrices
+```bash
+# Default: 2 subjects, chunk 0, Theiler=50
+python scripts/local_analysis/plot_recurrence.py
+
+# More subjects
+python scripts/local_analysis/plot_recurrence.py --n-subjects 5
+
+# Filter by condition (HID=healthy, MCI, AD)
+python scripts/local_analysis/plot_recurrence.py --conditions MCI
+python scripts/local_analysis/plot_recurrence.py --conditions HID MCI
+
+# Specific subject
+python scripts/local_analysis/plot_recurrence.py --subject S001
+
+# List available subjects first
+python scripts/local_analysis/plot_recurrence.py --list-subjects
+
+# List chunks for a subject
+python scripts/local_analysis/plot_recurrence.py --list-chunks S001
+
+# Select specific chunk (0-indexed)
+python scripts/local_analysis/plot_recurrence.py --chunk 3
+
+# Plot ALL chunks for a subject
+python scripts/local_analysis/plot_recurrence.py --subject S001 --chunk all
+
+# Disable Theiler window (shows diagonal structure)
+python scripts/local_analysis/plot_recurrence.py --no-theiler
+
+# Custom Theiler window (samples)
+python scripts/local_analysis/plot_recurrence.py --theiler 100
+
+# Don't show interactive plots (just save)
+python scripts/local_analysis/plot_recurrence.py --no-show
+```
+
+### 2. 3D UMAP Visualization
+```bash
+# Default: HC vs MCI comparison (no AD)
+python scripts/local_analysis/plot_umap_3d.py
+
+# Compare specific conditions
+python scripts/local_analysis/plot_umap_3d.py --conditions HID MCI      # HC vs MCI only
+python scripts/local_analysis/plot_umap_3d.py --conditions HID AD       # HC vs AD only
+python scripts/local_analysis/plot_umap_3d.py --conditions HID MCI AD   # All three groups
+
+# More subjects per group
+python scripts/local_analysis/plot_umap_3d.py --n-subjects 5
+
+# More chunks per subject
+python scripts/local_analysis/plot_umap_3d.py --n-chunks 10
+
+# Only mean latent UMAP
+python scripts/local_analysis/plot_umap_3d.py --mode mean
+
+# Only trajectory UMAP (all timepoints - slow!)
+python scripts/local_analysis/plot_umap_3d.py --mode trajectory
+
+# Only per-subject coloring
+python scripts/local_analysis/plot_umap_3d.py --mode subject
+
+# List available subjects
+python scripts/local_analysis/plot_umap_3d.py --list-subjects
+```
+
+### 3. Compare Groups (HC vs MCI vs AD)
+```bash
+# Default: HC vs MCI comparison (3 subjects per group)
+python scripts/local_analysis/compare_hc_mci.py
+
+# Compare specific conditions
+python scripts/local_analysis/compare_hc_mci.py --conditions HID MCI      # HC vs MCI only
+python scripts/local_analysis/compare_hc_mci.py --conditions HID AD       # HC vs AD only
+python scripts/local_analysis/compare_hc_mci.py --conditions HID MCI AD   # All three groups
+
+# More subjects
+python scripts/local_analysis/compare_hc_mci.py --n-subjects 5
+
+# Average RQA over multiple chunks
+python scripts/local_analysis/compare_hc_mci.py --n-chunks 3
+
+# Different RR target
+python scripts/local_analysis/compare_hc_mci.py --rr-target 0.05
+
+# List available subjects
+python scripts/local_analysis/compare_hc_mci.py --list-subjects
+```
+
+**Outputs:**
+- **Recurrence matrices** side-by-side for each group
+- **Violin plots** with all RQA features (DET, LAM, TT, L_mean, ENTR, DIV)
+- **Expected trends** note printed to console (based on brain criticality framework)
+
+**RQA Features:**
+| Feature | Description | Expected in MCI |
+|---------|-------------|-----------------|
+| DET | Determinism (diagonal lines) | ↑ higher |
+| LAM | Laminarity (vertical lines) | ↑ higher |
+| TT | Trapping Time | ↑ higher |
+| L_mean | Mean diagonal length | ↑ higher |
+| ENTR | Entropy of diagonals | ↓ lower |
+| DIV | Divergence (1/L_max) | ↓ lower |
+
+### 4. RQA-based Classification
+```bash
+# Default: HC vs MCI, 10 subjects per group, 3 chunks each
+python scripts/local_analysis/classify_rqa.py
+
+# Compare specific conditions
+python scripts/local_analysis/classify_rqa.py --conditions HID MCI      # HC vs MCI
+python scripts/local_analysis/classify_rqa.py --conditions HID AD       # HC vs AD
+python scripts/local_analysis/classify_rqa.py --conditions HID MCI AD   # HC vs all impaired
+
+# More subjects/chunks
+python scripts/local_analysis/classify_rqa.py --n-subjects 15
+python scripts/local_analysis/classify_rqa.py --n-chunks 5
+
+# Different RR target
+python scripts/local_analysis/classify_rqa.py --rr-target 0.05
+
+# Different test split
+python scripts/local_analysis/classify_rqa.py --test-size 0.3
+
+# Custom Theiler window
+python scripts/local_analysis/classify_rqa.py --theiler 100
+python scripts/local_analysis/classify_rqa.py --no-theiler
+
+# BASELINE: Use random weights (untrained model)
+python scripts/local_analysis/classify_rqa.py --random-weights
+
+# List available subjects
+python scripts/local_analysis/classify_rqa.py --list-subjects
+```
+
+**Outputs:**
+- **ROC curve** with AUC score (`roc_curve.png`)
+- **Feature importance** plot (`feature_importance.png`)
+- **Confusion matrix** (`confusion_matrix.png`)
+- Console output with segment-level and subject-level metrics
+
+**Classification approach:**
+- Binary classification: HC (label=0) vs Impaired (MCI/AD, label=1)
+- Train/test split by subject (no leakage)
+- Uses XGBoost (or RandomForest if XGBoost not available)
+- Reports both segment-level and subject-level AUC
+- `--random-weights` flag provides baseline: if trained model AUC >> random AUC, the learned representations are meaningful
+
+### 5. Rabinovich-style Trajectory Visualization
+```bash
+# Default: first HC subject, all visualizations
+python scripts/local_analysis/plot_trajectory.py
+
+# Specific subject
+python scripts/local_analysis/plot_trajectory.py --subject S001
+
+# More chunks for longer trajectory
+python scripts/local_analysis/plot_trajectory.py --n-chunks 10
+
+# Specific visualization mode
+python scripts/local_analysis/plot_trajectory.py --mode trajectory   # 2D/3D PCA
+python scripts/local_analysis/plot_trajectory.py --mode speed        # Speed + metastability
+python scripts/local_analysis/plot_trajectory.py --mode flow         # Flow field (quiver)
+python scripts/local_analysis/plot_trajectory.py --mode density      # Density heatmap
+
+# Compare trained vs random weights
+python scripts/local_analysis/plot_trajectory.py --compare-random
+
+# Use MCI subject
+python scripts/local_analysis/plot_trajectory.py --conditions MCI
+
+# List available subjects
+python scripts/local_analysis/plot_trajectory.py --list-subjects
+```
+
+**Outputs:**
+- **2D/3D trajectory** colored by time (`trajectory_2d.png`, `trajectory_3d.png`)
+- **Speed-colored trajectory** with metastable region markers (`trajectory_speed.png`)
+- **Density heatmap** showing where system spends time (`trajectory_density.png`)
+- **Flow field (quiver)** showing local displacement vectors (`trajectory_flow.png`)
+- **Trained vs random comparison** if `--compare-random` flag used
+
+**Trajectory statistics computed:**
+| Statistic | Description |
+|-----------|-------------|
+| Mean speed | Average velocity in latent space |
+| Speed CV | Coefficient of variation (consistency) |
+| Dwell episodes | Number of slow/metastable periods |
+| Tortuosity | Path complexity (path length / displacement) |
+| Explored variance | Volume of latent space explored |
+
+**Theoretical basis (Rabinovich IFPS):**
+- Trajectories reveal "channels" and preferred flow directions
+- Metastable regions (slow speed) indicate attractor-like states
+- MCI may show: fewer channels, longer dwell times, reduced flexibility
+- If trained >> random, learned representations capture meaningful dynamics
+
+### 6. Multi-Embedding Trajectory Analysis
+```bash
+# Default: HC vs MCI comparison with all embedding methods
+python scripts/local_analysis/multi_embedding.py
+
+# Specific subject only (no group comparison)
+python scripts/local_analysis/multi_embedding.py --subject S001
+
+# More subjects per group
+python scripts/local_analysis/multi_embedding.py --n-subjects 5
+
+# More chunks for longer trajectories
+python scripts/local_analysis/multi_embedding.py --n-chunks 10
+
+# Custom time lag for tPCA and delay embedding
+python scripts/local_analysis/multi_embedding.py --tau 10
+
+# Custom delay embedding dimension
+python scripts/local_analysis/multi_embedding.py --delay-dim 4
+
+# Compare trained vs random weights
+python scripts/local_analysis/multi_embedding.py --compare-random
+
+# Use MCI subjects
+python scripts/local_analysis/multi_embedding.py --conditions MCI
+
+# List available subjects
+python scripts/local_analysis/multi_embedding.py --list-subjects
+```
+
+**Outputs:**
+- **Multi-panel embedding comparison** showing all 5 methods side-by-side (`multi_embedding_comparison.png`)
+- **Flow metrics bar chart** comparing methods (`flow_metrics_comparison.png`)
+- **Cross-embedding consistency matrix** with Spearman correlations (`embedding_consistency.png`)
+- **Group comparison** (HC vs MCI) for each embedding method (`group_comparison.png`)
+- **Trained vs random comparison** if `--compare-random` flag used
+
+**Embedding methods:**
+| Method | Description | Key Parameters |
+|--------|-------------|----------------|
+| PCA | Standard linear projection | n_components=2 |
+| Time-lagged PCA | Captures temporal structure via lagged covariance | tau (lag samples) |
+| Diffusion Maps | Nonlinear, respects manifold geometry | k (neighbors), epsilon |
+| UMAP | Preserves local + global structure | n_neighbors, min_dist |
+| Delay Embedding | Takens reconstruction of dynamics | tau, dim |
+
+**Flow geometry metrics computed:**
+| Metric | Description | Expected in MCI |
+|--------|-------------|-----------------|
+| Mean speed | Average velocity in embedding | ↓ lower (more rigid) |
+| Speed CV | Coefficient of variation | ↓ lower (less variable) |
+| Dwell episodes | # of slow/metastable periods | ↑ higher |
+| Occupancy entropy | Uniformity of space exploration | ↓ lower |
+| Tortuosity | Path complexity (path/displacement) | ↓ lower |
+| Explored variance | Volume of space visited | ↓ lower |
+
+**Cross-embedding consistency:**
+- Computes pairwise Spearman correlation of flow metrics across methods
+- High consistency (>0.7) suggests robust geometric features
+- Low consistency may indicate method-specific artifacts
+- Theory: if embeddings agree, the underlying dynamics are captured reliably
+
+**Theoretical basis:**
+- Multiple embeddings provide complementary views of the same dynamics
+- Time-lagged PCA captures temporal correlations missed by standard PCA
+- Diffusion maps respect the intrinsic manifold geometry
+- Delay embedding (Takens) reconstructs attractor structure
+- Cross-method agreement validates that findings are not embedding artifacts
+
+### 7. Full-Dataset Statistical Analysis (Publication-Ready)
+```bash
+# Fast analysis (pca, tpca, delay) - default, recommended
+python scripts/local_analysis/full_dataset_analysis.py
+
+# Run without showing plots (saves automatically)
+python scripts/local_analysis/full_dataset_analysis.py --no-show
+
+# Quick test mode (100 bootstrap, 5 subjects per group)
+python scripts/local_analysis/full_dataset_analysis.py --quick --no-show
+
+# Include slow methods (diffusion maps, UMAP) - takes much longer
+python scripts/local_analysis/full_dataset_analysis.py --embedding all
+
+# Single embedding method
+python scripts/local_analysis/full_dataset_analysis.py --embedding pca
+
+# Custom bootstrap iterations
+python scripts/local_analysis/full_dataset_analysis.py --n-bootstrap 1000
+
+# More chunks per subject for longer trajectories
+python scripts/local_analysis/full_dataset_analysis.py --n-chunks 15
+```
+
+**Embedding options:**
+| Option | Methods | Speed |
+|--------|---------|-------|
+| `--embedding fast` (default) | PCA, tPCA, Delay | Fast |
+| `--embedding all` | PCA, tPCA, Delay, Diffusion, UMAP | Slow |
+| `--embedding pca` | PCA only | Fastest |
+
+**Purpose:**
+This script provides publication-ready statistical analysis for a systems neuroscience paper. It focuses on **quantifying robust reorganization of metastable brain dynamics** across groups, NOT classification or biomarkers.
+
+**Key features:**
+1. **Subject-level bootstrapping** (500+ iterations) for confidence intervals
+2. **Density difference maps with statistical masking** (95% CI excludes 0)
+3. **Radial density and speed profiles** with bootstrap CIs
+4. **Effect sizes (Cohen's d)** with bootstrap CIs
+5. **Cross-embedding robustness** quantification
+6. **Group flow fields (Rabinovich-style)** with density overlay
+7. **Flow field differences** with divergence analysis
+
+**Outputs:**
+- `bootstrap_metrics_*.png` - Flow metrics with 95% CIs per group
+- `density_diff_ci_*.png` - Density difference maps with significance masking
+- `radial_profiles_*.png` - Radial density/speed profiles with CIs
+- `group_flow_fields_*.png` - Rabinovich-style flow fields per group
+- `flow_difference_*.png` - Flow field differences (MCI-HC, AD-HC)
+- `effect_sizes.png` - Cohen's d effect sizes with interpretation
+- `cross_embedding_robustness.png` - Consistency heatmaps across embeddings
+- `full_analysis_results.json` - All results in machine-readable format
+- `summary_table.txt` - Human-readable summary table
+
+**Statistical approach:**
+| Analysis | Method | Output |
+|----------|--------|--------|
+| Flow metrics | Subject-level bootstrap (n=500) | Mean ± 95% CI |
+| Density differences | Per-pixel bootstrap | Masked difference maps |
+| Radial profiles | Subject-level bootstrap | Profile curves with CI bands |
+| Group flow fields | Aggregate displacement vectors | Quiver plots + magnitude maps |
+| Flow differences | Vector subtraction | Direction/magnitude/divergence maps |
+| Effect sizes | Bootstrap Cohen's d | d ± 95% CI with interpretation |
+| Cross-embedding | Spearman correlations | Mean off-diagonal ρ |
+
+**Effect size interpretation:**
+| |d| | Interpretation |
+|-----|----------------|
+| < 0.2 | Negligible |
+| 0.2 - 0.5 | Small |
+| 0.5 - 0.8 | Medium |
+| > 0.8 | Large |
+
+**Theoretical framing:**
+This analysis is about *dynamics*, not labels:
+- Disease labels are grouping variables, not prediction targets
+- The contribution is methodological + conceptual
+- Shows that entropy is preserved, flow geometry changes, metastability reorganizes
+- Effects are consistent across embeddings (embedding-invariant)
+- Effects persist under resampling (bootstrap-robust)
+
+## Output
+
+All plots are saved to:
+```
+/Users/luki/Documents/GitHub/eeg-state-biomarkers/results/local_analysis/
+```
+
+## File Structure
+
+```
+scripts/local_analysis/
+├── config.py                 # Edit paths here
+├── load_model.py             # Model loading utilities
+├── load_data.py              # Data loading and preprocessing
+├── plot_recurrence.py        # Recurrence matrix visualization
+├── plot_umap_3d.py           # 3D UMAP visualization
+├── plot_trajectory.py        # Rabinovich-style trajectory visualization
+├── compare_hc_mci.py         # Group comparison with violin plots
+├── classify_rqa.py           # RQA-based classification with ROC curves
+├── multi_embedding.py        # Multi-embedding trajectory analysis
+├── full_dataset_analysis.py  # Publication-ready statistical analysis
+└── README.md                 # This file
+```
+
+## Notes
+
+- Scripts use MPS (Metal Performance Shaders) by default for M1 Mac GPU acceleration
+- Change `DEVICE = "cpu"` in config.py if you have issues with MPS
+- The model auto-detects whether it's ConvLSTM or Transformer architecture
+- Phase data is extracted with circular representation (cos, sin, amplitude)
+
+## Data Organization
+
+The data directory has three clinical groups:
+
+```
+data/
+├── AD/     # Alzheimer's Disease (label=2)
+├── MCI/    # Mild Cognitive Impairment (label=1)
+└── HID/    # Healthy Controls (label=0)
+```
+
+**Labels:**
+- `HC` / `HID` = Healthy Controls (label=0) - **blue** in plots
+- `MCI` = Mild Cognitive Impairment (label=1) - **orange** in plots
+- `AD` = Alzheimer's Disease (label=2) - **red** in plots
+
+**Default behavior:**
+- By default, scripts compare HC vs MCI (no AD subjects)
+- Use `--conditions HID MCI AD` to include all three groups
+- Use `--conditions HID AD` to compare HC vs AD directly
diff --git a/scripts/local_analysis/classify_rqa.py b/scripts/local_analysis/classify_rqa.py
new file mode 100644
index 0000000..87b70a0
--- /dev/null
+++ b/scripts/local_analysis/classify_rqa.py
@@ -0,0 +1,619 @@
+#!/usr/bin/env python3
+"""
+Simple RQA-based Classification (No CV)
+
+Train/test split classification using RQA features from autoencoder latent space.
+Generates ROC curves and feature importance plots.
+
+Usage:
+    python classify_rqa.py                           # Default: HC vs MCI
+    python classify_rqa.py --conditions HID MCI      # HC vs MCI
+    python classify_rqa.py --conditions HID AD       # HC vs AD
+    python classify_rqa.py --conditions HID MCI AD   # All three (binary: HC vs impaired)
+    python classify_rqa.py --n-subjects 10           # Use 10 subjects per group
+    python classify_rqa.py --n-chunks 5              # Use 5 chunks per subject
+    python classify_rqa.py --rr-target 0.05          # 5% recurrence rate
+    python classify_rqa.py --test-size 0.3           # 30% test split
+    python classify_rqa.py --random-weights          # Use random weights (baseline)
+    python classify_rqa.py --list-subjects           # List available subjects
+"""
+
+import argparse
+import json
+import sys
+from datetime import datetime
+from pathlib import Path
+
+import numpy as np
+import matplotlib.pyplot as plt
+from sklearn.model_selection import train_test_split
+from sklearn.preprocessing import StandardScaler
+from sklearn.metrics import (
+    accuracy_score,
+    roc_auc_score,
+    roc_curve,
+    classification_report,
+    confusion_matrix,
+)
+
+# Add parent to path
+sys.path.insert(0, str(Path(__file__).parent))
+
+from config import (
+    CHECKPOINT_PATH, DATA_DIR, OUTPUT_DIR, DEVICE,
+    FILTER_LOW, FILTER_HIGH, CHUNK_DURATION, SFREQ,
+    THEILER_WINDOW, ensure_output_dir, get_fif_files,
+    get_subjects_by_group, get_label_name
+)
+from load_model import load_model_from_checkpoint, create_model, compute_latent_trajectory
+from load_data import load_and_preprocess_fif
+from plot_recurrence import compute_angular_distance_matrix, compute_recurrence_matrix, compute_rqa_stats
+
+# Check for XGBoost
+try:
+    import xgboost as xgb
+    HAS_XGBOOST = True
+except ImportError:
+    HAS_XGBOOST = False
+    from sklearn.ensemble import RandomForestClassifier
+
+# Color mapping
+GROUP_COLORS = {0: "blue", 1: "orange", 2: "red"}
+GROUP_NAMES = {0: "HC", 1: "MCI", 2: "AD"}
+
+
+def create_timestamped_output_dir(base_dir: Path, script_name: str) -> Path:
+    """Create a timestamped output directory for versioned results."""
+    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+    output_dir = base_dir / f"{script_name}_{timestamp}"
+    output_dir.mkdir(parents=True, exist_ok=True)
+    return output_dir
+
+
+def save_parameters(output_dir: Path, params: dict):
+    """Save parameters to a JSON file for reproducibility."""
+    params_path = output_dir / "parameters.json"
+
+    # Convert Path objects to strings
+    serializable_params = {}
+    for k, v in params.items():
+        if isinstance(v, Path):
+            serializable_params[k] = str(v)
+        else:
+            serializable_params[k] = v
+
+    serializable_params["timestamp"] = datetime.now().isoformat()
+
+    with open(params_path, 'w') as f:
+        json.dump(serializable_params, f, indent=2)
+    print(f"Parameters saved to: {params_path}")
+
+
+def extract_rqa_features(
+    model,
+    fif_path: Path,
+    model_info: dict,
+    rr_target: float = 0.02,
+    theiler: int = 50,
+    n_chunks: int = 1,
+) -> list[dict]:
+    """
+    Extract RQA features from all chunks of a subject.
+
+    Returns:
+        List of dicts with RQA features for each chunk
+    """
+    data = load_and_preprocess_fif(
+        fif_path, FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+        verbose=False,
+    )
+
+    available_chunks = len(data["chunks"])
+    if available_chunks == 0:
+        return []
+
+    chunks_to_use = list(range(min(n_chunks, available_chunks)))
+
+    features_list = []
+    for cidx in chunks_to_use:
+        phase_data = data["chunks"][cidx]
+        latent = compute_latent_trajectory(model, phase_data, DEVICE)
+        distance_matrix = compute_angular_distance_matrix(latent)
+        R, eps = compute_recurrence_matrix(distance_matrix, rr_target, theiler)
+        stats = compute_rqa_stats(R)
+        features_list.append(stats)
+
+    return features_list
+
+
+def collect_features_for_groups(
+    model,
+    model_info: dict,
+    groups: dict,
+    n_subjects_per_group: int,
+    n_chunks_per_subject: int,
+    rr_target: float,
+    theiler: int,
+) -> tuple[np.ndarray, np.ndarray, list[str], list[str]]:
+    """
+    Collect RQA features from all groups.
+
+    Returns:
+        features: (n_samples, n_features) array
+        labels: (n_samples,) array with 0=HC, 1=impaired
+        subject_ids: list of subject IDs
+        feature_names: list of feature names
+    """
+    all_features = []
+    all_labels = []
+    all_subject_ids = []
+    feature_names = None
+
+    for group_key, subjects in groups.items():
+        if not subjects:
+            continue
+
+        group_name = GROUP_NAMES.get(subjects[0][1], group_key.upper())
+        # Binary classification: HC (label=0) vs impaired (MCI/AD, label=1)
+        binary_label = 0 if subjects[0][1] == 0 else 1
+
+        print(f"\nExtracting features from {group_name} (targeting {n_subjects_per_group} subjects)...")
+
+        subjects_processed = 0
+        for fif_path, label, condition, subject_id in subjects:
+            if subjects_processed >= n_subjects_per_group:
+                break
+
+            print(f"  {subject_id} ({condition})...", end=" ")
+
+            features_list = extract_rqa_features(
+                model, fif_path, model_info,
+                rr_target=rr_target,
+                theiler=theiler,
+                n_chunks=n_chunks_per_subject,
+            )
+
+            if not features_list:
+                print("no chunks, skipping")
+                continue
+
+            print(f"{len(features_list)} chunks")
+
+            # Store feature names from first result
+            if feature_names is None:
+                feature_names = list(features_list[0].keys())
+
+            # Add features for each chunk
+            for feat_dict in features_list:
+                feat_vector = [feat_dict[k] for k in feature_names]
+                all_features.append(feat_vector)
+                all_labels.append(binary_label)
+                all_subject_ids.append(subject_id)
+
+            subjects_processed += 1
+
+        if subjects_processed < n_subjects_per_group:
+            print(f"  NOTE: Only found {subjects_processed} valid subjects")
+
+    return (
+        np.array(all_features),
+        np.array(all_labels),
+        all_subject_ids,
+        feature_names,
+    )
+
+
+def plot_roc_curve(
+    y_true: np.ndarray,
+    y_prob: np.ndarray,
+    output_dir: Path,
+    title_suffix: str = "",
+    show_plot: bool = True,
+):
+    """Plot ROC curve with AUC."""
+    fpr, tpr, thresholds = roc_curve(y_true, y_prob)
+    auc = roc_auc_score(y_true, y_prob)
+
+    fig, ax = plt.subplots(figsize=(8, 8))
+
+    ax.plot(fpr, tpr, 'b-', linewidth=2, label=f'ROC (AUC = {auc:.3f})')
+    ax.plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random (AUC = 0.5)')
+
+    ax.set_xlabel('False Positive Rate', fontsize=12)
+    ax.set_ylabel('True Positive Rate', fontsize=12)
+    ax.set_title(f'ROC Curve: HC vs Impaired{title_suffix}', fontsize=14, fontweight='bold')
+    ax.legend(loc='lower right', fontsize=11)
+    ax.grid(True, alpha=0.3)
+    ax.set_xlim([0, 1])
+    ax.set_ylim([0, 1])
+
+    # Add AUC annotation
+    ax.annotate(f'AUC = {auc:.3f}', xy=(0.6, 0.2), fontsize=16, fontweight='bold',
+                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
+
+    plt.tight_layout()
+
+    save_path = output_dir / "roc_curve.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return auc
+
+
+def plot_feature_importance(
+    feature_names: list[str],
+    importances: np.ndarray,
+    output_dir: Path,
+    show_plot: bool = True,
+):
+    """Plot feature importance from classifier."""
+    # Sort by importance
+    indices = np.argsort(importances)[::-1]
+
+    fig, ax = plt.subplots(figsize=(10, 6))
+
+    colors = plt.cm.Blues(np.linspace(0.4, 0.9, len(feature_names)))
+    colors = colors[indices][::-1]  # Reverse so most important is darkest
+
+    y_pos = np.arange(len(feature_names))
+    ax.barh(y_pos, importances[indices][::-1], color=colors)
+    ax.set_yticks(y_pos)
+    ax.set_yticklabels([feature_names[i] for i in indices[::-1]])
+    ax.set_xlabel('Importance', fontsize=12)
+    ax.set_title('RQA Feature Importance', fontsize=14, fontweight='bold')
+    ax.grid(True, alpha=0.3, axis='x')
+
+    plt.tight_layout()
+
+    save_path = output_dir / "feature_importance.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def plot_confusion_matrix(
+    y_true: np.ndarray,
+    y_pred: np.ndarray,
+    output_dir: Path,
+    show_plot: bool = True,
+):
+    """Plot confusion matrix."""
+    cm = confusion_matrix(y_true, y_pred)
+
+    fig, ax = plt.subplots(figsize=(6, 6))
+
+    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)
+    ax.figure.colorbar(im, ax=ax)
+
+    classes = ['HC', 'Impaired']
+    ax.set(
+        xticks=np.arange(cm.shape[1]),
+        yticks=np.arange(cm.shape[0]),
+        xticklabels=classes, yticklabels=classes,
+        ylabel='True label',
+        xlabel='Predicted label',
+    )
+    ax.set_title('Confusion Matrix', fontsize=14, fontweight='bold')
+
+    # Add text annotations
+    thresh = cm.max() / 2.
+    for i in range(cm.shape[0]):
+        for j in range(cm.shape[1]):
+            ax.text(j, i, format(cm[i, j], 'd'),
+                    ha="center", va="center",
+                    color="white" if cm[i, j] > thresh else "black",
+                    fontsize=16, fontweight='bold')
+
+    plt.tight_layout()
+
+    save_path = output_dir / "confusion_matrix.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def list_subjects(groups: dict):
+    """List available subjects by group."""
+    print("\n" + "=" * 60)
+    print("AVAILABLE SUBJECTS")
+    print("=" * 60)
+
+    for group_key in ["hc", "mci", "ad"]:
+        subjects = groups.get(group_key, [])
+        if not subjects:
+            continue
+
+        group_name = GROUP_NAMES.get(subjects[0][1], group_key.upper())
+        print(f"\n{group_name} subjects ({len(subjects)}):")
+        for i, (fif_path, label, condition, subject_id) in enumerate(subjects):
+            print(f"  {i+1:3d}. {subject_id:10s} ({condition})")
+
+    total = sum(len(groups.get(k, [])) for k in ["hc", "mci", "ad"])
+    print(f"\nTotal: {total} unique subjects")
+
+
+def main():
+    parser = argparse.ArgumentParser(
+        description="Simple RQA-based classification (no CV)",
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        epilog="""
+Examples:
+  python classify_rqa.py                           # Default: HC vs MCI
+  python classify_rqa.py --conditions HID MCI      # HC vs MCI
+  python classify_rqa.py --conditions HID AD       # HC vs AD
+  python classify_rqa.py --conditions HID MCI AD   # HC vs all impaired
+  python classify_rqa.py --n-subjects 10           # 10 subjects per group
+  python classify_rqa.py --n-chunks 5              # 5 chunks per subject
+  python classify_rqa.py --rr-target 0.05          # 5% recurrence rate
+  python classify_rqa.py --test-size 0.3           # 30% test split
+  python classify_rqa.py --list-subjects           # List available subjects
+        """
+    )
+    parser.add_argument("--n-subjects", type=int, default=10,
+                        help="Number of subjects per group (default: 10)")
+    parser.add_argument("--n-chunks", type=int, default=3,
+                        help="Number of chunks per subject (default: 3)")
+    parser.add_argument("--conditions", type=str, nargs="+", default=["HID", "MCI"],
+                        help="Conditions to compare: HID, MCI, AD (default: HID MCI)")
+    parser.add_argument("--rr-target", type=float, default=0.02,
+                        help="Target recurrence rate (default: 0.02 = 2%%)")
+    parser.add_argument("--theiler", type=int, default=THEILER_WINDOW,
+                        help=f"Theiler window in samples (default: {THEILER_WINDOW})")
+    parser.add_argument("--no-theiler", action="store_true",
+                        help="Disable Theiler window")
+    parser.add_argument("--test-size", type=float, default=0.25,
+                        help="Test set proportion (default: 0.25)")
+    parser.add_argument("--seed", type=int, default=42,
+                        help="Random seed (default: 42)")
+    parser.add_argument("--random-weights", action="store_true",
+                        help="Use random weights instead of trained model (baseline)")
+    parser.add_argument("--list-subjects", action="store_true",
+                        help="List available subjects and exit")
+    parser.add_argument("--no-show", action="store_true",
+                        help="Don't display plots interactively")
+    parser.add_argument("--output-dir", type=str, default=None,
+                        help="Custom output directory (default: timestamped folder)")
+    args = parser.parse_args()
+
+    # Handle Theiler window
+    theiler = 0 if args.no_theiler else args.theiler
+
+    # Create timestamped output directory
+    base_output_dir = ensure_output_dir()
+    if args.output_dir:
+        output_dir = Path(args.output_dir)
+        output_dir.mkdir(parents=True, exist_ok=True)
+    else:
+        output_dir = create_timestamped_output_dir(base_output_dir, "classify_rqa")
+
+    # Get subjects by group for selected conditions
+    fif_files = get_fif_files(args.conditions)
+    groups = get_subjects_by_group(fif_files)
+
+    # Print summary
+    group_counts = []
+    for key in ["hc", "mci", "ad"]:
+        if groups.get(key):
+            group_counts.append(f"{len(groups[key])} {key.upper()}")
+    print(f"Found {', '.join(group_counts)} subjects for conditions: {args.conditions}")
+
+    # Handle --list-subjects
+    if args.list_subjects:
+        list_subjects(groups)
+        return 0
+
+    # Check we have at least two groups
+    active_groups = {k: v for k, v in groups.items() if v}
+    if len(active_groups) < 2:
+        print("Need at least 2 groups with subjects for classification!")
+        print("Available groups:", list(active_groups.keys()))
+        return 1
+
+    # Check for HC group (required for binary classification)
+    if "hc" not in active_groups:
+        print("ERROR: Need HC/HID group for binary classification!")
+        return 1
+
+    # Save parameters
+    params = {
+        "n_subjects": args.n_subjects,
+        "n_chunks": args.n_chunks,
+        "conditions": args.conditions,
+        "rr_target": args.rr_target,
+        "theiler": theiler,
+        "test_size": args.test_size,
+        "seed": args.seed,
+        "random_weights": args.random_weights,
+        "checkpoint_path": str(CHECKPOINT_PATH),
+        "filter_low": FILTER_LOW,
+        "filter_high": FILTER_HIGH,
+        "chunk_duration": CHUNK_DURATION,
+        "sfreq": SFREQ,
+        "groups_found": {k: len(v) for k, v in active_groups.items()},
+        "classifier": "XGBoost" if HAS_XGBOOST else "RandomForest",
+    }
+    save_parameters(output_dir, params)
+
+    # Load model
+    print("\nLoading model...")
+    model_info = load_model_from_checkpoint(CHECKPOINT_PATH, DEVICE)
+
+    # Create model
+    first_subject = list(active_groups.values())[0][0]
+    first_data = load_and_preprocess_fif(
+        first_subject[0], FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+        verbose=False,
+    )
+
+    # Use random weights if requested (baseline comparison)
+    load_weights = not args.random_weights
+    model = create_model(first_data["n_channels"], model_info, DEVICE, load_weights=load_weights)
+
+    if args.random_weights:
+        print("\n*** BASELINE MODE: Using RANDOM weights (untrained model) ***")
+
+    # Collect features
+    print(f"\nSettings: RR={args.rr_target*100:.0f}%, Theiler={theiler}")
+    features, labels, subject_ids, feature_names = collect_features_for_groups(
+        model, model_info, active_groups,
+        n_subjects_per_group=args.n_subjects,
+        n_chunks_per_subject=args.n_chunks,
+        rr_target=args.rr_target,
+        theiler=theiler,
+    )
+
+    print(f"\nCollected {len(features)} samples from {len(set(subject_ids))} subjects")
+    print(f"Features: {feature_names}")
+    print(f"Class distribution: HC={sum(labels==0)}, Impaired={sum(labels==1)}")
+
+    if len(features) < 10:
+        print("ERROR: Not enough samples for classification!")
+        return 1
+
+    # Train/test split (stratified by subject to avoid leakage)
+    # Group samples by subject first
+    unique_subjects = list(set(subject_ids))
+    subject_labels = {s: labels[subject_ids.index(s)] for s in unique_subjects}
+
+    # Split subjects
+    subjects_0 = [s for s in unique_subjects if subject_labels[s] == 0]
+    subjects_1 = [s for s in unique_subjects if subject_labels[s] == 1]
+
+    n_test_0 = max(1, int(len(subjects_0) * args.test_size))
+    n_test_1 = max(1, int(len(subjects_1) * args.test_size))
+
+    np.random.seed(args.seed)
+    test_subjects_0 = list(np.random.choice(subjects_0, n_test_0, replace=False))
+    test_subjects_1 = list(np.random.choice(subjects_1, n_test_1, replace=False))
+    test_subjects = set(test_subjects_0 + test_subjects_1)
+
+    # Create train/test masks
+    test_mask = np.array([s in test_subjects for s in subject_ids])
+    train_mask = ~test_mask
+
+    X_train, X_test = features[train_mask], features[test_mask]
+    y_train, y_test = labels[train_mask], labels[test_mask]
+
+    print(f"\nTrain: {len(X_train)} samples ({sum(y_train==0)} HC, {sum(y_train==1)} Impaired)")
+    print(f"Test:  {len(X_test)} samples ({sum(y_test==0)} HC, {sum(y_test==1)} Impaired)")
+    print(f"Test subjects: {sorted(test_subjects)}")
+
+    # Scale features
+    scaler = StandardScaler()
+    X_train_scaled = scaler.fit_transform(X_train)
+    X_test_scaled = scaler.transform(X_test)
+
+    # Train classifier
+    print("\nTraining classifier...")
+    if HAS_XGBOOST:
+        clf = xgb.XGBClassifier(
+            n_estimators=100,
+            max_depth=4,
+            learning_rate=0.1,
+            random_state=args.seed,
+            use_label_encoder=False,
+            eval_metric="logloss",
+        )
+    else:
+        print("  (XGBoost not available, using RandomForest)")
+        clf = RandomForestClassifier(
+            n_estimators=100,
+            max_depth=4,
+            random_state=args.seed,
+        )
+
+    clf.fit(X_train_scaled, y_train)
+
+    # Predict
+    y_pred = clf.predict(X_test_scaled)
+    y_prob = clf.predict_proba(X_test_scaled)[:, 1]
+
+    # Compute metrics
+    accuracy = accuracy_score(y_test, y_pred)
+    auc = roc_auc_score(y_test, y_prob)
+
+    print("\n" + "=" * 60)
+    mode_str = " [RANDOM WEIGHTS BASELINE]" if args.random_weights else ""
+    print(f"RESULTS (Segment-level){mode_str}")
+    print("=" * 60)
+    print(f"Accuracy: {accuracy:.3f}")
+    print(f"AUC:      {auc:.3f}")
+    print(f"\nClassification Report:")
+    print(classification_report(y_test, y_pred, target_names=["HC", "Impaired"]))
+
+    # Subject-level aggregation
+    print("\n" + "=" * 60)
+    print("RESULTS (Subject-level)")
+    print("=" * 60)
+
+    test_subject_ids = [subject_ids[i] for i in range(len(subject_ids)) if test_mask[i]]
+
+    subj_probs = {}
+    subj_labels = {}
+    for i, s in enumerate(test_subject_ids):
+        if s not in subj_probs:
+            subj_probs[s] = []
+            subj_labels[s] = y_test[i]
+        subj_probs[s].append(y_prob[i])
+
+    # Average probabilities per subject
+    subj_mean_prob = np.array([np.mean(subj_probs[s]) for s in subj_probs])
+    subj_true = np.array([subj_labels[s] for s in subj_probs])
+    subj_pred = (subj_mean_prob > 0.5).astype(int)
+
+    subj_accuracy = accuracy_score(subj_true, subj_pred)
+    try:
+        subj_auc = roc_auc_score(subj_true, subj_mean_prob)
+    except ValueError:
+        subj_auc = np.nan
+        print("  (Cannot compute AUC - single class in test set)")
+
+    print(f"Subjects in test: {len(subj_probs)}")
+    print(f"Accuracy: {subj_accuracy:.3f}")
+    print(f"AUC:      {subj_auc:.3f}")
+
+    for s in sorted(subj_probs.keys()):
+        true_label = "HC" if subj_labels[s] == 0 else "Impaired"
+        pred_label = "HC" if np.mean(subj_probs[s]) < 0.5 else "Impaired"
+        correct = "✓" if true_label == pred_label else "✗"
+        print(f"  {s}: true={true_label:8s}, pred={pred_label:8s}, prob={np.mean(subj_probs[s]):.3f} {correct}")
+
+    # Generate plots
+    print("\nGenerating plots...")
+
+    # ROC curve
+    model_type = "RANDOM WEIGHTS" if args.random_weights else "Trained"
+    title_suffix = f"\nRR={args.rr_target*100:.0f}%, Theiler={theiler}, Model={model_type}"
+    plot_roc_curve(y_test, y_prob, output_dir, title_suffix, not args.no_show)
+
+    # Feature importance
+    if HAS_XGBOOST:
+        importances = clf.feature_importances_
+    else:
+        importances = clf.feature_importances_
+    plot_feature_importance(feature_names, importances, output_dir, not args.no_show)
+
+    # Confusion matrix
+    plot_confusion_matrix(y_test, y_pred, output_dir, not args.no_show)
+
+    print(f"\nAll plots saved to: {output_dir}")
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/scripts/local_analysis/compare_hc_mci.py b/scripts/local_analysis/compare_hc_mci.py
new file mode 100644
index 0000000..e6542f6
--- /dev/null
+++ b/scripts/local_analysis/compare_hc_mci.py
@@ -0,0 +1,537 @@
+#!/usr/bin/env python3
+"""
+Compare Groups in Latent Dynamics
+
+Side-by-side comparison of recurrence matrices and RQA features
+between different groups (HC, MCI, AD).
+
+Usage:
+    python compare_hc_mci.py                       # Default: HC vs MCI (3 subjects each)
+    python compare_hc_mci.py --conditions HID MCI  # Compare HC vs MCI
+    python compare_hc_mci.py --conditions HID AD   # Compare HC vs AD
+    python compare_hc_mci.py --conditions HID MCI AD  # Compare all three groups
+    python compare_hc_mci.py --n-subjects 5        # Compare 5 unique subjects per group
+    python compare_hc_mci.py --n-chunks 3          # Use 3 chunks per subject (averaged)
+    python compare_hc_mci.py --chunk 2             # Use specific chunk index
+    python compare_hc_mci.py --list-subjects       # List available subjects
+"""
+
+import argparse
+import json
+import sys
+from datetime import datetime
+from pathlib import Path
+
+import numpy as np
+import matplotlib.pyplot as plt
+
+# Add parent to path
+sys.path.insert(0, str(Path(__file__).parent))
+
+from config import (
+    CHECKPOINT_PATH, DATA_DIR, OUTPUT_DIR, DEVICE,
+    FILTER_LOW, FILTER_HIGH, CHUNK_DURATION, SFREQ,
+    RR_TARGETS, THEILER_WINDOW, ensure_output_dir, get_fif_files,
+    get_subjects_by_group, get_subject_id, get_label_name
+)
+from load_model import load_model_from_checkpoint, create_model, compute_latent_trajectory
+from load_data import load_and_preprocess_fif
+from plot_recurrence import compute_angular_distance_matrix, compute_recurrence_matrix, compute_rqa_stats
+
+# Color and name mapping for groups
+GROUP_COLORS = {
+    0: "blue",   # HC
+    1: "orange", # MCI
+    2: "red",    # AD
+}
+
+GROUP_NAMES = {
+    0: "HC",
+    1: "MCI",
+    2: "AD",
+}
+
+
+def create_timestamped_output_dir(base_dir: Path, script_name: str) -> Path:
+    """Create a timestamped output directory for versioned results."""
+    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+    output_dir = base_dir / f"{script_name}_{timestamp}"
+    output_dir.mkdir(parents=True, exist_ok=True)
+    return output_dir
+
+
+def save_parameters(output_dir: Path, params: dict):
+    """Save parameters to a JSON file for reproducibility."""
+    params_path = output_dir / "parameters.json"
+
+    # Convert Path objects to strings
+    serializable_params = {}
+    for k, v in params.items():
+        if isinstance(v, Path):
+            serializable_params[k] = str(v)
+        else:
+            serializable_params[k] = v
+
+    serializable_params["timestamp"] = datetime.now().isoformat()
+
+    with open(params_path, 'w') as f:
+        json.dump(serializable_params, f, indent=2)
+    print(f"Parameters saved to: {params_path}")
+
+
+def analyze_subject(
+    model,
+    fif_path: Path,
+    model_info: dict,
+    rr_target: float = 0.02,
+    theiler: int = 50,
+    chunk_idx: int | None = None,
+    n_chunks: int = 1,
+):
+    """
+    Analyze a single subject and return metrics.
+
+    Args:
+        chunk_idx: Specific chunk to use (None = use first n_chunks and average)
+        n_chunks: Number of chunks to average (if chunk_idx is None)
+    """
+    data = load_and_preprocess_fif(
+        fif_path, FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+        verbose=False,
+    )
+
+    available_chunks = len(data["chunks"])
+
+    # Determine which chunks to process
+    if chunk_idx is not None:
+        if chunk_idx >= available_chunks:
+            print(f"    Warning: chunk {chunk_idx} not available, using 0")
+            chunk_idx = 0
+        chunks_to_use = [chunk_idx]
+    else:
+        chunks_to_use = list(range(min(n_chunks, available_chunks)))
+
+    # Process each chunk and collect RQA stats
+    all_stats = []
+    last_latent = None
+    last_R = None
+
+    for cidx in chunks_to_use:
+        phase_data = data["chunks"][cidx]
+        latent = compute_latent_trajectory(model, phase_data, DEVICE)
+        distance_matrix = compute_angular_distance_matrix(latent)
+        R, eps = compute_recurrence_matrix(distance_matrix, rr_target, theiler)
+        stats = compute_rqa_stats(R)
+        all_stats.append(stats)
+        last_latent = latent
+        last_R = R
+
+    # Average stats across chunks (all RQA features)
+    feature_keys = all_stats[0].keys() if all_stats else []
+    avg_stats = {key: np.mean([s[key] for s in all_stats]) for key in feature_keys}
+
+    return {
+        "subject_id": data["subject_id"],
+        "latent": last_latent,  # Keep last for visualization
+        "recurrence_matrix": last_R,
+        "rqa_stats": avg_stats,
+        "n_chunks_used": len(chunks_to_use),
+        "n_chunks_available": available_chunks,
+    }
+
+
+def plot_group_comparison(
+    group_results: dict,
+    output_dir: Path,
+    rr_target: float,
+    show_plot: bool = True,
+):
+    """Create comparison plot between groups."""
+    # Determine layout
+    n_groups = len(group_results)
+    max_subjects = max(len(results) for results in group_results.values())
+
+    fig, axes = plt.subplots(n_groups, max_subjects, figsize=(5 * max_subjects, 5 * n_groups))
+
+    # Ensure axes is 2D
+    if n_groups == 1:
+        axes = axes.reshape(1, -1)
+    if max_subjects == 1:
+        axes = axes.reshape(-1, 1)
+
+    # Get time extent from first result
+    first_results = list(group_results.values())[0]
+    time_extent = first_results[0]["latent"].shape[0] / SFREQ
+
+    # Plot each group
+    for row_idx, (group_key, results) in enumerate(group_results.items()):
+        label = results[0]["rqa_stats"]  # Just to check structure
+        group_name = GROUP_NAMES.get(int(group_key) if group_key.isdigit() else {"hc": 0, "mci": 1, "ad": 2}.get(group_key, 0), group_key.upper())
+
+        for col_idx, res in enumerate(results):
+            axes[row_idx, col_idx].imshow(
+                res["recurrence_matrix"], cmap="binary", origin="lower",
+                extent=[0, time_extent, 0, time_extent]
+            )
+            axes[row_idx, col_idx].set_title(
+                f"{group_name}: {res['subject_id']}\nDET={res['rqa_stats']['DET']:.2f}",
+                fontsize=11
+            )
+            axes[row_idx, col_idx].set_xlabel("Time (s)")
+            if col_idx == 0:
+                axes[row_idx, col_idx].set_ylabel("Time (s)")
+
+        # Hide unused axes
+        for col_idx in range(len(results), max_subjects):
+            axes[row_idx, col_idx].axis("off")
+
+    # Compute group stats
+    group_stats = {}
+    for group_key, results in group_results.items():
+        group_name = GROUP_NAMES.get(int(group_key) if group_key.isdigit() else {"hc": 0, "mci": 1, "ad": 2}.get(group_key, 0), group_key.upper())
+        det_mean = np.mean([r["rqa_stats"]["DET"] for r in results])
+        group_stats[group_name] = det_mean
+
+    stats_str = " | ".join([f"{name} avg DET={det:.3f}" for name, det in group_stats.items()])
+    group_names_str = " vs ".join(group_stats.keys())
+
+    plt.suptitle(
+        f"{group_names_str} Recurrence Matrices (RR={rr_target*100:.0f}%)\n{stats_str}",
+        fontsize=14, fontweight="bold"
+    )
+    plt.tight_layout()
+
+    save_path = output_dir / f"group_comparison_rr{int(rr_target*100)}.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def plot_rqa_violin(group_results: dict, output_dir: Path, show_plot: bool = True):
+    """
+    Violin plots comparing all RQA features between groups.
+
+    Expected MCI vs HC trends (brain criticality framework):
+    =========================================================
+    MCI shows shift toward SUPERCRITICALITY (more rigid dynamics):
+
+    Feature   | MCI vs HC | Interpretation
+    ----------|-----------|-------------------------------------------
+    DET       | ↑ higher  | More deterministic/predictable dynamics
+    LAM       | ↑ higher  | More laminar states (system gets "stuck")
+    TT        | ↑ higher  | Longer trapping time in states
+    L_mean    | ↑ higher  | Longer diagonal lines = more predictable
+    L_max     | ↑ higher  | Longer max recurrence = attractor collapse
+    ENTR      | ↓ lower   | Reduced complexity in dynamics
+    DIV       | ↓ lower   | Lower divergence = less chaotic
+
+    Healthy brains operate at "edge of chaos" (critical point).
+    MCI/AD shifts toward overcoupled, rigid dynamics.
+    """
+
+    # Get all feature keys from first result
+    first_results = list(group_results.values())[0]
+    if not first_results:
+        print("No results to plot")
+        return
+
+    all_features = list(first_results[0]["rqa_stats"].keys())
+
+    # Features to plot (exclude RR as it's controlled)
+    features_to_plot = ["DET", "LAM", "TT", "L_mean", "ENTR", "DIV"]
+    features_to_plot = [f for f in features_to_plot if f in all_features]
+
+    n_features = len(features_to_plot)
+    n_cols = 3
+    n_rows = (n_features + n_cols - 1) // n_cols
+
+    fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))
+    axes = axes.flatten() if n_rows > 1 or n_cols > 1 else [axes]
+
+    # Prepare data for each feature
+    group_keys = list(group_results.keys())
+    group_names = [GROUP_NAMES.get({"hc": 0, "mci": 1, "ad": 2}.get(k.lower(), 0), k.upper()) for k in group_keys]
+    colors = [GROUP_COLORS.get({"hc": 0, "mci": 1, "ad": 2}.get(k.lower(), 0), "gray") for k in group_keys]
+
+    for idx, feature in enumerate(features_to_plot):
+        ax = axes[idx]
+
+        # Collect data for violin plot
+        data_per_group = []
+        for group_key in group_keys:
+            results = group_results[group_key]
+            values = [r["rqa_stats"][feature] for r in results]
+            data_per_group.append(values)
+
+        # Create violin plot
+        positions = np.arange(len(group_keys))
+        parts = ax.violinplot(data_per_group, positions=positions, showmeans=True, showmedians=True)
+
+        # Color the violins
+        for i, pc in enumerate(parts['bodies']):
+            pc.set_facecolor(colors[i])
+            pc.set_alpha(0.7)
+
+        # Style the lines
+        for partname in ['cmeans', 'cmedians', 'cbars', 'cmins', 'cmaxes']:
+            if partname in parts:
+                parts[partname].set_edgecolor('black')
+                parts[partname].set_linewidth(1)
+
+        # Add individual points (jittered)
+        for i, (group_key, values) in enumerate(zip(group_keys, data_per_group)):
+            jitter = np.random.uniform(-0.1, 0.1, len(values))
+            ax.scatter(positions[i] + jitter, values, c=colors[i], alpha=0.6, s=40, edgecolors='black', linewidths=0.5, zorder=5)
+
+        ax.set_xticks(positions)
+        ax.set_xticklabels(group_names)
+        ax.set_ylabel(feature)
+        ax.set_title(feature, fontweight="bold")
+
+        # Add expected trend annotation
+        expected_trends = {
+            "DET": "↑ in MCI",
+            "LAM": "↑ in MCI",
+            "TT": "↑ in MCI",
+            "L_mean": "↑ in MCI",
+            "ENTR": "↓ in MCI",
+            "DIV": "↓ in MCI",
+        }
+        if feature in expected_trends:
+            ax.annotate(f"Expected: {expected_trends[feature]}", xy=(0.02, 0.98),
+                       xycoords='axes fraction', fontsize=8, ha='left', va='top',
+                       style='italic', color='gray')
+
+    # Hide unused axes
+    for idx in range(len(features_to_plot), len(axes)):
+        axes[idx].axis("off")
+
+    group_str = " vs ".join(group_names)
+    plt.suptitle(f"RQA Features: {group_str}\n(Violin plots with individual subjects)", fontsize=14, fontweight="bold")
+    plt.tight_layout()
+
+    save_path = output_dir / "rqa_violin_comparison.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def print_expected_trends():
+    """Print expected MCI vs HC trends based on brain criticality framework."""
+    print("\n" + "=" * 70)
+    print("EXPECTED MCI vs HC TRENDS (Brain Criticality Framework)")
+    print("=" * 70)
+    print("""
+MCI represents shift toward SUPERCRITICALITY (more rigid, less flexible):
+
+Feature   | MCI vs HC | Interpretation
+----------|-----------|------------------------------------------------
+DET       | ↑ higher  | More deterministic dynamics (predictable)
+LAM       | ↑ higher  | More laminar states (system gets "stuck")
+TT        | ↑ higher  | Longer trapping time in attractor states
+L_mean    | ↑ higher  | Longer diagonal lines = sustained patterns
+L_max     | ↑ higher  | Max recurrence length = attractor collapse
+ENTR      | ↓ lower   | Reduced complexity/flexibility
+DIV       | ↓ lower   | Lower divergence = less chaotic
+
+Healthy brains operate near criticality ("edge of chaos").
+MCI/AD shows overcoupled, rigid dynamics = departure from optimal.
+""")
+    print("=" * 70)
+
+
+def list_subjects(groups: dict):
+    """List available subjects by group."""
+    print("\n" + "=" * 60)
+    print("AVAILABLE SUBJECTS")
+    print("=" * 60)
+
+    for group_key in ["hc", "mci", "ad"]:
+        subjects = groups.get(group_key, [])
+        if not subjects:
+            continue
+
+        group_name = GROUP_NAMES.get(subjects[0][1], group_key.upper())
+        print(f"\n{group_name} subjects ({len(subjects)}):")
+        for i, (fif_path, label, condition, subject_id) in enumerate(subjects):
+            print(f"  {i+1:3d}. {subject_id:10s} ({condition})")
+
+    total = sum(len(groups.get(k, [])) for k in ["hc", "mci", "ad"])
+    print(f"\nTotal: {total} unique subjects")
+
+
+def main():
+    parser = argparse.ArgumentParser(
+        description="Compare group latent dynamics",
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        epilog="""
+Examples:
+  python compare_hc_mci.py                       # Default: HC vs MCI, 3 subjects each
+  python compare_hc_mci.py --conditions HID MCI  # HC vs MCI only
+  python compare_hc_mci.py --conditions HID AD   # HC vs AD only
+  python compare_hc_mci.py --conditions HID MCI AD  # All three groups
+  python compare_hc_mci.py --n-subjects 5        # 5 subjects per group
+  python compare_hc_mci.py --n-chunks 3          # Average RQA over 3 chunks per subject
+  python compare_hc_mci.py --chunk 2             # Use only chunk 2 for each subject
+  python compare_hc_mci.py --list-subjects       # Show available subjects
+        """
+    )
+    parser.add_argument("--n-subjects", type=int, default=3,
+                        help="Number of unique subjects per group (default: 3)")
+    parser.add_argument("--n-chunks", type=int, default=1,
+                        help="Number of chunks to average per subject (default: 1)")
+    parser.add_argument("--chunk", type=int, default=None,
+                        help="Specific chunk index to use (overrides --n-chunks)")
+    parser.add_argument("--conditions", type=str, nargs="+", default=["HID", "MCI"],
+                        help="Conditions to compare: HID, MCI, AD (default: HID MCI)")
+    parser.add_argument("--rr-target", type=float, default=0.02,
+                        help="Target recurrence rate (default: 0.02 = 2%%)")
+    parser.add_argument("--theiler", type=int, default=THEILER_WINDOW,
+                        help=f"Theiler window in samples (default: {THEILER_WINDOW})")
+    parser.add_argument("--no-theiler", action="store_true",
+                        help="Disable Theiler window")
+    parser.add_argument("--list-subjects", action="store_true",
+                        help="List available subjects and exit")
+    parser.add_argument("--no-show", action="store_true",
+                        help="Don't display plots interactively")
+    parser.add_argument("--output-dir", type=str, default=None,
+                        help="Custom output directory (default: timestamped folder)")
+    args = parser.parse_args()
+
+    # Handle Theiler window
+    theiler = 0 if args.no_theiler else args.theiler
+
+    # Create timestamped output directory
+    base_output_dir = ensure_output_dir()
+    if args.output_dir:
+        output_dir = Path(args.output_dir)
+        output_dir.mkdir(parents=True, exist_ok=True)
+    else:
+        output_dir = create_timestamped_output_dir(base_output_dir, "compare_hc_mci")
+
+    # Get subjects by group for selected conditions
+    fif_files = get_fif_files(args.conditions)
+    groups = get_subjects_by_group(fif_files)
+
+    # Print summary
+    group_counts = []
+    for key in ["hc", "mci", "ad"]:
+        if groups.get(key):
+            group_counts.append(f"{len(groups[key])} {key.upper()}")
+    print(f"Found {', '.join(group_counts)} subjects for conditions: {args.conditions}")
+
+    # Handle --list-subjects
+    if args.list_subjects:
+        list_subjects(groups)
+        return 0
+
+    # Check we have at least two groups
+    active_groups = {k: v for k, v in groups.items() if v}
+    if len(active_groups) < 2:
+        print("Need at least 2 groups with subjects for comparison!")
+        print("Available groups:", list(active_groups.keys()))
+        return 1
+
+    # Save parameters
+    params = {
+        "n_subjects": args.n_subjects,
+        "n_chunks": args.n_chunks,
+        "chunk": args.chunk,
+        "conditions": args.conditions,
+        "rr_target": args.rr_target,
+        "theiler": theiler,
+        "checkpoint_path": str(CHECKPOINT_PATH),
+        "filter_low": FILTER_LOW,
+        "filter_high": FILTER_HIGH,
+        "chunk_duration": CHUNK_DURATION,
+        "sfreq": SFREQ,
+        "groups_found": {k: len(v) for k, v in active_groups.items()},
+    }
+    save_parameters(output_dir, params)
+
+    # Load model
+    print("\nLoading model...")
+    model_info = load_model_from_checkpoint(CHECKPOINT_PATH, DEVICE)
+
+    # Create model
+    first_subject = list(active_groups.values())[0][0]
+    first_data = load_and_preprocess_fif(
+        first_subject[0], FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+        verbose=False,
+    )
+    model = create_model(first_data["n_channels"], model_info, DEVICE)
+
+    # Analyze subjects from each group
+    group_results = {}
+
+    for group_key, subjects in active_groups.items():
+        group_name = GROUP_NAMES.get(subjects[0][1], group_key.upper())
+
+        print(f"\nAnalyzing {group_name} subjects (targeting {args.n_subjects})...")
+        group_results[group_key] = []
+
+        # Iterate through subjects until we have enough valid ones
+        subjects_processed = 0
+        for fif_path, label, condition, subject_id in subjects:
+            if subjects_processed >= args.n_subjects:
+                break
+
+            print(f"  {subject_id} ({condition})...")
+            res = analyze_subject(
+                model, fif_path, model_info,
+                rr_target=args.rr_target,
+                theiler=theiler,
+                chunk_idx=args.chunk,
+                n_chunks=args.n_chunks,
+            )
+
+            # Skip subjects with no chunks
+            if res['n_chunks_available'] == 0:
+                print(f"    WARNING: No chunks available, skipping subject")
+                continue
+
+            print(f"    Used {res['n_chunks_used']}/{res['n_chunks_available']} chunks")
+            group_results[group_key].append(res)
+            subjects_processed += 1
+
+        if subjects_processed < args.n_subjects:
+            print(f"  NOTE: Only found {subjects_processed} valid subjects (requested {args.n_subjects})")
+
+    # Print summary
+    print("\n" + "=" * 60)
+    print("SUMMARY")
+    print("=" * 60)
+    print(f"Settings: RR={args.rr_target*100:.0f}%, Theiler={theiler}, Chunks={args.n_chunks if args.chunk is None else f'#{args.chunk}'}")
+
+    for group_key, results in group_results.items():
+        group_name = GROUP_NAMES.get(results[0]["rqa_stats"] if isinstance(results[0]["rqa_stats"], int) else {"hc": 0, "mci": 1, "ad": 2}.get(group_key, 0), group_key.upper())
+        print(f"\n{group_name} subjects (n={len(results)}):")
+        for r in results:
+            print(f"  {r['subject_id']}: DET={r['rqa_stats']['DET']:.3f}, RR={r['rqa_stats']['RR']:.4f}")
+        mean_det = np.mean([r['rqa_stats']['DET'] for r in results])
+        print(f"  Mean DET: {mean_det:.3f}")
+
+    # Print expected trends before analysis summary
+    print_expected_trends()
+
+    # Generate plots
+    print("\nGenerating plots...")
+    plot_group_comparison(group_results, output_dir, args.rr_target, not args.no_show)
+    plot_rqa_violin(group_results, output_dir, not args.no_show)
+
+    print(f"\nAll plots saved to: {output_dir}")
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/scripts/local_analysis/config.py b/scripts/local_analysis/config.py
new file mode 100644
index 0000000..2157ea2
--- /dev/null
+++ b/scripts/local_analysis/config.py
@@ -0,0 +1,279 @@
+"""
+Local Analysis Configuration
+
+Edit these paths to match your local setup.
+Supports multiple datasets through DatasetConfig system.
+"""
+
+from pathlib import Path
+import sys
+
+# Add src to path for imports
+sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
+
+# =============================================================================
+# DATASET SELECTION - Choose which dataset to use
+# =============================================================================
+
+# Available datasets: "greek_resting", "meditation_bids"
+DATASET = "greek_resting"
+
+# =============================================================================
+# PATHS - Edit these to match your local setup
+# =============================================================================
+
+# Model checkpoint (from MatrixAutoEncoder repo or RunPod)
+CHECKPOINT_PATH = Path("/Users/luki/Documents/GitHub/eeg-state-biomarkers/models/best2.pt")
+
+# Data directories for different datasets
+DATA_PATHS = {
+    "greek_resting": Path("/Volumes/Nvme_Data/GreekData"),
+    "meditation_bids": Path("/Volumes/Nvme_Data/ds001787"),
+}
+
+# Legacy support: DATA_DIR for backwards compatibility
+DATA_DIR = DATA_PATHS.get(DATASET, DATA_PATHS["greek_resting"])
+
+# Subdirectory mapping for different conditions (Greek dataset only)
+# AD -> AD-RAW/FILT, MCI -> MCI-RAW/FILT, HC/HID -> HC-RAW/FILT
+CONDITION_SUBDIRS = {
+    "AD": "AD-RAW/FILT",
+    "MCI": "MCI-RAW/FILT",
+    "HID": "HC-RAW/FILT",
+    "HC": "HC-RAW/FILT",
+}
+
+# Output directory for plots
+OUTPUT_DIR = Path("/Users/luki/Documents/GitHub/eeg-state-biomarkers/results/local_analysis")
+
+# =============================================================================
+# ANALYSIS PARAMETERS
+# =============================================================================
+
+# Sampling frequency of EEG data
+SFREQ = 250.0
+
+# Filter settings for phase extraction
+# Default: 1-120Hz (broadband including high gamma)
+# Note: High frequencies (>40Hz) may contain EMG artifacts in frontal/temporal channels
+FILTER_LOW = 1.0
+FILTER_HIGH = 120.0
+
+# Chunk duration in seconds
+CHUNK_DURATION = 5.0
+
+# RQA parameters
+RR_TARGETS = [0.01, 0.02, 0.05]  # 1%, 2%, 5%
+THEILER_WINDOW = 50  # samples (~0.2 seconds)
+
+# UMAP parameters
+UMAP_N_NEIGHBORS = 15
+UMAP_MIN_DIST = 0.1
+UMAP_N_COMPONENTS = 3  # 3D for visualization
+
+# Device for model inference
+DEVICE = "mps"  # Use "mps" for M1 Mac, "cuda" for GPU, "cpu" for CPU
+
+# =============================================================================
+# HELPER FUNCTIONS
+# =============================================================================
+
+def ensure_output_dir():
+    """Create output directory if it doesn't exist."""
+    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
+    return OUTPUT_DIR
+
+
+def get_fif_files(conditions=None):
+    """
+    Get list of .fif files from data directory.
+
+    Args:
+        conditions: List of conditions to include (e.g., ["AD", "MCI", "HID"])
+                   If None, includes all.
+
+    Returns:
+        List of (fif_path, label, condition) tuples
+        Labels: HC/HID=0, MCI=1, AD=2
+    """
+    if conditions is None:
+        conditions = ["AD", "MCI", "HID"]
+
+    # Patterns to exclude (inverse solutions, processed files, etc.)
+    # These are source-space reconstructions, not raw EEG
+    exclude_patterns = [
+        "-inv.fif",      # Inverse solution
+        "-v-inv.fif",    # Vector inverse solution
+        "-ave.fif",      # Averaged evoked
+        "-cov.fif",      # Covariance matrix
+        "-fwd.fif",      # Forward solution
+        "-src.fif",      # Source space
+        "-trans.fif",    # Transformation matrix
+    ]
+
+    # Label mapping: HC/HID=0 (healthy), MCI=1, AD=2
+    label_map = {"HC": 0, "HID": 0, "MCI": 1, "AD": 2}
+
+    files = []
+    for condition in conditions:
+        # Use condition-specific subdirectory
+        subdir = CONDITION_SUBDIRS.get(condition, condition)
+        condition_dir = DATA_DIR / subdir
+
+        if condition_dir.exists():
+            label = label_map.get(condition, 1)  # Default to 1 if unknown
+            for fif_file in sorted(condition_dir.glob("**/*.fif")):
+                # Skip non-raw files (inverse solutions, etc.)
+                filename = fif_file.name.lower()
+                if any(pattern in filename for pattern in exclude_patterns):
+                    continue
+                # Only include files with "_eeg.fif" suffix (raw EEG)
+                if "_eeg.fif" in filename:
+                    files.append((fif_file, label, condition))
+        else:
+            print(f"Warning: Directory not found: {condition_dir}")
+
+    return files
+
+
+def get_subject_id(fif_path):
+    """
+    Extract subject ID from file path.
+
+    Examples:
+        .../i002 20150109 1027.fil/i002 20150109 1027.fil_good_1_eeg.fif -> i002
+        .../I040_20150702_1203/I040_20150702_1203_good_1_eeg.fif -> I040
+        .../S017 20140124 0857.fil/S017 20140124 0857.fil_good_1_eeg.fif -> S017
+    """
+    # Get the parent folder name (subject folder)
+    folder_name = fif_path.parent.name
+    # Extract first part (subject ID) - split on space or underscore
+    if " " in folder_name:
+        return folder_name.split()[0]
+    elif "_" in folder_name:
+        return folder_name.split("_")[0]
+    else:
+        return folder_name
+
+
+def get_unique_subjects(fif_files: list) -> dict:
+    """
+    Group fif files by unique subject ID.
+
+    Args:
+        fif_files: List of (fif_path, label, condition) tuples
+
+    Returns:
+        Dict mapping subject_id -> (fif_path, label, condition)
+        Only returns first file per subject (avoids duplicates)
+    """
+    subjects = {}
+    for fif_path, label, condition in fif_files:
+        subject_id = get_subject_id(fif_path)
+        if subject_id not in subjects:
+            subjects[subject_id] = (fif_path, label, condition)
+    return subjects
+
+
+def get_subjects_by_group(fif_files: list) -> dict:
+    """
+    Get unique subjects separated by group.
+
+    Args:
+        fif_files: List of (fif_path, label, condition) tuples
+
+    Returns:
+        Dict with keys 'hc', 'mci', 'ad', each containing list of
+        (fif_path, label, condition, subject_id) tuples
+    """
+    subjects = get_unique_subjects(fif_files)
+
+    groups = {
+        "hc": [],   # label=0 (HC/HID)
+        "mci": [],  # label=1
+        "ad": [],   # label=2
+    }
+
+    for subject_id, (fif_path, label, condition) in subjects.items():
+        entry = (fif_path, label, condition, subject_id)
+        if label == 0:
+            groups["hc"].append(entry)
+        elif label == 1:
+            groups["mci"].append(entry)
+        elif label == 2:
+            groups["ad"].append(entry)
+
+    return groups
+
+
+def get_label_name(label: int) -> str:
+    """Get human-readable name for label."""
+    return {0: "HC", 1: "MCI", 2: "AD"}.get(label, "Unknown")
+
+
+# =============================================================================
+# DATASET CONFIG INTEGRATION
+# =============================================================================
+
+def get_dataset_config():
+    """
+    Get the DatasetConfig for the currently selected dataset.
+
+    Returns:
+        DatasetConfig instance for the current dataset
+    """
+    try:
+        from eeg_biomarkers.data.dataset_config import get_dataset_config as _get_config
+        return _get_config(DATASET)
+    except ImportError:
+        print("Warning: eeg_biomarkers not installed. Using legacy config.")
+        return None
+
+
+def get_data_files_via_config(groups=None):
+    """
+    Get data files using the DatasetConfig system.
+
+    This is the preferred method for multi-dataset support.
+
+    Args:
+        groups: List of group names to include (e.g., ["HC", "MCI"])
+                If None, includes all groups in the dataset config.
+
+    Returns:
+        List of (file_path, label, group_name) tuples
+    """
+    config = get_dataset_config()
+    if config is None:
+        # Fallback to legacy method
+        return get_fif_files(groups)
+
+    data_dir = DATA_PATHS.get(DATASET, DATA_DIR)
+
+    files = []
+    for group in config.groups:
+        if groups is not None and group.name not in groups:
+            continue
+
+        group_files = config.get_files_for_group(data_dir, group)
+        for file_path in group_files:
+            files.append((file_path, group.label, group.name))
+
+    return files
+
+
+def get_subject_id_via_config(file_path: Path) -> str:
+    """
+    Extract subject ID using the DatasetConfig system.
+
+    Args:
+        file_path: Path to data file
+
+    Returns:
+        Subject ID string
+    """
+    config = get_dataset_config()
+    if config is None:
+        return get_subject_id(file_path)
+
+    return config.get_subject_id(file_path)
diff --git a/scripts/local_analysis/full_dataset_analysis.py b/scripts/local_analysis/full_dataset_analysis.py
new file mode 100644
index 0000000..904a6c6
--- /dev/null
+++ b/scripts/local_analysis/full_dataset_analysis.py
@@ -0,0 +1,1707 @@
+#!/usr/bin/env python3
+"""
+Full-Dataset Statistical Analysis for Systems Neuroscience Paper
+
+Quantifies reorganization of metastable brain dynamics across cognitive decline
+using embedding-invariant state-space flow statistics with bootstrap confidence.
+
+This is NOT about classification or biomarkers.
+It IS about:
+- Robust flow geometry changes across groups
+- Statistical confidence via subject-level bootstrapping
+- Cross-embedding consistency validation
+
+Usage:
+    python full_dataset_analysis.py                    # Fast analysis (pca, tpca, delay)
+    python full_dataset_analysis.py --no-show          # Run without displaying plots
+    python full_dataset_analysis.py --embedding all    # Include slow methods (diffusion, umap)
+    python full_dataset_analysis.py --n-bootstrap 500  # Custom bootstrap iterations
+    python full_dataset_analysis.py --quick            # Quick test (100 bootstrap, 5 subjects)
+    python full_dataset_analysis.py --embedding pca    # Single embedding method
+
+Key outputs:
+    - Bootstrap confidence intervals for all flow metrics
+    - Density difference maps with statistical masking
+    - Radial density and speed profiles
+    - Effect sizes (Cohen's d) with CIs
+    - Cross-embedding robustness metrics
+"""
+
+import argparse
+import sys
+from pathlib import Path
+from dataclasses import dataclass, field
+from typing import Optional
+import json
+from datetime import datetime
+
+import numpy as np
+import matplotlib.pyplot as plt
+from matplotlib.colors import Normalize
+from scipy.ndimage import gaussian_filter
+from scipy.spatial.distance import pdist, squareform
+from scipy.stats import spearmanr, entropy
+from sklearn.decomposition import PCA
+from tqdm import tqdm
+
+# Add parent to path
+sys.path.insert(0, str(Path(__file__).parent))
+
+from config import (
+    CHECKPOINT_PATH, DATA_DIR, OUTPUT_DIR, DEVICE,
+    FILTER_LOW, FILTER_HIGH, CHUNK_DURATION, SFREQ,
+    ensure_output_dir, get_fif_files, get_subjects_by_group
+)
+from load_model import load_model_from_checkpoint, create_model, compute_latent_trajectory
+from load_data import load_and_preprocess_fif
+
+# Optional imports
+try:
+    import umap
+    HAS_UMAP = True
+except ImportError:
+    HAS_UMAP = False
+
+GROUP_COLORS = {0: "#1f77b4", 1: "#ff7f0e", 2: "#d62728"}  # Blue, Orange, Red
+GROUP_NAMES = {0: "HC", 1: "MCI", 2: "AD"}
+
+
+def create_timestamped_output_dir(base_dir: Path, script_name: str) -> Path:
+    """Create a timestamped output directory for versioned results."""
+    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+    output_dir = base_dir / f"{script_name}_{timestamp}"
+    output_dir.mkdir(parents=True, exist_ok=True)
+    return output_dir
+
+
+def save_parameters(output_dir: Path, params: dict):
+    """Save parameters to a JSON file for reproducibility."""
+    params_path = output_dir / "parameters.json"
+
+    # Convert Path objects to strings
+    serializable_params = {}
+    for k, v in params.items():
+        if isinstance(v, Path):
+            serializable_params[k] = str(v)
+        else:
+            serializable_params[k] = v
+
+    serializable_params["timestamp"] = datetime.now().isoformat()
+
+    with open(params_path, 'w') as f:
+        json.dump(serializable_params, f, indent=2)
+    print(f"Parameters saved to: {params_path}")
+
+
+# =============================================================================
+# DATA STRUCTURES
+# =============================================================================
+
+@dataclass
+class FlowMetrics:
+    """Flow geometry metrics for a trajectory."""
+    mean_speed: float
+    speed_std: float
+    speed_cv: float
+    n_dwell_episodes: int
+    total_dwell_time: int
+    mean_dwell_duration: float
+    occupancy_entropy: float
+    path_tortuosity: float
+    explored_variance: float
+
+
+@dataclass
+class BootstrapResult:
+    """Bootstrap confidence interval result."""
+    mean: float
+    std: float
+    ci_low: float
+    ci_high: float
+    samples: np.ndarray = field(repr=False)
+
+    @classmethod
+    def from_samples(cls, samples: np.ndarray, ci: float = 0.95):
+        alpha = (1 - ci) / 2
+        return cls(
+            mean=np.mean(samples),
+            std=np.std(samples),
+            ci_low=np.percentile(samples, alpha * 100),
+            ci_high=np.percentile(samples, (1 - alpha) * 100),
+            samples=samples,
+        )
+
+
+@dataclass
+class SubjectData:
+    """Data for a single subject."""
+    subject_id: str
+    group: str  # "hc", "mci", "ad"
+    label: int
+    trajectory: np.ndarray  # (T, D) latent trajectory
+
+
+# =============================================================================
+# EMBEDDING METHODS (from multi_embedding.py)
+# =============================================================================
+
+def pca_embedding(z: np.ndarray, n_components: int = 2) -> tuple[np.ndarray, dict]:
+    """Standard PCA embedding."""
+    pca = PCA(n_components=n_components)
+    embedded = pca.fit_transform(z)
+    return embedded, {
+        "method": "PCA",
+        "var_explained": pca.explained_variance_ratio_.tolist(),
+        "total_var": sum(pca.explained_variance_ratio_),
+    }
+
+
+def time_lagged_pca(z: np.ndarray, tau: int = 5, n_components: int = 2) -> tuple[np.ndarray, dict]:
+    """Time-lagged PCA: PCA on [z(t), z(t+τ)] pairs."""
+    T, D = z.shape
+    z_lagged = np.hstack([z[:-tau], z[tau:]])
+    pca = PCA(n_components=n_components)
+    embedded = pca.fit_transform(z_lagged)
+    return embedded, {
+        "method": f"tPCA (τ={tau})",
+        "tau": tau,
+        "var_explained": pca.explained_variance_ratio_.tolist(),
+        "total_var": sum(pca.explained_variance_ratio_),
+    }
+
+
+def diffusion_maps(z: np.ndarray, n_components: int = 2, k: int = 10) -> tuple[np.ndarray, dict]:
+    """Diffusion maps embedding."""
+    T = z.shape[0]
+    max_points = 2000
+    if T > max_points:
+        indices = np.linspace(0, T - 1, max_points, dtype=int)
+        z_sub = z[indices]
+    else:
+        indices = np.arange(T)
+        z_sub = z
+
+    distances = squareform(pdist(z_sub, metric='euclidean'))
+    knn_dists = np.sort(distances, axis=1)[:, 1:k+1]
+    eps = np.median(knn_dists)
+    K = np.exp(-distances**2 / (2 * eps**2))
+    D_inv = np.diag(1.0 / K.sum(axis=1))
+    P = D_inv @ K
+    eigenvalues, eigenvectors = np.linalg.eig(P)
+    idx = np.argsort(np.abs(eigenvalues))[::-1]
+    eigenvalues = eigenvalues[idx].real
+    eigenvectors = eigenvectors[:, idx].real
+    embedded_sub = eigenvectors[:, 1:n_components+1]
+
+    if T > max_points:
+        from scipy.interpolate import interp1d
+        f = interp1d(indices, embedded_sub, axis=0, kind='linear', fill_value='extrapolate')
+        embedded = f(np.arange(T))
+    else:
+        embedded = embedded_sub
+
+    return embedded, {"method": f"Diffusion Maps (k={k})", "epsilon": eps, "k": k}
+
+
+def delay_embedding(z: np.ndarray, tau: int = 5, dim: int = 3, n_components: int = 2) -> tuple[np.ndarray, dict]:
+    """Takens-style delay embedding followed by PCA."""
+    T, D = z.shape
+    n_valid = T - (dim - 1) * tau
+    if n_valid < 100:
+        return pca_embedding(z, n_components)  # Fallback
+
+    delayed = np.zeros((n_valid, D * dim))
+    for i in range(dim):
+        delayed[:, i*D:(i+1)*D] = z[i*tau:i*tau + n_valid]
+
+    pca = PCA(n_components=n_components)
+    embedded = pca.fit_transform(delayed)
+    return embedded, {"method": f"Delay (τ={tau}, d={dim})", "tau": tau, "dim": dim}
+
+
+# =============================================================================
+# FLOW METRICS
+# =============================================================================
+
+def compute_instantaneous_speed(embedded: np.ndarray) -> np.ndarray:
+    """Compute speed in embedding space."""
+    diff = np.diff(embedded, axis=0)
+    return np.linalg.norm(diff, axis=1)
+
+
+def detect_dwell_episodes(speed: np.ndarray, threshold_percentile: float = 20, min_duration: int = 10) -> list:
+    """Detect contiguous low-speed runs."""
+    threshold = np.percentile(speed, threshold_percentile)
+    is_slow = speed < threshold
+    episodes = []
+    in_episode = False
+    start = 0
+    for i, slow in enumerate(is_slow):
+        if slow and not in_episode:
+            in_episode = True
+            start = i
+        elif not slow and in_episode:
+            in_episode = False
+            if i - start >= min_duration:
+                episodes.append((start, i))
+    if in_episode and len(is_slow) - start >= min_duration:
+        episodes.append((start, len(is_slow)))
+    return episodes
+
+
+def compute_occupancy_entropy(embedded: np.ndarray, bins: int = 20) -> float:
+    """Compute entropy of occupancy distribution."""
+    H, _, _ = np.histogram2d(embedded[:, 0], embedded[:, 1], bins=bins)
+    H = H.flatten()
+    H = H[H > 0]
+    p = H / H.sum()
+    return entropy(p)
+
+
+def compute_flow_metrics(embedded: np.ndarray) -> FlowMetrics:
+    """Compute comprehensive flow geometry metrics."""
+    speed = compute_instantaneous_speed(embedded)
+    episodes = detect_dwell_episodes(speed)
+    path_length = speed.sum()
+    displacement = np.linalg.norm(embedded[-1] - embedded[0])
+    tortuosity = path_length / displacement if displacement > 0 else np.inf
+    explored_variance = np.var(embedded, axis=0).sum()
+    occ_entropy = compute_occupancy_entropy(embedded)
+
+    return FlowMetrics(
+        mean_speed=speed.mean(),
+        speed_std=speed.std(),
+        speed_cv=speed.std() / speed.mean() if speed.mean() > 0 else 0,
+        n_dwell_episodes=len(episodes),
+        total_dwell_time=sum(e[1] - e[0] for e in episodes),
+        mean_dwell_duration=np.mean([e[1] - e[0] for e in episodes]) if episodes else 0,
+        occupancy_entropy=occ_entropy,
+        path_tortuosity=tortuosity,
+        explored_variance=explored_variance,
+    )
+
+
+# =============================================================================
+# POOLED EMBEDDER (for shared coordinate space)
+# =============================================================================
+
+class PooledEmbedder:
+    """Fits embedding on pooled data, transforms individual trajectories."""
+
+    def __init__(self, method: str = "pca", tau: int = 5, delay_dim: int = 3):
+        self.method = method
+        self.tau = tau
+        self.delay_dim = delay_dim
+        self.transformer = None
+        self.bounds = None
+        self.centroid = None
+
+    def _preprocess(self, z: np.ndarray) -> np.ndarray:
+        if self.method == "tpca":
+            return np.hstack([z[:-self.tau], z[self.tau:]])
+        elif self.method == "delay":
+            T, D = z.shape
+            n_valid = T - (self.delay_dim - 1) * self.tau
+            if n_valid < 10:
+                return z
+            delayed = np.zeros((n_valid, D * self.delay_dim))
+            for d in range(self.delay_dim):
+                start = d * self.tau
+                end = start + n_valid
+                delayed[:, d * D:(d + 1) * D] = z[start:end]
+            return delayed
+        return z
+
+    def fit(self, trajectories: list[np.ndarray], n_samples_per_traj: int = 500):
+        """Fit embedding on pooled data."""
+        pooled = []
+        for traj in trajectories:
+            processed = self._preprocess(traj)
+            if len(processed) <= n_samples_per_traj:
+                pooled.append(processed)
+            else:
+                indices = np.linspace(0, len(processed) - 1, n_samples_per_traj, dtype=int)
+                pooled.append(processed[indices])
+
+        pooled_data = np.vstack(pooled)
+
+        if self.method in ["pca", "tpca", "delay"]:
+            self.transformer = PCA(n_components=2)
+            embedded = self.transformer.fit_transform(pooled_data)
+        elif self.method == "diffusion":
+            self._fit_diffusion_maps(pooled_data)
+            embedded = self._transform_diffusion(pooled_data)
+        elif self.method == "umap" and HAS_UMAP:
+            self.transformer = umap.UMAP(n_components=2, n_neighbors=30, min_dist=0.2, random_state=42)
+            embedded = self.transformer.fit_transform(pooled_data)
+        else:
+            self.transformer = PCA(n_components=2)
+            embedded = self.transformer.fit_transform(pooled_data)
+
+        # Store bounds and centroid
+        margin = 0.05
+        x_range = embedded[:, 0].max() - embedded[:, 0].min()
+        y_range = embedded[:, 1].max() - embedded[:, 1].min()
+        self.bounds = (
+            embedded[:, 0].min() - margin * x_range,
+            embedded[:, 0].max() + margin * x_range,
+            embedded[:, 1].min() - margin * y_range,
+            embedded[:, 1].max() + margin * y_range,
+        )
+        self.centroid = embedded.mean(axis=0)
+
+    def _fit_diffusion_maps(self, data: np.ndarray, k: int = 10):
+        max_points = 2000
+        if len(data) > max_points:
+            indices = np.linspace(0, len(data) - 1, max_points, dtype=int)
+            data_sub = data[indices]
+        else:
+            data_sub = data
+        distances = squareform(pdist(data_sub, metric='euclidean'))
+        knn_dists = np.sort(distances, axis=1)[:, 1:k+1]
+        self.dm_epsilon = np.median(knn_dists)
+        K = np.exp(-distances**2 / (2 * self.dm_epsilon**2))
+        D_inv = np.diag(1.0 / K.sum(axis=1))
+        P = D_inv @ K
+        eigenvalues, eigenvectors = np.linalg.eig(P)
+        idx = np.argsort(np.abs(eigenvalues))[::-1]
+        self.dm_eigenvalues = eigenvalues[idx].real
+        self.dm_eigenvectors = eigenvectors[:, idx].real
+        self.dm_fit_data = data_sub
+
+    def _transform_diffusion(self, data: np.ndarray) -> np.ndarray:
+        from scipy.spatial.distance import cdist
+        distances = cdist(data, self.dm_fit_data, metric='euclidean')
+        K = np.exp(-distances**2 / (2 * self.dm_epsilon**2))
+        K_normalized = K / K.sum(axis=1, keepdims=True)
+        return K_normalized @ self.dm_eigenvectors[:, 1:3]
+
+    def transform(self, trajectory: np.ndarray) -> np.ndarray:
+        processed = self._preprocess(trajectory)
+        if self.method in ["pca", "tpca", "delay"]:
+            return self.transformer.transform(processed)
+        elif self.method == "diffusion":
+            return self._transform_diffusion(processed)
+        elif self.method == "umap" and HAS_UMAP:
+            return self.transformer.transform(processed)
+        return self.transformer.transform(processed)
+
+    def get_method_name(self) -> str:
+        names = {
+            "pca": "PCA",
+            "tpca": f"tPCA (τ={self.tau})",
+            "diffusion": "Diffusion Maps",
+            "delay": f"Delay (τ={self.tau}, d={self.delay_dim})",
+            "umap": "UMAP",
+        }
+        return names.get(self.method, self.method.upper())
+
+
+# =============================================================================
+# DENSITY AND RADIAL COMPUTATIONS
+# =============================================================================
+
+def compute_density_on_grid(embedded: np.ndarray, bounds: tuple, bins: int = 50, sigma: float = 1.5) -> np.ndarray:
+    """Compute normalized 2D density on a shared grid."""
+    xmin, xmax, ymin, ymax = bounds
+    H, _, _ = np.histogram2d(embedded[:, 0], embedded[:, 1], bins=bins, range=[[xmin, xmax], [ymin, ymax]])
+    H = gaussian_filter(H.T, sigma=sigma)
+    if H.sum() > 0:
+        H = H / H.sum()
+    return H
+
+
+def compute_radial_profile(embedded: np.ndarray, centroid: np.ndarray, n_bins: int = 20) -> tuple[np.ndarray, np.ndarray]:
+    """Compute radial density profile from centroid."""
+    radii = np.linalg.norm(embedded - centroid, axis=1)
+    max_r = np.percentile(radii, 99)
+    bins = np.linspace(0, max_r, n_bins + 1)
+    counts, _ = np.histogram(radii, bins=bins)
+    # Normalize by ring area
+    ring_areas = np.pi * (bins[1:]**2 - bins[:-1]**2)
+    density = counts / (ring_areas * len(radii))
+    bin_centers = (bins[:-1] + bins[1:]) / 2
+    return bin_centers, density
+
+
+def compute_radial_speed_profile(embedded: np.ndarray, centroid: np.ndarray, n_bins: int = 20) -> tuple[np.ndarray, np.ndarray]:
+    """Compute mean speed as a function of radius."""
+    speed = compute_instantaneous_speed(embedded)
+    radii = np.linalg.norm(embedded[:-1] - centroid, axis=1)  # Match speed length
+    max_r = np.percentile(radii, 99)
+    bins = np.linspace(0, max_r, n_bins + 1)
+    bin_indices = np.digitize(radii, bins) - 1
+    bin_indices = np.clip(bin_indices, 0, n_bins - 1)
+
+    mean_speeds = np.zeros(n_bins)
+    for i in range(n_bins):
+        mask = bin_indices == i
+        if mask.sum() > 0:
+            mean_speeds[i] = speed[mask].mean()
+
+    bin_centers = (bins[:-1] + bins[1:]) / 2
+    return bin_centers, mean_speeds
+
+
+# =============================================================================
+# EFFECT SIZE COMPUTATION
+# =============================================================================
+
+def cohens_d(group1: np.ndarray, group2: np.ndarray) -> float:
+    """Compute Cohen's d effect size."""
+    n1, n2 = len(group1), len(group2)
+    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)
+    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))
+    if pooled_std == 0:
+        return 0
+    return (np.mean(group1) - np.mean(group2)) / pooled_std
+
+
+# =============================================================================
+# BOOTSTRAP ANALYSIS
+# =============================================================================
+
+def bootstrap_flow_metrics(
+    subjects: list[SubjectData],
+    embedder: PooledEmbedder,
+    n_bootstrap: int = 500,
+    random_state: int = 42,
+) -> dict[str, BootstrapResult]:
+    """Bootstrap flow metrics at the subject level."""
+    rng = np.random.RandomState(random_state)
+    n_subjects = len(subjects)
+
+    metric_names = ["mean_speed", "speed_cv", "n_dwell_episodes", "occupancy_entropy", "path_tortuosity", "explored_variance"]
+    samples = {name: [] for name in metric_names}
+
+    for _ in range(n_bootstrap):
+        # Sample subjects with replacement
+        indices = rng.choice(n_subjects, size=n_subjects, replace=True)
+
+        # Compute metrics for bootstrap sample
+        bootstrap_metrics = []
+        for idx in indices:
+            embedded = embedder.transform(subjects[idx].trajectory)
+            metrics = compute_flow_metrics(embedded)
+            bootstrap_metrics.append(metrics)
+
+        # Store mean of each metric
+        for name in metric_names:
+            vals = [getattr(m, name) for m in bootstrap_metrics]
+            samples[name].append(np.mean(vals))
+
+    # Convert to BootstrapResult
+    results = {}
+    for name in metric_names:
+        results[name] = BootstrapResult.from_samples(np.array(samples[name]))
+
+    return results
+
+
+def bootstrap_density_difference(
+    hc_subjects: list[SubjectData],
+    disease_subjects: list[SubjectData],
+    embedder: PooledEmbedder,
+    n_bootstrap: int = 500,
+    bins: int = 50,
+    random_state: int = 42,
+) -> tuple[np.ndarray, np.ndarray, np.ndarray]:
+    """
+    Bootstrap density difference maps.
+
+    Returns:
+        mean_diff: Mean difference map
+        ci_low: Lower 95% CI
+        ci_high: Upper 95% CI
+    """
+    rng = np.random.RandomState(random_state)
+    n_hc, n_disease = len(hc_subjects), len(disease_subjects)
+
+    diff_samples = []
+
+    for _ in range(n_bootstrap):
+        # Sample subjects with replacement
+        hc_indices = rng.choice(n_hc, size=n_hc, replace=True)
+        disease_indices = rng.choice(n_disease, size=n_disease, replace=True)
+
+        # Compute densities
+        hc_densities = []
+        for idx in hc_indices:
+            embedded = embedder.transform(hc_subjects[idx].trajectory)
+            density = compute_density_on_grid(embedded, embedder.bounds, bins=bins)
+            hc_densities.append(density)
+
+        disease_densities = []
+        for idx in disease_indices:
+            embedded = embedder.transform(disease_subjects[idx].trajectory)
+            density = compute_density_on_grid(embedded, embedder.bounds, bins=bins)
+            disease_densities.append(density)
+
+        # Compute difference
+        hc_mean = np.mean(hc_densities, axis=0)
+        disease_mean = np.mean(disease_densities, axis=0)
+        diff_samples.append(disease_mean - hc_mean)
+
+    diff_samples = np.array(diff_samples)
+    mean_diff = np.mean(diff_samples, axis=0)
+    ci_low = np.percentile(diff_samples, 2.5, axis=0)
+    ci_high = np.percentile(diff_samples, 97.5, axis=0)
+
+    return mean_diff, ci_low, ci_high
+
+
+def bootstrap_radial_profiles(
+    subjects: list[SubjectData],
+    embedder: PooledEmbedder,
+    n_bootstrap: int = 500,
+    n_bins: int = 20,
+    random_state: int = 42,
+) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
+    """
+    Bootstrap radial density and speed profiles.
+
+    Returns:
+        bin_centers, density_ci (mean, low, high), speed_ci (mean, low, high)
+    """
+    rng = np.random.RandomState(random_state)
+    n_subjects = len(subjects)
+
+    density_samples = []
+    speed_samples = []
+    bin_centers = None
+
+    for _ in range(n_bootstrap):
+        indices = rng.choice(n_subjects, size=n_subjects, replace=True)
+
+        all_densities = []
+        all_speeds = []
+
+        for idx in indices:
+            embedded = embedder.transform(subjects[idx].trajectory)
+            bc, density = compute_radial_profile(embedded, embedder.centroid, n_bins)
+            _, speed = compute_radial_speed_profile(embedded, embedder.centroid, n_bins)
+            all_densities.append(density)
+            all_speeds.append(speed)
+            if bin_centers is None:
+                bin_centers = bc
+
+        density_samples.append(np.mean(all_densities, axis=0))
+        speed_samples.append(np.mean(all_speeds, axis=0))
+
+    density_samples = np.array(density_samples)
+    speed_samples = np.array(speed_samples)
+
+    density_ci = (
+        np.mean(density_samples, axis=0),
+        np.percentile(density_samples, 2.5, axis=0),
+        np.percentile(density_samples, 97.5, axis=0),
+    )
+
+    speed_ci = (
+        np.mean(speed_samples, axis=0),
+        np.percentile(speed_samples, 2.5, axis=0),
+        np.percentile(speed_samples, 97.5, axis=0),
+    )
+
+    return bin_centers, density_ci, speed_ci
+
+
+def bootstrap_effect_size(
+    group1_values: np.ndarray,
+    group2_values: np.ndarray,
+    n_bootstrap: int = 500,
+    random_state: int = 42,
+) -> BootstrapResult:
+    """Bootstrap Cohen's d effect size."""
+    rng = np.random.RandomState(random_state)
+    n1, n2 = len(group1_values), len(group2_values)
+
+    d_samples = []
+    for _ in range(n_bootstrap):
+        idx1 = rng.choice(n1, size=n1, replace=True)
+        idx2 = rng.choice(n2, size=n2, replace=True)
+        d = cohens_d(group1_values[idx1], group2_values[idx2])
+        d_samples.append(d)
+
+    return BootstrapResult.from_samples(np.array(d_samples))
+
+
+# =============================================================================
+# CROSS-EMBEDDING ROBUSTNESS
+# =============================================================================
+
+def compute_cross_embedding_robustness(
+    subjects: list[SubjectData],
+    methods: list[str],
+    tau: int = 5,
+    delay_dim: int = 3,
+) -> dict:
+    """Quantify similarity of flow metrics across embeddings."""
+    # Get all trajectories
+    trajectories = [s.trajectory for s in subjects]
+
+    # Fit embedders for each method
+    embedders = {}
+    for method in methods:
+        embedder = PooledEmbedder(method=method, tau=tau, delay_dim=delay_dim)
+        embedder.fit(trajectories)
+        embedders[method] = embedder
+
+    # Compute metrics for each subject under each embedding
+    all_metrics = {method: [] for method in methods}
+    for subject in subjects:
+        for method in methods:
+            embedded = embedders[method].transform(subject.trajectory)
+            metrics = compute_flow_metrics(embedded)
+            all_metrics[method].append(metrics)
+
+    # Compute cross-embedding correlations for each metric
+    metric_names = ["mean_speed", "speed_cv", "occupancy_entropy", "path_tortuosity", "explored_variance"]
+    correlations = {}
+
+    for metric_name in metric_names:
+        method_values = {}
+        for method in methods:
+            method_values[method] = np.array([getattr(m, metric_name) for m in all_metrics[method]])
+
+        # Pairwise correlations
+        corr_matrix = np.zeros((len(methods), len(methods)))
+        for i, m1 in enumerate(methods):
+            for j, m2 in enumerate(methods):
+                corr, _ = spearmanr(method_values[m1], method_values[m2])
+                corr_matrix[i, j] = corr
+
+        correlations[metric_name] = {
+            "matrix": corr_matrix,
+            "mean_off_diagonal": np.mean(corr_matrix[np.triu_indices(len(methods), k=1)]),
+        }
+
+    return {
+        "correlations": correlations,
+        "methods": methods,
+        "n_subjects": len(subjects),
+    }
+
+
+# =============================================================================
+# PLOTTING FUNCTIONS
+# =============================================================================
+
+def plot_bootstrap_metrics_comparison(
+    group_results: dict[str, dict[str, BootstrapResult]],
+    output_dir: Path,
+    embedding_name: str,
+    show_plot: bool = True,
+):
+    """Plot bootstrap flow metrics comparison across groups."""
+    metric_names = ["mean_speed", "speed_cv", "n_dwell_episodes", "occupancy_entropy", "path_tortuosity", "explored_variance"]
+    metric_labels = ["Mean Speed", "Speed CV", "Dwell Episodes", "Occupancy Entropy", "Tortuosity", "Explored Variance"]
+
+    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
+
+    groups = list(group_results.keys())
+    colors = [GROUP_COLORS.get({"HC": 0, "MCI": 1, "AD": 2}.get(g, 0), "gray") for g in groups]
+
+    for idx, (metric, label) in enumerate(zip(metric_names, metric_labels)):
+        ax = axes.flatten()[idx]
+
+        x_pos = np.arange(len(groups))
+        means = [group_results[g][metric].mean for g in groups]
+        ci_lows = [group_results[g][metric].mean - group_results[g][metric].ci_low for g in groups]
+        ci_highs = [group_results[g][metric].ci_high - group_results[g][metric].mean for g in groups]
+
+        bars = ax.bar(x_pos, means, color=colors, alpha=0.7, edgecolor='black')
+        ax.errorbar(x_pos, means, yerr=[ci_lows, ci_highs], fmt='none', color='black', capsize=5, linewidth=2)
+
+        ax.set_xticks(x_pos)
+        ax.set_xticklabels(groups)
+        ax.set_ylabel(label)
+        ax.set_title(label, fontweight='bold')
+
+        # Add CI text
+        for i, g in enumerate(groups):
+            ci_text = f"[{group_results[g][metric].ci_low:.3f}, {group_results[g][metric].ci_high:.3f}]"
+            ax.text(i, means[i] + ci_highs[i] + 0.01 * max(means), ci_text, ha='center', fontsize=7)
+
+    plt.suptitle(f"Flow Metrics with 95% Bootstrap CIs ({embedding_name})", fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / f"bootstrap_metrics_{embedding_name.lower().replace(' ', '_')}.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def plot_density_difference_with_ci(
+    mean_diff: np.ndarray,
+    ci_low: np.ndarray,
+    ci_high: np.ndarray,
+    bounds: tuple,
+    group_name: str,
+    output_dir: Path,
+    embedding_name: str,
+    show_plot: bool = True,
+):
+    """Plot density difference with statistical masking."""
+    fig, axes = plt.subplots(1, 3, figsize=(18, 6))
+    extent = list(bounds)
+
+    # Mean difference
+    vmax = np.abs(mean_diff).max()
+    im1 = axes[0].imshow(mean_diff, origin='lower', extent=extent, cmap='RdBu_r', vmin=-vmax, vmax=vmax, aspect='equal')
+    axes[0].set_title(f"{group_name} − HC\nMean Difference", fontweight='bold')
+    plt.colorbar(im1, ax=axes[0], label='Δ Probability', shrink=0.8)
+
+    # Statistically significant regions (CI doesn't include 0)
+    significant = (ci_low > 0) | (ci_high < 0)
+    masked_diff = np.where(significant, mean_diff, np.nan)
+    im2 = axes[1].imshow(masked_diff, origin='lower', extent=extent, cmap='RdBu_r', vmin=-vmax, vmax=vmax, aspect='equal')
+    axes[1].set_title(f"Significant Regions\n(95% CI excludes 0)", fontweight='bold')
+    plt.colorbar(im2, ax=axes[1], label='Δ Probability', shrink=0.8)
+
+    # CI width (uncertainty)
+    ci_width = ci_high - ci_low
+    im3 = axes[2].imshow(ci_width, origin='lower', extent=extent, cmap='viridis', aspect='equal')
+    axes[2].set_title("Uncertainty\n(CI Width)", fontweight='bold')
+    plt.colorbar(im3, ax=axes[2], label='CI Width', shrink=0.8)
+
+    for ax in axes:
+        ax.set_xlabel("Dim 1")
+        ax.set_ylabel("Dim 2")
+
+    plt.suptitle(f"Density Difference: {group_name} vs HC ({embedding_name})", fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / f"density_diff_ci_{group_name.lower()}_{embedding_name.lower().replace(' ', '_')}.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def plot_radial_profiles(
+    group_profiles: dict[str, tuple],  # group -> (bin_centers, density_ci, speed_ci)
+    output_dir: Path,
+    embedding_name: str,
+    show_plot: bool = True,
+):
+    """Plot radial density and speed profiles with CIs."""
+    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
+
+    for group, (bin_centers, density_ci, speed_ci) in group_profiles.items():
+        color = GROUP_COLORS.get({"HC": 0, "MCI": 1, "AD": 2}.get(group, 0), "gray")
+
+        # Density profile
+        axes[0].plot(bin_centers, density_ci[0], color=color, label=group, linewidth=2)
+        axes[0].fill_between(bin_centers, density_ci[1], density_ci[2], color=color, alpha=0.2)
+
+        # Speed profile
+        axes[1].plot(bin_centers, speed_ci[0], color=color, label=group, linewidth=2)
+        axes[1].fill_between(bin_centers, speed_ci[1], speed_ci[2], color=color, alpha=0.2)
+
+    axes[0].set_xlabel("Radial Distance from Centroid")
+    axes[0].set_ylabel("Density")
+    axes[0].set_title("Radial Density Profile", fontweight='bold')
+    axes[0].legend()
+
+    axes[1].set_xlabel("Radial Distance from Centroid")
+    axes[1].set_ylabel("Mean Speed")
+    axes[1].set_title("Radial Speed Profile", fontweight='bold')
+    axes[1].legend()
+
+    plt.suptitle(f"Radial Profiles with 95% Bootstrap CIs ({embedding_name})", fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / f"radial_profiles_{embedding_name.lower().replace(' ', '_')}.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def plot_effect_sizes(
+    effect_sizes: dict[str, dict[str, BootstrapResult]],
+    output_dir: Path,
+    show_plot: bool = True,
+):
+    """Plot Cohen's d effect sizes with CIs."""
+    comparisons = list(effect_sizes.keys())
+    metrics = list(effect_sizes[comparisons[0]].keys())
+
+    n_comparisons = len(comparisons)
+    n_metrics = len(metrics)
+
+    fig, ax = plt.subplots(figsize=(12, 6))
+
+    x = np.arange(n_metrics)
+    width = 0.8 / n_comparisons
+
+    colors = ['#1f77b4', '#ff7f0e', '#2ca02c']
+
+    for i, comparison in enumerate(comparisons):
+        means = [effect_sizes[comparison][m].mean for m in metrics]
+        ci_lows = [effect_sizes[comparison][m].mean - effect_sizes[comparison][m].ci_low for m in metrics]
+        ci_highs = [effect_sizes[comparison][m].ci_high - effect_sizes[comparison][m].mean for m in metrics]
+
+        offset = (i - n_comparisons / 2 + 0.5) * width
+        bars = ax.bar(x + offset, means, width, label=comparison, color=colors[i % len(colors)], alpha=0.7)
+        ax.errorbar(x + offset, means, yerr=[ci_lows, ci_highs], fmt='none', color='black', capsize=3)
+
+    # Add reference lines
+    ax.axhline(0, color='black', linestyle='-', linewidth=0.5)
+    ax.axhline(0.2, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)
+    ax.axhline(-0.2, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)
+    ax.axhline(0.5, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)
+    ax.axhline(-0.5, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)
+    ax.axhline(0.8, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)
+    ax.axhline(-0.8, color='gray', linestyle='--', linewidth=0.5, alpha=0.5)
+
+    ax.set_xticks(x)
+    ax.set_xticklabels([m.replace('_', ' ').title() for m in metrics])
+    ax.set_ylabel("Cohen's d")
+    ax.set_title("Effect Sizes with 95% Bootstrap CIs\n(|d|>0.2: small, |d|>0.5: medium, |d|>0.8: large)", fontweight='bold')
+    ax.legend()
+
+    plt.tight_layout()
+
+    save_path = output_dir / "effect_sizes.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def plot_cross_embedding_robustness(
+    robustness: dict,
+    output_dir: Path,
+    show_plot: bool = True,
+):
+    """Plot cross-embedding consistency heatmaps."""
+    metrics = list(robustness["correlations"].keys())
+    methods = robustness["methods"]
+    n_metrics = len(metrics)
+
+    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
+
+    for idx, metric in enumerate(metrics):
+        ax = axes.flatten()[idx]
+        corr_matrix = robustness["correlations"][metric]["matrix"]
+        mean_corr = robustness["correlations"][metric]["mean_off_diagonal"]
+
+        im = ax.imshow(corr_matrix, cmap='RdYlGn', vmin=-1, vmax=1)
+        ax.set_xticks(range(len(methods)))
+        ax.set_yticks(range(len(methods)))
+        ax.set_xticklabels(methods, rotation=45, ha='right')
+        ax.set_yticklabels(methods)
+        ax.set_title(f"{metric.replace('_', ' ').title()}\nMean ρ = {mean_corr:.2f}", fontweight='bold')
+
+        # Add correlation values
+        for i in range(len(methods)):
+            for j in range(len(methods)):
+                ax.text(j, i, f'{corr_matrix[i, j]:.2f}', ha='center', va='center', fontsize=8)
+
+        plt.colorbar(im, ax=ax, label='Spearman ρ')
+
+    # Hide unused axes
+    for idx in range(n_metrics, 6):
+        axes.flatten()[idx].axis('off')
+
+    plt.suptitle(f"Cross-Embedding Robustness (n={robustness['n_subjects']} subjects)", fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / "cross_embedding_robustness.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+# =============================================================================
+# GROUP FLOW FIELD ANALYSIS (Rabinovich-style)
+# =============================================================================
+
+def compute_group_flow_field(
+    subjects: list[SubjectData],
+    embedder: PooledEmbedder,
+    grid_size: int = 20,
+) -> tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
+    """
+    Compute aggregate flow field for a group of subjects.
+
+    Returns:
+        X, Y: Grid coordinates
+        flow_x, flow_y: Mean flow vectors
+        counts: Number of samples per bin
+    """
+    # Get bounds from embedder
+    xmin, xmax, ymin, ymax = embedder.bounds
+
+    x_edges = np.linspace(xmin, xmax, grid_size + 1)
+    y_edges = np.linspace(ymin, ymax, grid_size + 1)
+
+    # Accumulate flow across all subjects
+    flow_x_sum = np.zeros((grid_size, grid_size))
+    flow_y_sum = np.zeros((grid_size, grid_size))
+    counts = np.zeros((grid_size, grid_size))
+
+    for subject in subjects:
+        embedded = embedder.transform(subject.trajectory)
+
+        # Compute displacements
+        dx = np.diff(embedded[:, 0])
+        dy = np.diff(embedded[:, 1])
+
+        # Bin displacements
+        for i in range(len(dx)):
+            x_bin = np.searchsorted(x_edges[:-1], embedded[i, 0]) - 1
+            y_bin = np.searchsorted(y_edges[:-1], embedded[i, 1]) - 1
+
+            x_bin = np.clip(x_bin, 0, grid_size - 1)
+            y_bin = np.clip(y_bin, 0, grid_size - 1)
+
+            flow_x_sum[y_bin, x_bin] += dx[i]
+            flow_y_sum[y_bin, x_bin] += dy[i]
+            counts[y_bin, x_bin] += 1
+
+    # Average
+    flow_x = np.zeros_like(flow_x_sum)
+    flow_y = np.zeros_like(flow_y_sum)
+    mask = counts > 0
+    flow_x[mask] = flow_x_sum[mask] / counts[mask]
+    flow_y[mask] = flow_y_sum[mask] / counts[mask]
+
+    # Grid centers
+    x_centers = (x_edges[:-1] + x_edges[1:]) / 2
+    y_centers = (y_edges[:-1] + y_edges[1:]) / 2
+    X, Y = np.meshgrid(x_centers, y_centers)
+
+    return X, Y, flow_x, flow_y, counts
+
+
+def compute_flow_divergence(flow_x: np.ndarray, flow_y: np.ndarray, dx: float = 1.0) -> np.ndarray:
+    """Compute divergence of flow field (positive = sources, negative = sinks)."""
+    # Use central differences
+    dfx_dx = np.gradient(flow_x, dx, axis=1)
+    dfy_dy = np.gradient(flow_y, dx, axis=0)
+    return dfx_dx + dfy_dy
+
+
+def compute_flow_curl(flow_x: np.ndarray, flow_y: np.ndarray, dx: float = 1.0) -> np.ndarray:
+    """Compute curl (vorticity) of 2D flow field."""
+    dfx_dy = np.gradient(flow_x, dx, axis=0)
+    dfy_dx = np.gradient(flow_y, dx, axis=1)
+    return dfy_dx - dfx_dy
+
+
+def plot_group_flow_fields(
+    subject_data: dict[str, list[SubjectData]],
+    embedder: PooledEmbedder,
+    output_dir: Path,
+    embedding_name: str,
+    grid_size: int = 20,
+    show_plot: bool = True,
+):
+    """
+    Plot Rabinovich-style flow fields for each group side by side.
+
+    Shows:
+    - Flow vectors (quiver plot)
+    - Flow magnitude
+    - Trajectory density overlay
+    """
+    groups_with_data = [g for g in ["HC", "MCI", "AD"] if len(subject_data.get(g, [])) >= 3]
+    n_groups = len(groups_with_data)
+
+    if n_groups < 2:
+        print("  Not enough groups for flow field comparison")
+        return
+
+    fig, axes = plt.subplots(2, n_groups, figsize=(6 * n_groups, 12))
+    if n_groups == 1:
+        axes = axes.reshape(-1, 1)
+
+    extent = list(embedder.bounds)
+    flow_data = {}
+
+    for idx, group in enumerate(groups_with_data):
+        subjects = subject_data[group]
+
+        # Compute flow field
+        X, Y, flow_x, flow_y, counts = compute_group_flow_field(subjects, embedder, grid_size)
+        flow_data[group] = (X, Y, flow_x, flow_y, counts)
+
+        magnitude = np.sqrt(flow_x**2 + flow_y**2)
+        color = GROUP_COLORS.get({"HC": 0, "MCI": 1, "AD": 2}.get(group, 0), "gray")
+
+        # Top row: Flow field with trajectory overlay
+        ax = axes[0, idx]
+
+        # Get all embedded trajectories for density
+        all_embedded = []
+        for s in subjects:
+            emb = embedder.transform(s.trajectory)
+            all_embedded.append(emb)
+        all_embedded = np.vstack(all_embedded)
+
+        # Plot trajectory density as background
+        H, _, _ = np.histogram2d(all_embedded[:, 0], all_embedded[:, 1], bins=50,
+                                  range=[[extent[0], extent[1]], [extent[2], extent[3]]])
+        H = gaussian_filter(H.T, sigma=1.5)
+        ax.imshow(H, origin='lower', extent=extent, cmap='Greys', alpha=0.4, aspect='equal')
+
+        # Plot flow vectors
+        ax.quiver(X, Y, flow_x, flow_y, magnitude, cmap='plasma', alpha=0.9, scale=None)
+
+        ax.set_xlim(extent[0], extent[1])
+        ax.set_ylim(extent[2], extent[3])
+        ax.set_aspect('equal')
+        ax.set_xlabel("Dim 1")
+        ax.set_ylabel("Dim 2")
+        ax.set_title(f"{group} (n={len(subjects)})\nFlow Field + Density", fontweight='bold', color=color)
+
+        # Bottom row: Flow magnitude heatmap
+        ax = axes[1, idx]
+        im = ax.imshow(magnitude, origin='lower', extent=extent, cmap='viridis', aspect='equal')
+        plt.colorbar(im, ax=ax, label='Flow Magnitude', shrink=0.8)
+
+        ax.set_xlim(extent[0], extent[1])
+        ax.set_ylim(extent[2], extent[3])
+        ax.set_aspect('equal')
+        ax.set_xlabel("Dim 1")
+        ax.set_ylabel("Dim 2")
+        ax.set_title(f"{group} Flow Magnitude", fontweight='bold')
+
+    plt.suptitle(f"Group Flow Fields - Rabinovich-style ({embedding_name})", fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / f"group_flow_fields_{embedding_name.lower().replace(' ', '_')}.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return flow_data
+
+
+def plot_flow_difference(
+    subject_data: dict[str, list[SubjectData]],
+    embedder: PooledEmbedder,
+    output_dir: Path,
+    embedding_name: str,
+    grid_size: int = 20,
+    show_plot: bool = True,
+):
+    """
+    Plot flow field differences between groups.
+
+    Shows:
+    - Vector difference (MCI - HC, AD - HC)
+    - Magnitude difference
+    - Divergence/curl differences
+    """
+    hc_subjects = subject_data.get("HC", [])
+    if len(hc_subjects) < 3:
+        print("  Not enough HC subjects for flow difference")
+        return
+
+    # Compute HC flow
+    X, Y, hc_flow_x, hc_flow_y, hc_counts = compute_group_flow_field(hc_subjects, embedder, grid_size)
+    hc_mag = np.sqrt(hc_flow_x**2 + hc_flow_y**2)
+
+    disease_groups = [g for g in ["MCI", "AD"] if len(subject_data.get(g, [])) >= 3]
+    n_disease = len(disease_groups)
+
+    if n_disease == 0:
+        print("  Not enough disease subjects for flow difference")
+        return
+
+    fig, axes = plt.subplots(3, n_disease, figsize=(7 * n_disease, 15))
+    if n_disease == 1:
+        axes = axes.reshape(-1, 1)
+
+    extent = list(embedder.bounds)
+
+    for idx, disease_group in enumerate(disease_groups):
+        disease_subjects = subject_data[disease_group]
+
+        # Compute disease flow
+        _, _, disease_flow_x, disease_flow_y, disease_counts = compute_group_flow_field(
+            disease_subjects, embedder, grid_size
+        )
+        disease_mag = np.sqrt(disease_flow_x**2 + disease_flow_y**2)
+
+        # Differences
+        diff_flow_x = disease_flow_x - hc_flow_x
+        diff_flow_y = disease_flow_y - hc_flow_y
+        diff_mag = disease_mag - hc_mag
+        diff_vector_mag = np.sqrt(diff_flow_x**2 + diff_flow_y**2)
+
+        color = GROUP_COLORS.get({"HC": 0, "MCI": 1, "AD": 2}.get(disease_group, 1))
+
+        # Row 1: Vector difference (quiver)
+        ax = axes[0, idx]
+        ax.quiver(X, Y, diff_flow_x, diff_flow_y, diff_vector_mag, cmap='coolwarm', alpha=0.9)
+        ax.set_xlim(extent[0], extent[1])
+        ax.set_ylim(extent[2], extent[3])
+        ax.set_aspect('equal')
+        ax.set_xlabel("Dim 1")
+        ax.set_ylabel("Dim 2")
+        ax.set_title(f"{disease_group} − HC\nFlow Vector Difference", fontweight='bold', color=color)
+
+        # Row 2: Magnitude difference (heatmap)
+        ax = axes[1, idx]
+        vmax = np.abs(diff_mag).max()
+        im = ax.imshow(diff_mag, origin='lower', extent=extent, cmap='RdBu_r',
+                       vmin=-vmax, vmax=vmax, aspect='equal')
+        plt.colorbar(im, ax=ax, label='Δ Magnitude', shrink=0.8)
+        ax.set_xlim(extent[0], extent[1])
+        ax.set_ylim(extent[2], extent[3])
+        ax.set_aspect('equal')
+        ax.set_xlabel("Dim 1")
+        ax.set_ylabel("Dim 2")
+        ax.set_title(f"{disease_group} − HC\nMagnitude Difference", fontweight='bold')
+
+        # Row 3: Divergence difference
+        ax = axes[2, idx]
+        hc_div = compute_flow_divergence(hc_flow_x, hc_flow_y)
+        disease_div = compute_flow_divergence(disease_flow_x, disease_flow_y)
+        diff_div = disease_div - hc_div
+        vmax = np.abs(diff_div).max()
+        im = ax.imshow(diff_div, origin='lower', extent=extent, cmap='PuOr',
+                       vmin=-vmax, vmax=vmax, aspect='equal')
+        plt.colorbar(im, ax=ax, label='Δ Divergence', shrink=0.8)
+        ax.set_xlim(extent[0], extent[1])
+        ax.set_ylim(extent[2], extent[3])
+        ax.set_aspect('equal')
+        ax.set_xlabel("Dim 1")
+        ax.set_ylabel("Dim 2")
+        ax.set_title(f"{disease_group} − HC\nDivergence Difference\n(+sources, −sinks)", fontweight='bold')
+
+    plt.suptitle(f"Flow Field Differences ({embedding_name})", fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / f"flow_difference_{embedding_name.lower().replace(' ', '_')}.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def compute_flow_statistics(
+    X: np.ndarray, Y: np.ndarray,
+    flow_x: np.ndarray, flow_y: np.ndarray,
+    counts: np.ndarray
+) -> dict:
+    """Compute summary statistics for a flow field."""
+    magnitude = np.sqrt(flow_x**2 + flow_y**2)
+
+    # Weight by counts (how well sampled each bin is)
+    valid = counts > 10
+
+    if valid.sum() == 0:
+        return {"mean_magnitude": 0, "std_magnitude": 0, "mean_divergence": 0}
+
+    weighted_mag = magnitude[valid]
+
+    # Divergence and curl
+    div = compute_flow_divergence(flow_x, flow_y)
+    curl = compute_flow_curl(flow_x, flow_y)
+
+    return {
+        "mean_magnitude": weighted_mag.mean(),
+        "std_magnitude": weighted_mag.std(),
+        "max_magnitude": weighted_mag.max(),
+        "mean_divergence": div[valid].mean(),
+        "std_divergence": div[valid].std(),
+        "mean_curl": curl[valid].mean(),
+        "std_curl": curl[valid].std(),
+        "coverage": valid.sum() / valid.size,  # Fraction of grid with data
+    }
+
+
+# =============================================================================
+# MAIN ANALYSIS
+# =============================================================================
+
+def load_all_subjects(
+    model,
+    model_info: dict,
+    groups: dict,
+    n_subjects_per_group: Optional[int],
+    n_chunks: int,
+) -> dict[str, list[SubjectData]]:
+    """Load all subjects' latent trajectories."""
+    subject_data = {"HC": [], "MCI": [], "AD": []}
+
+    for group_key in ["hc", "mci", "ad"]:
+        subjects = groups.get(group_key, [])
+        if not subjects:
+            continue
+
+        group_name = GROUP_NAMES.get(subjects[0][1], group_key.upper())
+        max_subjects = n_subjects_per_group if n_subjects_per_group else len(subjects)
+
+        print(f"\nLoading {group_name} subjects (max {max_subjects})...")
+        subjects_processed = 0
+
+        for fif_path, label, condition, subject_id in tqdm(subjects, desc=group_name):
+            if subjects_processed >= max_subjects:
+                break
+
+            # Load and extract latent
+            data = load_and_preprocess_fif(
+                fif_path, FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+                include_amplitude=model_info["include_amplitude"],
+                verbose=False,
+            )
+
+            if len(data["chunks"]) == 0:
+                continue
+
+            chunks_to_use = min(n_chunks, len(data["chunks"]))
+            latents = []
+            for cidx in range(chunks_to_use):
+                latent = compute_latent_trajectory(model, data["chunks"][cidx], DEVICE)
+                latents.append(latent)
+
+            trajectory = np.concatenate(latents, axis=0)
+
+            subject_data[group_name].append(SubjectData(
+                subject_id=subject_id,
+                group=group_key,
+                label=label,
+                trajectory=trajectory,
+            ))
+            subjects_processed += 1
+
+        print(f"  Loaded {subjects_processed} {group_name} subjects")
+
+    return subject_data
+
+
+def run_full_analysis(
+    subject_data: dict[str, list[SubjectData]],
+    output_dir: Path,
+    methods: list[str],
+    n_bootstrap: int,
+    tau: int,
+    delay_dim: int,
+    show_plot: bool,
+):
+    """Run full statistical analysis."""
+    results = {
+        "timestamp": datetime.now().isoformat(),
+        "n_bootstrap": n_bootstrap,
+        "methods": methods,
+        "n_subjects": {g: len(s) for g, s in subject_data.items()},
+    }
+
+    # Get all trajectories for pooled fitting
+    all_subjects = []
+    for group_subjects in subject_data.values():
+        all_subjects.extend(group_subjects)
+    all_trajectories = [s.trajectory for s in all_subjects]
+
+    for method in methods:
+        print(f"\n{'='*80}")
+        print(f"ANALYZING: {method.upper()}")
+        print(f"{'='*80}")
+
+        # Fit pooled embedder
+        print(f"\nFitting {method} embedder on pooled data...")
+        embedder = PooledEmbedder(method=method, tau=tau, delay_dim=delay_dim)
+        embedder.fit(all_trajectories)
+        embedding_name = embedder.get_method_name()
+
+        # 1. Bootstrap flow metrics for each group
+        print(f"\nBootstrapping flow metrics ({n_bootstrap} iterations)...")
+        group_bootstrap_results = {}
+        for group, subjects in subject_data.items():
+            if len(subjects) < 3:
+                print(f"  Skipping {group} (only {len(subjects)} subjects)")
+                continue
+            print(f"  {group}...", end=" ", flush=True)
+            group_bootstrap_results[group] = bootstrap_flow_metrics(subjects, embedder, n_bootstrap)
+            print("done")
+
+        # Plot bootstrap metrics comparison
+        if len(group_bootstrap_results) > 1:
+            plot_bootstrap_metrics_comparison(group_bootstrap_results, output_dir, embedding_name, show_plot)
+
+        # 2. Density difference with CI
+        print(f"\nBootstrapping density differences...")
+        hc_subjects = subject_data.get("HC", [])
+        for disease_group in ["MCI", "AD"]:
+            disease_subjects = subject_data.get(disease_group, [])
+            if len(hc_subjects) < 3 or len(disease_subjects) < 3:
+                continue
+
+            print(f"  {disease_group} - HC...", end=" ", flush=True)
+            mean_diff, ci_low, ci_high = bootstrap_density_difference(
+                hc_subjects, disease_subjects, embedder, n_bootstrap
+            )
+            print("done")
+
+            plot_density_difference_with_ci(
+                mean_diff, ci_low, ci_high, embedder.bounds, disease_group, output_dir, embedding_name, show_plot
+            )
+
+        # 3. Radial profiles
+        print(f"\nBootstrapping radial profiles...")
+        group_profiles = {}
+        for group, subjects in subject_data.items():
+            if len(subjects) < 3:
+                continue
+            print(f"  {group}...", end=" ", flush=True)
+            bin_centers, density_ci, speed_ci = bootstrap_radial_profiles(subjects, embedder, n_bootstrap)
+            group_profiles[group] = (bin_centers, density_ci, speed_ci)
+            print("done")
+
+        if len(group_profiles) > 1:
+            plot_radial_profiles(group_profiles, output_dir, embedding_name, show_plot)
+
+        # 4. Group flow fields (Rabinovich-style)
+        print(f"\nComputing group flow fields...")
+        flow_data = plot_group_flow_fields(subject_data, embedder, output_dir, embedding_name, show_plot=show_plot)
+
+        # 5. Flow field differences
+        print(f"\nComputing flow field differences...")
+        plot_flow_difference(subject_data, embedder, output_dir, embedding_name, show_plot=show_plot)
+
+        # Compute flow statistics for results
+        flow_stats = {}
+        if flow_data:
+            for group, (X, Y, flow_x, flow_y, counts) in flow_data.items():
+                flow_stats[group] = compute_flow_statistics(X, Y, flow_x, flow_y, counts)
+
+        # Store results
+        results[method] = {
+            "bootstrap_metrics": {
+                g: {m: {"mean": r.mean, "ci_low": r.ci_low, "ci_high": r.ci_high}
+                    for m, r in metrics.items()}
+                for g, metrics in group_bootstrap_results.items()
+            },
+            "flow_statistics": flow_stats,
+        }
+
+    # 6. Effect sizes (aggregate across embeddings using PCA)
+    print(f"\n{'='*80}")
+    print("COMPUTING EFFECT SIZES")
+    print(f"{'='*80}")
+
+    # Use PCA for effect size computation
+    pca_embedder = PooledEmbedder(method="pca", tau=tau, delay_dim=delay_dim)
+    pca_embedder.fit(all_trajectories)
+
+    # Get metrics for each subject
+    group_metric_values = {g: {} for g in subject_data.keys()}
+    metric_names = ["mean_speed", "speed_cv", "n_dwell_episodes", "occupancy_entropy", "path_tortuosity", "explored_variance"]
+
+    for group, subjects in subject_data.items():
+        for metric in metric_names:
+            group_metric_values[group][metric] = []
+        for subject in subjects:
+            embedded = pca_embedder.transform(subject.trajectory)
+            metrics = compute_flow_metrics(embedded)
+            for metric in metric_names:
+                group_metric_values[group][metric].append(getattr(metrics, metric))
+
+    # Compute effect sizes
+    effect_sizes = {}
+    hc_values = group_metric_values.get("HC", {})
+
+    for disease_group in ["MCI", "AD"]:
+        disease_values = group_metric_values.get(disease_group, {})
+        if not disease_values or not hc_values:
+            continue
+
+        comparison = f"HC vs {disease_group}"
+        effect_sizes[comparison] = {}
+
+        for metric in metric_names:
+            if metric not in hc_values or metric not in disease_values:
+                continue
+            if len(hc_values[metric]) < 3 or len(disease_values[metric]) < 3:
+                continue
+
+            print(f"  {comparison} - {metric}...", end=" ", flush=True)
+            effect_sizes[comparison][metric] = bootstrap_effect_size(
+                np.array(hc_values[metric]),
+                np.array(disease_values[metric]),
+                n_bootstrap
+            )
+            print(f"d = {effect_sizes[comparison][metric].mean:.3f}")
+
+    if effect_sizes:
+        plot_effect_sizes(effect_sizes, output_dir, show_plot)
+
+    # 5. Cross-embedding robustness
+    print(f"\n{'='*80}")
+    print("COMPUTING CROSS-EMBEDDING ROBUSTNESS")
+    print(f"{'='*80}")
+
+    robustness = compute_cross_embedding_robustness(all_subjects, methods, tau, delay_dim)
+    plot_cross_embedding_robustness(robustness, output_dir, show_plot)
+
+    # Store effect sizes
+    results["effect_sizes"] = {
+        comparison: {m: {"mean": r.mean, "ci_low": r.ci_low, "ci_high": r.ci_high}
+                     for m, r in metrics.items()}
+        for comparison, metrics in effect_sizes.items()
+    }
+
+    # Store robustness
+    results["cross_embedding_robustness"] = {
+        metric: {"mean_correlation": data["mean_off_diagonal"]}
+        for metric, data in robustness["correlations"].items()
+    }
+
+    # Save results to JSON
+    results_path = output_dir / "full_analysis_results.json"
+
+    # Convert numpy types for JSON serialization
+    def convert_numpy(obj):
+        if isinstance(obj, np.ndarray):
+            return obj.tolist()
+        elif isinstance(obj, (np.float32, np.float64)):
+            return float(obj)
+        elif isinstance(obj, (np.int32, np.int64)):
+            return int(obj)
+        return obj
+
+    import json
+    class NumpyEncoder(json.JSONEncoder):
+        def default(self, obj):
+            return convert_numpy(obj)
+
+    with open(results_path, 'w') as f:
+        json.dump(results, f, indent=2, cls=NumpyEncoder)
+    print(f"\nResults saved to: {results_path}")
+
+    return results
+
+
+def print_summary_table(results: dict, output_dir: Path):
+    """Print and save summary table."""
+    print("\n" + "=" * 100)
+    print("SUMMARY TABLE: Flow Metrics with Bootstrap 95% CIs")
+    print("=" * 100)
+
+    lines = []
+    lines.append("=" * 100)
+    lines.append("SUMMARY TABLE: Flow Metrics with Bootstrap 95% CIs")
+    lines.append("=" * 100)
+
+    for method in results.get("methods", []):
+        if method not in results:
+            continue
+
+        lines.append(f"\n--- {method.upper()} ---")
+        print(f"\n--- {method.upper()} ---")
+
+        bootstrap = results[method].get("bootstrap_metrics", {})
+        if not bootstrap:
+            continue
+
+        groups = list(bootstrap.keys())
+        metrics = list(bootstrap[groups[0]].keys()) if groups else []
+
+        header = f"{'Metric':<25}" + "".join([f"{g:>30}" for g in groups])
+        lines.append(header)
+        print(header)
+
+        lines.append("-" * (25 + 30 * len(groups)))
+        print("-" * (25 + 30 * len(groups)))
+
+        for metric in metrics:
+            row = f"{metric.replace('_', ' ').title():<25}"
+            for g in groups:
+                m = bootstrap[g][metric]
+                row += f"{m['mean']:>10.3f} [{m['ci_low']:.3f}, {m['ci_high']:.3f}]"
+            lines.append(row)
+            print(row)
+
+    # Effect sizes
+    effect_sizes = results.get("effect_sizes", {})
+    if effect_sizes:
+        lines.append("\n" + "=" * 100)
+        lines.append("EFFECT SIZES (Cohen's d) with 95% CIs")
+        lines.append("=" * 100)
+        print("\n" + "=" * 100)
+        print("EFFECT SIZES (Cohen's d) with 95% CIs")
+        print("=" * 100)
+
+        for comparison, metrics in effect_sizes.items():
+            lines.append(f"\n{comparison}:")
+            print(f"\n{comparison}:")
+            for metric, data in metrics.items():
+                line = f"  {metric.replace('_', ' ').title():<25}: d = {data['mean']:>6.3f} [{data['ci_low']:.3f}, {data['ci_high']:.3f}]"
+                # Interpret
+                d = abs(data['mean'])
+                if d >= 0.8:
+                    interp = "(large)"
+                elif d >= 0.5:
+                    interp = "(medium)"
+                elif d >= 0.2:
+                    interp = "(small)"
+                else:
+                    interp = "(negligible)"
+                line += f" {interp}"
+                lines.append(line)
+                print(line)
+
+    # Save to file
+    table_path = output_dir / "summary_table.txt"
+    with open(table_path, 'w') as f:
+        f.write('\n'.join(lines))
+    print(f"\nSummary table saved to: {table_path}")
+
+
+def main():
+    parser = argparse.ArgumentParser(
+        description="Full-dataset statistical analysis for systems neuroscience paper",
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+    )
+    parser.add_argument("--n-subjects", type=int, default=None,
+                        help="Max subjects per group (default: all)")
+    parser.add_argument("--n-chunks", type=int, default=10,
+                        help="Chunks per subject (default: 10)")
+    parser.add_argument("--n-bootstrap", type=int, default=500,
+                        help="Bootstrap iterations (default: 500)")
+    parser.add_argument("--conditions", type=str, nargs="+", default=["HID", "MCI", "AD"],
+                        help="Conditions to include (default: HID MCI AD)")
+    parser.add_argument("--embedding", type=str, default="fast",
+                        choices=["pca", "tpca", "diffusion", "delay", "umap", "fast", "all"],
+                        help="Embedding method: 'fast' (pca+tpca+delay, default), 'all' (includes slow diffusion/umap), or single method")
+    parser.add_argument("--tau", type=int, default=5,
+                        help="Time lag for tPCA and delay embedding (default: 5)")
+    parser.add_argument("--delay-dim", type=int, default=3,
+                        help="Delay embedding dimension (default: 3)")
+    parser.add_argument("--quick", action="store_true",
+                        help="Quick test mode (100 bootstrap, 5 subjects)")
+    parser.add_argument("--no-show", action="store_true",
+                        help="Don't display plots interactively")
+    args = parser.parse_args()
+
+    # Set matplotlib backend to non-interactive if --no-show
+    if args.no_show:
+        import matplotlib
+        matplotlib.use('Agg')
+        print("Non-interactive mode: plots will be saved but not displayed")
+
+    # Quick mode overrides
+    if args.quick:
+        args.n_bootstrap = 100
+        args.n_subjects = 5
+        print("QUICK MODE: 100 bootstrap iterations, 5 subjects per group")
+
+    # Create timestamped output directory
+    base_output_dir = ensure_output_dir()
+    output_dir = create_timestamped_output_dir(base_output_dir, "full_dataset_analysis")
+    print(f"Output directory: {output_dir}")
+
+    # Get subjects
+    fif_files = get_fif_files(args.conditions)
+    groups = get_subjects_by_group(fif_files)
+
+    print(f"\nDataset overview:")
+    for key in ["hc", "mci", "ad"]:
+        if groups.get(key):
+            print(f"  {key.upper()}: {len(groups[key])} subjects")
+
+    # Load model
+    print("\nLoading model...")
+    model_info = load_model_from_checkpoint(CHECKPOINT_PATH, DEVICE)
+
+    # Get n_channels
+    all_subjects_list = []
+    for key in ["hc", "mci", "ad"]:
+        all_subjects_list.extend(groups.get(key, []))
+
+    first_data = load_and_preprocess_fif(
+        all_subjects_list[0][0], FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+        verbose=False,
+    )
+    model = create_model(first_data["n_channels"], model_info, DEVICE)
+
+    # Load all subject data
+    subject_data = load_all_subjects(
+        model, model_info, groups,
+        args.n_subjects, args.n_chunks
+    )
+
+    # Determine methods
+    if args.embedding == "fast":
+        methods = ["pca", "tpca", "delay"]  # Fast methods only (no diffusion/umap)
+    elif args.embedding == "all":
+        methods = ["pca", "tpca", "delay", "diffusion"]
+        if HAS_UMAP:
+            methods.append("umap")
+    else:
+        methods = [args.embedding]
+
+    print(f"Using embedding methods: {methods}")
+
+    # Save parameters for reproducibility
+    save_parameters(output_dir, {
+        "n_subjects": args.n_subjects,
+        "n_chunks": args.n_chunks,
+        "n_bootstrap": args.n_bootstrap,
+        "conditions": args.conditions,
+        "embedding": args.embedding,
+        "methods": methods,
+        "tau": args.tau,
+        "delay_dim": args.delay_dim,
+        "quick_mode": args.quick,
+        "filter_low": FILTER_LOW,
+        "filter_high": FILTER_HIGH,
+        "chunk_duration": CHUNK_DURATION,
+        "sfreq": SFREQ,
+        "checkpoint_path": CHECKPOINT_PATH,
+        "data_dir": DATA_DIR,
+        "device": DEVICE,
+    })
+
+    # Run analysis
+    results = run_full_analysis(
+        subject_data, output_dir, methods,
+        args.n_bootstrap, args.tau, args.delay_dim,
+        not args.no_show
+    )
+
+    # Print summary
+    print_summary_table(results, output_dir)
+
+    print(f"\n{'='*80}")
+    print("ANALYSIS COMPLETE")
+    print(f"{'='*80}")
+    print(f"All outputs saved to: {output_dir}")
+
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/scripts/local_analysis/load_data.py b/scripts/local_analysis/load_data.py
new file mode 100644
index 0000000..33fcf58
--- /dev/null
+++ b/scripts/local_analysis/load_data.py
@@ -0,0 +1,198 @@
+"""
+Data Loading Utilities for Local Analysis
+
+Handles loading .fif files and extracting phase data.
+"""
+
+from pathlib import Path
+import numpy as np
+import sys
+
+# Add src to path
+sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
+
+
+def load_eeg_from_fif(fif_path: Path, verbose: bool = True):
+    """
+    Load EEG data from .fif file.
+
+    Args:
+        fif_path: Path to .fif file
+        verbose: Whether to print info
+
+    Returns:
+        Tuple of (raw_data, sfreq, channel_names)
+        raw_data: (n_channels, n_samples) numpy array
+    """
+    import mne
+
+    # Suppress MNE info messages
+    mne.set_log_level("WARNING")
+
+    raw = mne.io.read_raw_fif(fif_path, preload=True)
+
+    if verbose:
+        print(f"Loaded: {fif_path.name}")
+        print(f"  Channels: {len(raw.ch_names)}")
+        print(f"  Duration: {raw.times[-1]:.1f}s")
+        print(f"  Sfreq: {raw.info['sfreq']} Hz")
+
+    return raw.get_data(), raw.info["sfreq"], raw.ch_names
+
+
+def extract_phase_circular(
+    data: np.ndarray,
+    sfreq: float,
+    filter_low: float = 1.0,
+    filter_high: float = 30.0,
+    include_amplitude: bool = True,
+) -> np.ndarray:
+    """
+    Extract circular phase representation (cos, sin) and optionally amplitude.
+
+    This is the CORRECT phase representation that avoids wraparound issues.
+
+    Args:
+        data: (n_channels, n_samples) raw EEG data
+        sfreq: Sampling frequency
+        filter_low: Low cutoff for bandpass filter
+        filter_high: High cutoff for bandpass filter
+        include_amplitude: Whether to include log-amplitude as third channel
+
+    Returns:
+        phase_data: (n_channels * phase_channels, n_samples) where phase_channels is 2 or 3
+    """
+    from scipy.signal import hilbert, butter, filtfilt
+
+    n_channels, n_samples = data.shape
+
+    # Bandpass filter
+    nyq = sfreq / 2
+    low = filter_low / nyq
+    high = filter_high / nyq
+    b, a = butter(4, [low, high], btype="band")
+
+    filtered = filtfilt(b, a, data, axis=1)
+
+    # Hilbert transform for analytic signal
+    analytic = hilbert(filtered, axis=1)
+
+    # Extract phase and amplitude
+    phase = np.angle(analytic)  # [-pi, pi]
+    amplitude = np.abs(analytic)
+
+    # Circular representation: (cos(phase), sin(phase))
+    cos_phase = np.cos(phase)
+    sin_phase = np.sin(phase)
+
+    if include_amplitude:
+        # Log-amplitude (more stable for neural networks)
+        log_amplitude = np.log1p(amplitude)
+        # Stack: (n_channels*3, n_samples)
+        phase_data = np.vstack([cos_phase, sin_phase, log_amplitude])
+    else:
+        # Stack: (n_channels*2, n_samples)
+        phase_data = np.vstack([cos_phase, sin_phase])
+
+    return phase_data.astype(np.float32)
+
+
+def chunk_data(data: np.ndarray, chunk_samples: int, overlap: float = 0.0):
+    """
+    Split data into chunks.
+
+    Args:
+        data: (n_features, n_samples) array
+        chunk_samples: Number of samples per chunk
+        overlap: Overlap fraction (0.0 = no overlap, 0.5 = 50% overlap)
+
+    Returns:
+        List of (n_features, chunk_samples) arrays
+    """
+    n_features, n_samples = data.shape
+    step = int(chunk_samples * (1 - overlap))
+
+    chunks = []
+    for start in range(0, n_samples - chunk_samples + 1, step):
+        end = start + chunk_samples
+        chunks.append(data[:, start:end])
+
+    return chunks
+
+
+def load_and_preprocess_fif(
+    fif_path: Path,
+    filter_low: float = 1.0,
+    filter_high: float = 30.0,
+    chunk_duration: float = 5.0,
+    include_amplitude: bool = True,
+    verbose: bool = True,
+):
+    """
+    Load .fif file and extract phase chunks ready for model.
+
+    Args:
+        fif_path: Path to .fif file
+        filter_low: Bandpass low cutoff
+        filter_high: Bandpass high cutoff
+        chunk_duration: Chunk duration in seconds
+        include_amplitude: Include amplitude in phase representation
+        verbose: Print progress info
+
+    Returns:
+        Dict with:
+            - chunks: List of (n_features, n_samples) arrays
+            - n_channels: Number of EEG channels
+            - sfreq: Sampling frequency
+            - channel_names: List of channel names
+            - subject_id: Extracted subject ID
+    """
+    # Load raw data
+    raw_data, sfreq, channel_names = load_eeg_from_fif(fif_path, verbose)
+    n_channels = len(channel_names)
+
+    # Extract phase
+    if verbose:
+        print(f"  Extracting phase ({filter_low}-{filter_high} Hz)...")
+    phase_data = extract_phase_circular(
+        raw_data, sfreq, filter_low, filter_high, include_amplitude
+    )
+
+    # Chunk
+    chunk_samples = int(chunk_duration * sfreq)
+    chunks = chunk_data(phase_data, chunk_samples)
+
+    if verbose:
+        print(f"  Created {len(chunks)} chunks of {chunk_duration}s")
+        print(f"  Phase shape per chunk: {chunks[0].shape}")
+
+    # Extract subject ID
+    subject_id = fif_path.parent.name.split()[0]
+
+    return {
+        "chunks": chunks,
+        "n_channels": n_channels,
+        "sfreq": sfreq,
+        "channel_names": channel_names,
+        "subject_id": subject_id,
+        "fif_path": fif_path,
+    }
+
+
+if __name__ == "__main__":
+    # Quick test with example file
+    from config import DATA_DIR, FILTER_LOW, FILTER_HIGH, CHUNK_DURATION
+
+    # Find first available .fif file
+    fif_files = list(DATA_DIR.rglob("*.fif"))
+    if fif_files:
+        result = load_and_preprocess_fif(
+            fif_files[0],
+            FILTER_LOW,
+            FILTER_HIGH,
+            CHUNK_DURATION,
+            include_amplitude=True,
+        )
+        print(f"\nLoaded {result['subject_id']}: {len(result['chunks'])} chunks")
+    else:
+        print(f"No .fif files found in {DATA_DIR}")
diff --git a/scripts/local_analysis/load_model.py b/scripts/local_analysis/load_model.py
new file mode 100644
index 0000000..165cbb1
--- /dev/null
+++ b/scripts/local_analysis/load_model.py
@@ -0,0 +1,159 @@
+"""
+Model Loading Utilities for Local Analysis
+
+Handles loading both ConvLSTM and Transformer autoencoders.
+"""
+
+from pathlib import Path
+import torch
+import sys
+
+# Add src to path
+sys.path.insert(0, str(Path(__file__).parent.parent.parent / "src"))
+
+from eeg_biomarkers.models import ConvLSTMAutoencoder, TransformerAutoencoder
+
+
+def load_model_from_checkpoint(checkpoint_path: Path, device: str = "mps"):
+    """
+    Load autoencoder model from checkpoint.
+
+    Automatically detects model type (ConvLSTM vs Transformer).
+
+    Args:
+        checkpoint_path: Path to .pt checkpoint file
+        device: Device to load model on ("mps", "cuda", "cpu")
+
+    Returns:
+        Tuple of (model, model_info_dict)
+    """
+    checkpoint = torch.load(checkpoint_path, map_location="cpu", weights_only=False)
+    state_dict = checkpoint["model_state_dict"]
+    config = checkpoint.get("config", {})
+
+    model_cfg = config.get("model", {})
+    encoder_cfg = model_cfg.get("encoder", {})
+    phase_cfg = model_cfg.get("phase", {})
+
+    # Extract model parameters
+    hidden_size = encoder_cfg.get("hidden_size", 64)
+    complexity = encoder_cfg.get("complexity", 2)
+    phase_channels = 3 if phase_cfg.get("include_amplitude", False) else 2
+    include_amplitude = phase_cfg.get("include_amplitude", False)
+
+    # Detect model type
+    model_name = model_cfg.get("name", "convlstm_autoencoder")
+    is_transformer = "transformer" in model_name.lower() or any(
+        "transformer_encoder" in k for k in state_dict.keys()
+    )
+
+    # Transformer-specific params
+    n_heads = encoder_cfg.get("n_heads", 4)
+    n_transformer_layers = encoder_cfg.get("n_transformer_layers", 2)
+    dim_feedforward = encoder_cfg.get("dim_feedforward", 256)
+
+    model_info = {
+        "hidden_size": hidden_size,
+        "complexity": complexity,
+        "phase_channels": phase_channels,
+        "include_amplitude": include_amplitude,
+        "is_transformer": is_transformer,
+        "n_heads": n_heads,
+        "n_transformer_layers": n_transformer_layers,
+        "dim_feedforward": dim_feedforward,
+        "state_dict": state_dict,
+        "val_loss": checkpoint.get("val_loss", None),
+        "epoch": checkpoint.get("epoch", None),
+    }
+
+    print(f"Loaded checkpoint: {checkpoint_path}")
+    print(f"  Model type: {'Transformer' if is_transformer else 'ConvLSTM'}")
+    print(f"  Hidden size: {hidden_size}")
+    print(f"  Phase channels: {phase_channels} ({'with amplitude' if include_amplitude else 'cos+sin only'})")
+    if model_info["val_loss"]:
+        print(f"  Val loss: {model_info['val_loss']:.4f}")
+
+    return model_info
+
+
+def create_model(n_channels: int, model_info: dict, device: str = "mps", load_weights: bool = True):
+    """
+    Create model instance with correct architecture.
+
+    Args:
+        n_channels: Number of EEG channels (e.g., 256)
+        model_info: Dict from load_model_from_checkpoint
+        device: Device to put model on
+        load_weights: If True, load trained weights. If False, use random initialization.
+
+    Returns:
+        Loaded model ready for inference
+    """
+    if model_info["is_transformer"]:
+        model = TransformerAutoencoder(
+            n_channels=n_channels,
+            hidden_size=model_info["hidden_size"],
+            complexity=model_info["complexity"],
+            phase_channels=model_info["phase_channels"],
+            n_heads=model_info["n_heads"],
+            n_transformer_layers=model_info["n_transformer_layers"],
+            dim_feedforward=model_info["dim_feedforward"],
+        )
+    else:
+        model = ConvLSTMAutoencoder(
+            n_channels=n_channels,
+            hidden_size=model_info["hidden_size"],
+            complexity=model_info["complexity"],
+            phase_channels=model_info["phase_channels"],
+        )
+
+    if load_weights:
+        model.load_state_dict(model_info["state_dict"])
+        print(f"Model loaded with trained weights on {device}")
+    else:
+        print(f"Model initialized with RANDOM weights on {device}")
+
+    model.to(device)
+    model.eval()
+
+    return model
+
+
+def compute_latent_trajectory(model, phase_data, device: str = "mps"):
+    """
+    Compute latent trajectory from phase data.
+
+    Args:
+        model: Trained autoencoder
+        phase_data: (n_features, n_samples) numpy array
+        device: Device for inference
+
+    Returns:
+        (n_samples, hidden_size) numpy array - latent trajectory over time
+    """
+    import numpy as np
+
+    model.eval()
+
+    # Add batch dimension: (1, n_features, n_samples)
+    x = torch.from_numpy(phase_data).float().unsqueeze(0).to(device)
+
+    with torch.no_grad():
+        _, latent = model(x)
+
+    # Handle different output formats
+    latent = latent.squeeze(0)  # Remove batch dim
+
+    # If shape is (hidden_size, time) where hidden_size < time, transpose
+    if latent.shape[0] < latent.shape[1]:
+        latent = latent.permute(1, 0)
+
+    return latent.cpu().numpy()
+
+
+if __name__ == "__main__":
+    # Quick test
+    from config import CHECKPOINT_PATH, DEVICE
+
+    model_info = load_model_from_checkpoint(CHECKPOINT_PATH, DEVICE)
+    print("\nModel info loaded successfully!")
diff --git a/scripts/local_analysis/multi_embedding.py b/scripts/local_analysis/multi_embedding.py
new file mode 100644
index 0000000..1a94728
--- /dev/null
+++ b/scripts/local_analysis/multi_embedding.py
@@ -0,0 +1,1534 @@
+#!/usr/bin/env python3
+"""
+Multi-Embedding Trajectory Analysis
+
+Compare trajectory dynamics across multiple embedding methods:
+- PCA (baseline linear projection)
+- Time-lagged PCA (sensitive to temporal structure)
+- Diffusion Maps (metastability-aware nonlinear embedding)
+- UMAP (visualization only - explicitly qualified)
+
+Theory-aligned with Rabinovich's Information Flow Phase Space (IFPS).
+Focus on FLOW GEOMETRY, not classification.
+
+Usage:
+    python multi_embedding.py                           # Default: first HC subject
+    python multi_embedding.py --subject S001            # Specific subject
+    python multi_embedding.py --n-chunks 10             # Use 10 consecutive chunks
+    python multi_embedding.py --compare-groups          # Compare HC vs MCI
+    python multi_embedding.py --embedding pca           # Single embedding
+    python multi_embedding.py --embedding diffusion     # Diffusion maps only
+    python multi_embedding.py --list-subjects           # List available subjects
+"""
+
+import argparse
+import sys
+import json
+from pathlib import Path
+from dataclasses import dataclass
+from datetime import datetime
+
+import numpy as np
+import matplotlib.pyplot as plt
+from matplotlib.collections import LineCollection
+from matplotlib.colors import Normalize
+from scipy.ndimage import gaussian_filter
+from scipy.spatial.distance import pdist, squareform
+from scipy.stats import spearmanr, entropy
+from sklearn.decomposition import PCA
+
+# Add parent to path
+sys.path.insert(0, str(Path(__file__).parent))
+
+from config import (
+    CHECKPOINT_PATH, DATA_DIR, OUTPUT_DIR, DEVICE,
+    FILTER_LOW, FILTER_HIGH, CHUNK_DURATION, SFREQ,
+    ensure_output_dir, get_fif_files, get_subjects_by_group
+)
+from load_model import load_model_from_checkpoint, create_model, compute_latent_trajectory
+from load_data import load_and_preprocess_fif
+
+# Optional imports
+try:
+    import umap
+    HAS_UMAP = True
+except ImportError:
+    HAS_UMAP = False
+
+GROUP_COLORS = {0: "blue", 1: "orange", 2: "red"}
+GROUP_NAMES = {0: "HC", 1: "MCI", 2: "AD"}
+
+
+def create_timestamped_output_dir(base_dir: Path, script_name: str) -> Path:
+    """Create a timestamped output directory for versioned results."""
+    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+    output_dir = base_dir / f"{script_name}_{timestamp}"
+    output_dir.mkdir(parents=True, exist_ok=True)
+    return output_dir
+
+
+def save_parameters(output_dir: Path, params: dict):
+    """Save parameters to a JSON file for reproducibility."""
+    params_path = output_dir / "parameters.json"
+
+    # Convert Path objects to strings
+    serializable_params = {}
+    for k, v in params.items():
+        if isinstance(v, Path):
+            serializable_params[k] = str(v)
+        else:
+            serializable_params[k] = v
+
+    serializable_params["timestamp"] = datetime.now().isoformat()
+
+    with open(params_path, 'w') as f:
+        json.dump(serializable_params, f, indent=2)
+    print(f"Parameters saved to: {params_path}")
+
+
+@dataclass
+class FlowMetrics:
+    """Flow geometry metrics for a trajectory."""
+    mean_speed: float
+    speed_std: float
+    speed_cv: float
+    n_dwell_episodes: int
+    total_dwell_time: int
+    mean_dwell_duration: float
+    occupancy_entropy: float
+    path_tortuosity: float
+    explored_variance: float
+
+
+# =============================================================================
+# EMBEDDING METHODS
+# =============================================================================
+
+def pca_embedding(z: np.ndarray, n_components: int = 2) -> tuple[np.ndarray, dict]:
+    """
+    Standard PCA embedding.
+
+    Returns:
+        embedded: (T, n_components) array
+        info: dict with explained variance etc.
+    """
+    pca = PCA(n_components=n_components)
+    embedded = pca.fit_transform(z)
+
+    return embedded, {
+        "method": "PCA",
+        "var_explained": pca.explained_variance_ratio_.tolist(),
+        "total_var": sum(pca.explained_variance_ratio_),
+    }
+
+
+def time_lagged_pca(z: np.ndarray, tau: int = 5, n_components: int = 2) -> tuple[np.ndarray, dict]:
+    """
+    Time-lagged PCA: PCA on [z(t), z(t+τ)] pairs.
+
+    Sensitive to temporal structure, reveals directions of maximal
+    predictive change rather than static variance.
+
+    Args:
+        z: (T, D) latent trajectory
+        tau: time lag in samples
+        n_components: output dimensions
+
+    Returns:
+        embedded: (T-tau, n_components) array
+        info: dict with metadata
+    """
+    T, D = z.shape
+
+    # Create time-lagged pairs: [z(t), z(t+τ)]
+    z_lagged = np.hstack([z[:-tau], z[tau:]])  # (T-tau, 2D)
+
+    pca = PCA(n_components=n_components)
+    embedded = pca.fit_transform(z_lagged)
+
+    return embedded, {
+        "method": f"tPCA (τ={tau})",
+        "tau": tau,
+        "var_explained": pca.explained_variance_ratio_.tolist(),
+        "total_var": sum(pca.explained_variance_ratio_),
+    }
+
+
+def diffusion_maps(
+    z: np.ndarray,
+    n_components: int = 2,
+    k: int = 10,
+    epsilon: str = "median",
+) -> tuple[np.ndarray, dict]:
+    """
+    Diffusion maps embedding.
+
+    Uncovers slow collective variables, directly connected to metastability.
+    Embedding organized by transition probabilities.
+
+    Args:
+        z: (T, D) latent trajectory
+        n_components: output dimensions
+        k: number of nearest neighbors for kernel
+        epsilon: kernel bandwidth ('median' or float)
+
+    Returns:
+        embedded: (T, n_components) array
+        info: dict with metadata
+    """
+    T = z.shape[0]
+
+    # Subsample if too large (diffusion maps is O(n²))
+    max_points = 2000
+    if T > max_points:
+        indices = np.linspace(0, T - 1, max_points, dtype=int)
+        z_sub = z[indices]
+    else:
+        indices = np.arange(T)
+        z_sub = z
+
+    # Compute pairwise distances
+    distances = squareform(pdist(z_sub, metric='euclidean'))
+
+    # Determine epsilon (kernel bandwidth)
+    if epsilon == "median":
+        # Use median of k-nearest neighbor distances
+        knn_dists = np.sort(distances, axis=1)[:, 1:k+1]
+        eps = np.median(knn_dists)
+    else:
+        eps = float(epsilon)
+
+    # Gaussian kernel
+    K = np.exp(-distances**2 / (2 * eps**2))
+
+    # Normalize to get transition matrix
+    D_inv = np.diag(1.0 / K.sum(axis=1))
+    P = D_inv @ K
+
+    # Eigendecomposition
+    eigenvalues, eigenvectors = np.linalg.eig(P)
+
+    # Sort by eigenvalue magnitude (skip first which is trivial)
+    idx = np.argsort(np.abs(eigenvalues))[::-1]
+    eigenvalues = eigenvalues[idx].real
+    eigenvectors = eigenvectors[:, idx].real
+
+    # Take components 1 to n_components+1 (skip trivial)
+    embedded_sub = eigenvectors[:, 1:n_components+1]
+
+    # If subsampled, interpolate back (simple nearest neighbor)
+    if T > max_points:
+        from scipy.interpolate import interp1d
+        f = interp1d(indices, embedded_sub, axis=0, kind='linear', fill_value='extrapolate')
+        embedded = f(np.arange(T))
+    else:
+        embedded = embedded_sub
+
+    return embedded, {
+        "method": f"Diffusion Maps (k={k}, ε={eps:.3f})",
+        "epsilon": eps,
+        "k": k,
+        "eigenvalues": eigenvalues[1:n_components+2].tolist(),
+    }
+
+
+def umap_embedding(
+    z: np.ndarray,
+    n_components: int = 2,
+    n_neighbors: int = 30,
+    min_dist: float = 0.2,
+) -> tuple[np.ndarray, dict]:
+    """
+    UMAP embedding (visualization only - explicitly qualified).
+
+    Use for global manifold shape visualization, NOT for flow vectors
+    or quantitative metrics.
+
+    Args:
+        z: (T, D) latent trajectory
+        n_components: output dimensions
+        n_neighbors: UMAP parameter
+        min_dist: UMAP parameter
+
+    Returns:
+        embedded: (T, n_components) array
+        info: dict with metadata
+    """
+    if not HAS_UMAP:
+        # Fallback to PCA
+        print("  WARNING: UMAP not available, falling back to PCA")
+        return pca_embedding(z, n_components)
+
+    reducer = umap.UMAP(
+        n_components=n_components,
+        n_neighbors=n_neighbors,
+        min_dist=min_dist,
+        metric='euclidean',
+        random_state=42,
+    )
+    embedded = reducer.fit_transform(z)
+
+    return embedded, {
+        "method": f"UMAP (nn={n_neighbors}, md={min_dist})",
+        "n_neighbors": n_neighbors,
+        "min_dist": min_dist,
+        "note": "Visualization only - do not use for flow quantification",
+    }
+
+
+def delay_embedding(
+    z: np.ndarray,
+    tau: int = 5,
+    dim: int = 3,
+    n_components: int = 2,
+) -> tuple[np.ndarray, dict]:
+    """
+    Takens-style delay embedding followed by PCA.
+
+    Creates [z(t), z(t+τ), z(t+2τ), ...] then reduces with PCA.
+    Theoretically grounded, makes recurrence structure explicit.
+
+    Args:
+        z: (T, D) latent trajectory
+        tau: delay in samples
+        dim: embedding dimension (number of delays)
+        n_components: output dimensions after PCA
+
+    Returns:
+        embedded: (T - (dim-1)*tau, n_components) array
+        info: dict with metadata
+    """
+    T, D = z.shape
+    n_valid = T - (dim - 1) * tau
+
+    if n_valid < 100:
+        raise ValueError(f"Not enough points for delay embedding: {n_valid}")
+
+    # Create delay vectors
+    delayed = np.zeros((n_valid, D * dim))
+    for i in range(dim):
+        delayed[:, i*D:(i+1)*D] = z[i*tau:i*tau + n_valid]
+
+    # PCA on delayed vectors
+    pca = PCA(n_components=n_components)
+    embedded = pca.fit_transform(delayed)
+
+    return embedded, {
+        "method": f"Delay (τ={tau}, d={dim})",
+        "tau": tau,
+        "dim": dim,
+        "var_explained": pca.explained_variance_ratio_.tolist(),
+    }
+
+
+# =============================================================================
+# FLOW METRICS
+# =============================================================================
+
+def compute_instantaneous_speed(embedded: np.ndarray) -> np.ndarray:
+    """Compute speed in embedding space."""
+    diff = np.diff(embedded, axis=0)
+    return np.linalg.norm(diff, axis=1)
+
+
+def detect_dwell_episodes(
+    speed: np.ndarray,
+    threshold_percentile: float = 20,
+    min_duration: int = 10,
+) -> list[tuple[int, int]]:
+    """Detect contiguous low-speed runs."""
+    threshold = np.percentile(speed, threshold_percentile)
+    is_slow = speed < threshold
+
+    episodes = []
+    in_episode = False
+    start = 0
+
+    for i, slow in enumerate(is_slow):
+        if slow and not in_episode:
+            in_episode = True
+            start = i
+        elif not slow and in_episode:
+            in_episode = False
+            if i - start >= min_duration:
+                episodes.append((start, i))
+
+    if in_episode and len(is_slow) - start >= min_duration:
+        episodes.append((start, len(is_slow)))
+
+    return episodes
+
+
+def compute_occupancy_entropy(embedded: np.ndarray, bins: int = 20) -> float:
+    """
+    Compute entropy of occupancy distribution.
+
+    Higher entropy = more uniform exploration.
+    Lower entropy = concentrated in few regions.
+    """
+    H, _, _ = np.histogram2d(embedded[:, 0], embedded[:, 1], bins=bins)
+    H = H.flatten()
+    H = H[H > 0]  # Remove empty bins
+    p = H / H.sum()
+    return entropy(p)
+
+
+def compute_flow_metrics(embedded: np.ndarray) -> FlowMetrics:
+    """Compute comprehensive flow geometry metrics."""
+    speed = compute_instantaneous_speed(embedded)
+    episodes = detect_dwell_episodes(speed)
+
+    # Path metrics
+    path_length = speed.sum()
+    displacement = np.linalg.norm(embedded[-1] - embedded[0])
+    tortuosity = path_length / displacement if displacement > 0 else np.inf
+
+    # Explored variance
+    explored_variance = np.var(embedded, axis=0).sum()
+
+    # Occupancy entropy
+    occ_entropy = compute_occupancy_entropy(embedded)
+
+    return FlowMetrics(
+        mean_speed=speed.mean(),
+        speed_std=speed.std(),
+        speed_cv=speed.std() / speed.mean() if speed.mean() > 0 else 0,
+        n_dwell_episodes=len(episodes),
+        total_dwell_time=sum(e[1] - e[0] for e in episodes),
+        mean_dwell_duration=np.mean([e[1] - e[0] for e in episodes]) if episodes else 0,
+        occupancy_entropy=occ_entropy,
+        path_tortuosity=tortuosity,
+        explored_variance=explored_variance,
+    )
+
+
+# =============================================================================
+# VISUALIZATION
+# =============================================================================
+
+def plot_single_embedding(
+    embedded: np.ndarray,
+    info: dict,
+    ax: plt.Axes,
+    color_by: str = "time",
+    speed: np.ndarray = None,
+):
+    """Plot trajectory in a single embedding."""
+    T = len(embedded)
+
+    if color_by == "time":
+        colors = np.linspace(0, 1, T)
+        cmap = 'viridis'
+        label = 'Time'
+    elif color_by == "speed" and speed is not None:
+        colors = speed
+        colors = np.concatenate([colors, [colors[-1]]])  # Pad to match T
+        cmap = 'coolwarm_r'
+        label = 'Speed'
+    else:
+        colors = np.linspace(0, 1, T)
+        cmap = 'viridis'
+        label = 'Time'
+
+    # Line collection for colored trajectory
+    points = embedded.reshape(-1, 1, 2)
+    segments = np.concatenate([points[:-1], points[1:]], axis=1)
+
+    norm = Normalize(vmin=colors.min(), vmax=colors.max())
+    lc = LineCollection(segments, cmap=cmap, norm=norm, linewidth=0.5, alpha=0.7)
+    lc.set_array(colors[:-1])
+    ax.add_collection(lc)
+
+    ax.autoscale()
+    ax.set_title(info["method"], fontsize=10, fontweight='bold')
+
+    # Variance info if available (only for PCA-based methods)
+    if "total_var" in info:
+        var_str = f"Var: {info['total_var']*100:.1f}%"
+        ax.text(0.02, 0.98, var_str, transform=ax.transAxes, fontsize=8,
+                va='top', ha='left', color='gray')
+
+
+def plot_density_overlay(embedded: np.ndarray, ax: plt.Axes, bins: int = 30):
+    """Add density heatmap to axis."""
+    H, xedges, yedges = np.histogram2d(embedded[:, 0], embedded[:, 1], bins=bins)
+    H = gaussian_filter(H.T, sigma=1)
+
+    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]
+    ax.imshow(H, origin='lower', extent=extent, cmap='hot', aspect='equal', alpha=0.6)
+
+
+def plot_multi_embedding_comparison(
+    latent: np.ndarray,
+    output_dir: Path,
+    subject_id: str,
+    group_name: str,
+    show_plot: bool = True,
+):
+    """
+    Generate comprehensive multi-embedding comparison plot.
+    """
+    print(f"\nComputing embeddings for {subject_id} ({group_name})...")
+
+    # Compute all embeddings
+    embeddings = {}
+
+    print("  PCA...")
+    embeddings["PCA"] = pca_embedding(latent)
+
+    print("  Time-lagged PCA (τ=5)...")
+    embeddings["tPCA"] = time_lagged_pca(latent, tau=5)
+
+    print("  Diffusion Maps...")
+    try:
+        embeddings["Diffusion"] = diffusion_maps(latent, k=10)
+    except Exception as e:
+        print(f"    Failed: {e}")
+        embeddings["Diffusion"] = pca_embedding(latent)  # Fallback
+
+    print("  Delay Embedding (τ=5, d=3)...")
+    try:
+        embeddings["Delay"] = delay_embedding(latent, tau=5, dim=3)
+    except Exception as e:
+        print(f"    Failed: {e}")
+        embeddings["Delay"] = pca_embedding(latent)
+
+    if HAS_UMAP:
+        print("  UMAP (visualization only)...")
+        embeddings["UMAP"] = umap_embedding(latent)
+
+    # Compute metrics for each
+    metrics = {}
+    for name, (emb, info) in embeddings.items():
+        metrics[name] = compute_flow_metrics(emb)
+
+    # Create figure
+    n_emb = len(embeddings)
+    fig, axes = plt.subplots(2, n_emb, figsize=(5 * n_emb, 10))
+
+    for col, (name, (emb, info)) in enumerate(embeddings.items()):
+        speed = compute_instantaneous_speed(emb)
+
+        # Top row: time-colored trajectory
+        plot_single_embedding(emb, info, axes[0, col], color_by="time")
+
+        # Bottom row: density + trajectory
+        plot_density_overlay(emb, axes[1, col])
+        plot_single_embedding(emb, info, axes[1, col], color_by="speed", speed=speed)
+        axes[1, col].set_title(f"{name}: Density + Speed", fontsize=10)
+
+    plt.suptitle(
+        f"Multi-Embedding Comparison: {subject_id} ({group_name})\n"
+        "Top: Time-colored | Bottom: Density + Speed-colored",
+        fontsize=14, fontweight='bold'
+    )
+    plt.tight_layout()
+
+    save_path = output_dir / f"multi_embedding_{subject_id}.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    # Print metrics comparison
+    print("\n" + "=" * 80)
+    print("FLOW METRICS COMPARISON")
+    print("=" * 80)
+    print(f"{'Metric':<20} " + " ".join([f"{name:>12}" for name in embeddings.keys()]))
+    print("-" * 80)
+
+    metric_names = [
+        ("mean_speed", "Mean Speed"),
+        ("speed_cv", "Speed CV"),
+        ("n_dwell_episodes", "Dwell Episodes"),
+        ("occupancy_entropy", "Occ. Entropy"),
+        ("path_tortuosity", "Tortuosity"),
+        ("explored_variance", "Explored Var"),
+    ]
+
+    for attr, label in metric_names:
+        values = [f"{getattr(metrics[name], attr):>12.3f}" for name in embeddings.keys()]
+        print(f"{label:<20} " + " ".join(values))
+
+    # Highlight occupancy entropy consistency across embeddings
+    occ_vals = [metrics[name].occupancy_entropy for name in embeddings.keys()]
+    occ_cv = np.std(occ_vals) / np.mean(occ_vals) if np.mean(occ_vals) > 0 else 0
+
+    print("\n" + "-" * 80)
+    print("OCCUPANCY ENTROPY CONSISTENCY (across embeddings)")
+    print(f"  Mean: {np.mean(occ_vals):.3f}, Std: {np.std(occ_vals):.3f}, CV: {occ_cv:.3f}")
+    if occ_cv < 0.15:
+        print("  → HIGH consistency: Occupancy entropy robust across methods")
+    elif occ_cv < 0.30:
+        print("  → MODERATE consistency: Some method-dependent variation")
+    else:
+        print("  → LOW consistency: Occupancy entropy varies by embedding method")
+
+    return embeddings, metrics
+
+
+def compute_cross_embedding_consistency(
+    embeddings: dict,
+) -> dict:
+    """
+    Quantify consistency of flow metrics across embeddings.
+
+    If patterns persist across embeddings → robust dynamical signature.
+    """
+    embedding_names = list(embeddings.keys())
+    n_emb = len(embedding_names)
+
+    # Compute speed for each
+    speeds = {}
+    for name, (emb, _) in embeddings.items():
+        speeds[name] = compute_instantaneous_speed(emb)
+
+    # Compute rank correlations of speed
+    speed_correlations = np.zeros((n_emb, n_emb))
+    for i, name1 in enumerate(embedding_names):
+        for j, name2 in enumerate(embedding_names):
+            # Align lengths (they may differ due to lag)
+            len1, len2 = len(speeds[name1]), len(speeds[name2])
+            min_len = min(len1, len2)
+            corr, _ = spearmanr(speeds[name1][:min_len], speeds[name2][:min_len])
+            speed_correlations[i, j] = corr
+
+    return {
+        "speed_correlation_matrix": speed_correlations,
+        "embedding_names": embedding_names,
+        "mean_cross_correlation": np.mean(speed_correlations[np.triu_indices(n_emb, k=1)]),
+    }
+
+
+def plot_consistency_analysis(
+    embeddings: dict,
+    output_dir: Path,
+    subject_id: str,
+    show_plot: bool = True,
+):
+    """Plot cross-embedding consistency analysis."""
+    consistency = compute_cross_embedding_consistency(embeddings)
+
+    fig, axes = plt.subplots(1, 2, figsize=(14, 6))
+
+    # Left: Correlation matrix
+    ax = axes[0]
+    im = ax.imshow(consistency["speed_correlation_matrix"], cmap='RdYlGn', vmin=-1, vmax=1)
+    ax.set_xticks(range(len(consistency["embedding_names"])))
+    ax.set_yticks(range(len(consistency["embedding_names"])))
+    ax.set_xticklabels(consistency["embedding_names"], rotation=45, ha='right')
+    ax.set_yticklabels(consistency["embedding_names"])
+    ax.set_title("Speed Rank Correlation Across Embeddings", fontsize=12, fontweight='bold')
+    plt.colorbar(im, ax=ax, label='Spearman ρ')
+
+    # Add correlation values as text
+    for i in range(len(consistency["embedding_names"])):
+        for j in range(len(consistency["embedding_names"])):
+            ax.text(j, i, f'{consistency["speed_correlation_matrix"][i, j]:.2f}',
+                    ha='center', va='center', fontsize=9)
+
+    # Right: Interpretation
+    ax = axes[1]
+    ax.axis('off')
+
+    mean_corr = consistency["mean_cross_correlation"]
+
+    interpretation = f"""
+CROSS-EMBEDDING CONSISTENCY ANALYSIS
+=====================================
+
+Subject: {subject_id}
+Mean Cross-Correlation: {mean_corr:.3f}
+
+INTERPRETATION:
+---------------
+"""
+    if mean_corr > 0.7:
+        interpretation += """
+HIGH CONSISTENCY (ρ > 0.7)
+✓ Flow patterns are ROBUST across embedding methods
+✓ Dynamical features are not artifacts of projection
+✓ Strong evidence for intrinsic structure
+"""
+    elif mean_corr > 0.4:
+        interpretation += """
+MODERATE CONSISTENCY (0.4 < ρ < 0.7)
+~ Some patterns persist across embeddings
+~ Partial support for intrinsic structure
+~ Consider which embeddings agree/disagree
+"""
+    else:
+        interpretation += """
+LOW CONSISTENCY (ρ < 0.4)
+⚠ Flow patterns differ across embeddings
+⚠ Results may be projection-dependent
+⚠ Interpret with caution
+"""
+
+    interpretation += """
+
+NOTE: High consistency is necessary (not sufficient)
+for claiming robust dynamical signatures.
+"""
+
+    ax.text(0.1, 0.9, interpretation, transform=ax.transAxes, fontsize=10,
+            verticalalignment='top', fontfamily='monospace',
+            bbox=dict(boxstyle='round', facecolor='lightyellow', alpha=0.8))
+
+    plt.suptitle(f"Consistency Analysis: {subject_id}", fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / f"embedding_consistency_{subject_id}.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return consistency
+
+
+def compare_groups(
+    model,
+    model_info: dict,
+    groups: dict,
+    n_subjects: int,
+    n_chunks: int,
+    output_dir: Path,
+    show_plot: bool = True,
+):
+    """Compare flow metrics between HC and MCI across embeddings."""
+    print("\n" + "=" * 80)
+    print("GROUP COMPARISON: HC vs MCI")
+    print("=" * 80)
+
+    group_metrics = {"HC": [], "MCI": []}
+
+    for group_key in ["hc", "mci"]:
+        subjects = groups.get(group_key, [])
+        if not subjects:
+            continue
+
+        group_name = "HC" if group_key == "hc" else "MCI"
+        print(f"\nProcessing {group_name} subjects...")
+
+        subjects_processed = 0
+        for fif_path, label, condition, subject_id in subjects:
+            if subjects_processed >= n_subjects:
+                break
+
+            print(f"  {subject_id}...", end=" ")
+
+            # Load data
+            data = load_and_preprocess_fif(
+                fif_path, FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+                include_amplitude=model_info["include_amplitude"],
+                verbose=False,
+            )
+
+            if len(data["chunks"]) == 0:
+                print("no chunks, skip")
+                continue
+
+            # Extract latent
+            chunks_to_use = min(n_chunks, len(data["chunks"]))
+            latents = []
+            for cidx in range(chunks_to_use):
+                latent = compute_latent_trajectory(model, data["chunks"][cidx], DEVICE)
+                latents.append(latent)
+            latent = np.concatenate(latents, axis=0)
+
+            # Compute PCA embedding and metrics (use PCA for group comparison - most stable)
+            emb, _ = pca_embedding(latent)
+            metrics = compute_flow_metrics(emb)
+            group_metrics[group_name].append(metrics)
+
+            print(f"speed_cv={metrics.speed_cv:.3f}")
+            subjects_processed += 1
+
+    # Create comparison plot
+    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
+
+    metric_pairs = [
+        ("mean_speed", "Mean Speed"),
+        ("speed_cv", "Speed CV"),
+        ("n_dwell_episodes", "Dwell Episodes"),
+        ("occupancy_entropy", "Occupancy Entropy"),
+        ("path_tortuosity", "Tortuosity"),
+        ("explored_variance", "Explored Variance"),
+    ]
+
+    for idx, (attr, label) in enumerate(metric_pairs):
+        ax = axes.flatten()[idx]
+
+        hc_vals = [getattr(m, attr) for m in group_metrics["HC"]]
+        mci_vals = [getattr(m, attr) for m in group_metrics["MCI"]]
+
+        # Violin plot
+        parts = ax.violinplot([hc_vals, mci_vals], positions=[0, 1], showmeans=True)
+        parts['bodies'][0].set_facecolor('blue')
+        parts['bodies'][0].set_alpha(0.7)
+        parts['bodies'][1].set_facecolor('orange')
+        parts['bodies'][1].set_alpha(0.7)
+
+        # Individual points
+        ax.scatter(np.zeros(len(hc_vals)) + np.random.uniform(-0.1, 0.1, len(hc_vals)),
+                   hc_vals, c='blue', alpha=0.6, s=30, edgecolors='black', linewidths=0.5)
+        ax.scatter(np.ones(len(mci_vals)) + np.random.uniform(-0.1, 0.1, len(mci_vals)),
+                   mci_vals, c='orange', alpha=0.6, s=30, edgecolors='black', linewidths=0.5)
+
+        ax.set_xticks([0, 1])
+        ax.set_xticklabels(['HC', 'MCI'])
+        ax.set_ylabel(label)
+        ax.set_title(label, fontweight='bold')
+
+        # Add means
+        hc_mean, mci_mean = np.mean(hc_vals), np.mean(mci_vals)
+        ax.axhline(hc_mean, color='blue', linestyle='--', alpha=0.5, xmin=0.1, xmax=0.4)
+        ax.axhline(mci_mean, color='orange', linestyle='--', alpha=0.5, xmin=0.6, xmax=0.9)
+
+    plt.suptitle(
+        "Flow Geometry Metrics: HC vs MCI\n"
+        "(PCA embedding, multiple subjects)",
+        fontsize=14, fontweight='bold'
+    )
+    plt.tight_layout()
+
+    save_path = output_dir / "group_comparison_flow_metrics.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"\nSaved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    # Print summary
+    print("\n" + "=" * 80)
+    print("GROUP SUMMARY (Mean ± Std)")
+    print("=" * 80)
+    print(f"{'Metric':<25} {'HC':>20} {'MCI':>20}")
+    print("-" * 80)
+
+    for attr, label in metric_pairs:
+        hc_vals = [getattr(m, attr) for m in group_metrics["HC"]]
+        mci_vals = [getattr(m, attr) for m in group_metrics["MCI"]]
+        print(f"{label:<25} {np.mean(hc_vals):>8.3f} ± {np.std(hc_vals):<8.3f} "
+              f"{np.mean(mci_vals):>8.3f} ± {np.std(mci_vals):<8.3f}")
+
+    # Highlight occupancy entropy interpretation (key metric per critic agent)
+    hc_occ = [m.occupancy_entropy for m in group_metrics["HC"]]
+    mci_occ = [m.occupancy_entropy for m in group_metrics["MCI"]]
+    occ_diff = np.mean(hc_occ) - np.mean(mci_occ)
+
+    print("\n" + "=" * 80)
+    print("KEY METRIC: OCCUPANCY ENTROPY")
+    print("=" * 80)
+    print(f"  HC mean:  {np.mean(hc_occ):.3f} (higher = more uniform exploration)")
+    print(f"  MCI mean: {np.mean(mci_occ):.3f}")
+    print(f"  Δ (HC - MCI): {occ_diff:+.3f}")
+    print()
+    if occ_diff > 0:
+        print("  → HC explores latent space more uniformly")
+        print("  → MCI shows concentrated/trapped dynamics (supercriticality)")
+    else:
+        print("  → Unexpected: MCI shows more uniform exploration")
+        print("  → May indicate model not capturing expected dynamics")
+
+    return group_metrics
+
+
+def extract_continuous_latent(
+    model,
+    fif_path: Path,
+    model_info: dict,
+    n_chunks: int,
+) -> np.ndarray:
+    """Extract continuous latent trajectory from consecutive chunks."""
+    data = load_and_preprocess_fif(
+        fif_path, FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+        verbose=False,
+    )
+
+    if len(data["chunks"]) == 0:
+        return None
+
+    chunks_to_use = min(n_chunks, len(data["chunks"]))
+    latents = []
+
+    for cidx in range(chunks_to_use):
+        latent = compute_latent_trajectory(model, data["chunks"][cidx], DEVICE)
+        latents.append(latent)
+
+    return np.concatenate(latents, axis=0)
+
+
+# =============================================================================
+# COMPARE-ALL MODE: Pooled embedding with per-class density aggregation
+# =============================================================================
+
+class PooledEmbedder:
+    """
+    Fits embedding on pooled data, then transforms individual trajectories.
+    Ensures all subjects are in the same 2D space for density comparison.
+    """
+
+    def __init__(self, method: str = "pca", tau: int = 5, delay_dim: int = 3):
+        self.method = method
+        self.tau = tau
+        self.delay_dim = delay_dim
+        self.transformer = None
+        self.bounds = None  # (xmin, xmax, ymin, ymax) for shared grid
+
+    def _preprocess_for_method(self, z: np.ndarray) -> np.ndarray:
+        """Preprocess trajectory based on embedding method."""
+        if self.method == "tpca":
+            # Time-lagged: [z(t), z(t+τ)]
+            return np.hstack([z[:-self.tau], z[self.tau:]])
+        elif self.method == "delay":
+            # Takens delay embedding
+            T, D = z.shape
+            n_valid = T - (self.delay_dim - 1) * self.tau
+            if n_valid < 10:
+                return z  # Fallback
+            delayed = np.zeros((n_valid, D * self.delay_dim))
+            for d in range(self.delay_dim):
+                start = d * self.tau
+                end = start + n_valid
+                delayed[:, d * D:(d + 1) * D] = z[start:end]
+            return delayed
+        else:
+            return z
+
+    def fit(self, trajectories: list[np.ndarray], n_samples_per_traj: int = 500):
+        """
+        Fit embedding on pooled data sampled equally from each trajectory.
+
+        Args:
+            trajectories: List of (T_i, D) arrays
+            n_samples_per_traj: Samples to take from each trajectory
+        """
+        # Sample equally from each trajectory
+        pooled = []
+        for traj in trajectories:
+            processed = self._preprocess_for_method(traj)
+            if len(processed) <= n_samples_per_traj:
+                pooled.append(processed)
+            else:
+                indices = np.linspace(0, len(processed) - 1, n_samples_per_traj, dtype=int)
+                pooled.append(processed[indices])
+
+        pooled_data = np.vstack(pooled)
+        print(f"  Pooled {len(trajectories)} trajectories → {pooled_data.shape[0]} points")
+
+        # Fit transformer
+        if self.method in ["pca", "tpca", "delay"]:
+            self.transformer = PCA(n_components=2)
+            embedded = self.transformer.fit_transform(pooled_data)
+        elif self.method == "diffusion":
+            # Diffusion maps: compute on pooled, store eigenvectors
+            self._fit_diffusion_maps(pooled_data)
+            embedded = self._transform_diffusion(pooled_data)
+        elif self.method == "umap":
+            if HAS_UMAP:
+                self.transformer = umap.UMAP(
+                    n_components=2,
+                    n_neighbors=30,
+                    min_dist=0.2,
+                    metric='euclidean',
+                    random_state=42,  # Fixed seed
+                )
+                embedded = self.transformer.fit_transform(pooled_data)
+            else:
+                self.transformer = PCA(n_components=2)
+                embedded = self.transformer.fit_transform(pooled_data)
+
+        # Store bounds for shared grid
+        margin = 0.05
+        x_range = embedded[:, 0].max() - embedded[:, 0].min()
+        y_range = embedded[:, 1].max() - embedded[:, 1].min()
+        self.bounds = (
+            embedded[:, 0].min() - margin * x_range,
+            embedded[:, 0].max() + margin * x_range,
+            embedded[:, 1].min() - margin * y_range,
+            embedded[:, 1].max() + margin * y_range,
+        )
+
+    def _fit_diffusion_maps(self, data: np.ndarray, k: int = 10):
+        """Fit diffusion maps on pooled data."""
+        # Subsample if needed
+        max_points = 2000
+        if len(data) > max_points:
+            indices = np.linspace(0, len(data) - 1, max_points, dtype=int)
+            data_sub = data[indices]
+        else:
+            data_sub = data
+
+        distances = squareform(pdist(data_sub, metric='euclidean'))
+        knn_dists = np.sort(distances, axis=1)[:, 1:k+1]
+        self.dm_epsilon = np.median(knn_dists)
+
+        K = np.exp(-distances**2 / (2 * self.dm_epsilon**2))
+        D_inv = np.diag(1.0 / K.sum(axis=1))
+        P = D_inv @ K
+
+        eigenvalues, eigenvectors = np.linalg.eig(P)
+        idx = np.argsort(np.abs(eigenvalues))[::-1]
+        self.dm_eigenvalues = eigenvalues[idx].real
+        self.dm_eigenvectors = eigenvectors[:, idx].real
+        self.dm_fit_data = data_sub
+
+    def _transform_diffusion(self, data: np.ndarray) -> np.ndarray:
+        """Transform new data using fitted diffusion maps (Nyström extension)."""
+        # Compute kernel between new points and fit points
+        from scipy.spatial.distance import cdist
+        distances = cdist(data, self.dm_fit_data, metric='euclidean')
+        K = np.exp(-distances**2 / (2 * self.dm_epsilon**2))
+        K_normalized = K / K.sum(axis=1, keepdims=True)
+
+        # Project onto eigenvectors
+        embedded = K_normalized @ self.dm_eigenvectors[:, 1:3]
+        return embedded
+
+    def transform(self, trajectory: np.ndarray) -> np.ndarray:
+        """Transform a single trajectory into the shared 2D space."""
+        processed = self._preprocess_for_method(trajectory)
+
+        if self.method in ["pca", "tpca", "delay"]:
+            return self.transformer.transform(processed)
+        elif self.method == "diffusion":
+            return self._transform_diffusion(processed)
+        elif self.method == "umap":
+            if HAS_UMAP and hasattr(self.transformer, 'transform'):
+                return self.transformer.transform(processed)
+            else:
+                return self.transformer.transform(processed)
+
+    def get_method_name(self) -> str:
+        """Get display name for the method."""
+        names = {
+            "pca": "PCA",
+            "tpca": f"tPCA (τ={self.tau})",
+            "diffusion": "Diffusion Maps",
+            "delay": f"Delay (τ={self.tau}, d={self.delay_dim})",
+            "umap": "UMAP",
+        }
+        return names.get(self.method, self.method.upper())
+
+
+def compute_density_on_grid(
+    embedded: np.ndarray,
+    bounds: tuple,
+    bins: int = 50,
+    sigma: float = 1.5,
+) -> np.ndarray:
+    """
+    Compute normalized 2D density on a shared grid.
+
+    Args:
+        embedded: (T, 2) trajectory in shared space
+        bounds: (xmin, xmax, ymin, ymax)
+        bins: Grid resolution
+        sigma: Gaussian smoothing sigma
+
+    Returns:
+        (bins, bins) density array, normalized to sum=1
+    """
+    xmin, xmax, ymin, ymax = bounds
+    H, _, _ = np.histogram2d(
+        embedded[:, 0], embedded[:, 1],
+        bins=bins,
+        range=[[xmin, xmax], [ymin, ymax]]
+    )
+    H = gaussian_filter(H.T, sigma=sigma)
+
+    # Normalize to probability distribution
+    if H.sum() > 0:
+        H = H / H.sum()
+
+    return H
+
+
+def compare_all_groups(
+    model,
+    model_info: dict,
+    groups: dict,
+    n_subjects_per_group: int,
+    n_chunks: int,
+    output_dir: Path,
+    show_plot: bool = True,
+    tau: int = 5,
+    delay_dim: int = 3,
+):
+    """
+    Compare phase-space density/flow across all groups using shared embeddings.
+
+    For each embedding method:
+    1. Pool trajectories from all subjects, fit embedding once
+    2. Transform each subject's trajectory into shared 2D space
+    3. Compute per-subject density on shared grid
+    4. Average densities within each class
+    5. Output density maps and difference maps
+    """
+    print("\n" + "=" * 80)
+    print("COMPARE-ALL MODE: Pooled Embedding with Per-Class Density Aggregation")
+    print("=" * 80)
+
+    # Collect trajectories by group
+    group_trajectories = {}  # group_key -> [(subject_id, trajectory), ...]
+    all_trajectories = []    # For pooled fitting
+
+    for group_key in ["hc", "mci", "ad"]:
+        subjects = groups.get(group_key, [])
+        if not subjects:
+            continue
+
+        group_name = GROUP_NAMES.get(subjects[0][1], group_key.upper())
+        group_trajectories[group_key] = []
+
+        print(f"\nExtracting {group_name} trajectories...")
+        subjects_processed = 0
+
+        for fif_path, label, condition, subject_id in subjects:
+            if subjects_processed >= n_subjects_per_group:
+                break
+
+            latent = extract_continuous_latent(model, fif_path, model_info, n_chunks)
+            if latent is None or len(latent) < 100:
+                print(f"  {subject_id}: skipped (insufficient data)")
+                continue
+
+            group_trajectories[group_key].append((subject_id, latent))
+            all_trajectories.append(latent)
+            print(f"  {subject_id}: {latent.shape[0]} timepoints")
+            subjects_processed += 1
+
+        print(f"  → {subjects_processed} subjects extracted")
+
+    if not all_trajectories:
+        print("No trajectories extracted!")
+        return
+
+    # Embedding methods to compare
+    methods = ["pca", "tpca", "diffusion", "delay"]
+    if HAS_UMAP:
+        methods.append("umap")
+
+    # Store results for summary
+    all_metrics = {method: {} for method in methods}  # method -> group -> [FlowMetrics]
+
+    for method in methods:
+        print(f"\n{'='*60}")
+        print(f"EMBEDDING: {method.upper()}")
+        print(f"{'='*60}")
+
+        # Create and fit pooled embedder
+        embedder = PooledEmbedder(method=method, tau=tau, delay_dim=delay_dim)
+        print(f"Fitting {embedder.get_method_name()} on pooled data...")
+        embedder.fit(all_trajectories)
+
+        # Transform each subject and compute density
+        group_densities = {}   # group_key -> [density arrays]
+        group_metrics_list = {}  # group_key -> [FlowMetrics]
+
+        bins = 50
+        for group_key, traj_list in group_trajectories.items():
+            if not traj_list:
+                continue
+
+            group_name = GROUP_NAMES.get(
+                groups[group_key][0][1] if groups.get(group_key) else 0,
+                group_key.upper()
+            )
+            print(f"\nProcessing {group_name}...")
+
+            group_densities[group_key] = []
+            group_metrics_list[group_key] = []
+
+            for subject_id, trajectory in traj_list:
+                # Transform to shared space
+                embedded = embedder.transform(trajectory)
+
+                # Compute density
+                density = compute_density_on_grid(embedded, embedder.bounds, bins=bins)
+                group_densities[group_key].append(density)
+
+                # Compute flow metrics
+                metrics = compute_flow_metrics(embedded)
+                group_metrics_list[group_key].append(metrics)
+
+                print(f"  {subject_id}: occ_entropy={metrics.occupancy_entropy:.3f}")
+
+            all_metrics[method][group_key] = group_metrics_list[group_key]
+
+        # Compute mean densities per class
+        mean_densities = {}
+        for group_key, densities in group_densities.items():
+            if densities:
+                mean_densities[group_key] = np.mean(densities, axis=0)
+
+        # Create figure: density maps + difference maps
+        active_groups = [k for k in ["hc", "mci", "ad"] if k in mean_densities]
+        n_groups = len(active_groups)
+        n_diffs = n_groups - 1 if "hc" in active_groups else 0
+
+        fig, axes = plt.subplots(2, max(n_groups, n_diffs + 1), figsize=(5 * max(n_groups, 3), 10))
+
+        extent = [embedder.bounds[0], embedder.bounds[1],
+                  embedder.bounds[2], embedder.bounds[3]]
+
+        # Row 1: Per-class densities
+        for col, group_key in enumerate(active_groups):
+            ax = axes[0, col] if n_groups > 1 else axes[0]
+            density = mean_densities[group_key]
+            group_name = GROUP_NAMES.get(
+                groups[group_key][0][1] if groups.get(group_key) else 0,
+                group_key.upper()
+            )
+
+            im = ax.imshow(density, origin='lower', extent=extent,
+                          cmap='hot', aspect='equal')
+            ax.set_title(f"{group_name} Mean Density\n(n={len(group_densities[group_key])})",
+                        fontweight='bold')
+            ax.set_xlabel("PC1" if "pca" in method else "Dim 1")
+            ax.set_ylabel("PC2" if "pca" in method else "Dim 2")
+            ax.set_aspect('equal')
+            plt.colorbar(im, ax=ax, label='Probability', shrink=0.8)
+
+        # Hide unused axes in row 1
+        for col in range(n_groups, axes.shape[1]):
+            axes[0, col].axis('off')
+
+        # Row 2: Difference maps (MCI-HC, AD-HC)
+        diff_idx = 0
+        if "hc" in mean_densities:
+            hc_density = mean_densities["hc"]
+
+            for group_key in ["mci", "ad"]:
+                if group_key not in mean_densities:
+                    continue
+
+                ax = axes[1, diff_idx] if axes.ndim > 1 else axes[1]
+                diff = mean_densities[group_key] - hc_density
+                group_name = GROUP_NAMES.get(
+                    groups[group_key][0][1] if groups.get(group_key) else 0,
+                    group_key.upper()
+                )
+
+                # Symmetric colormap for differences
+                vmax = np.abs(diff).max()
+                im = ax.imshow(diff, origin='lower', extent=extent,
+                              cmap='RdBu_r', vmin=-vmax, vmax=vmax, aspect='equal')
+                ax.set_title(f"{group_name} − HC\n(Difference Map)", fontweight='bold')
+                ax.set_xlabel("PC1" if "pca" in method else "Dim 1")
+                ax.set_ylabel("PC2" if "pca" in method else "Dim 2")
+                ax.set_aspect('equal')
+                cbar = plt.colorbar(im, ax=ax, shrink=0.8)
+                cbar.set_label('Δ Probability')
+
+                diff_idx += 1
+
+        # Hide unused axes in row 2
+        for col in range(diff_idx, axes.shape[1]):
+            axes[1, col].axis('off')
+
+        plt.suptitle(f"{embedder.get_method_name()}: Class-Averaged Densities",
+                    fontsize=14, fontweight='bold')
+        plt.tight_layout()
+
+        save_path = output_dir / f"compare_all_{method}_density.png"
+        fig.savefig(save_path, dpi=150, bbox_inches="tight")
+        print(f"\nSaved: {save_path}")
+
+        if show_plot:
+            plt.show()
+        else:
+            plt.close(fig)
+
+    # Output summary table of flow metrics by class
+    print("\n" + "=" * 80)
+    print("FLOW METRICS SUMMARY BY CLASS (Mean ± Std)")
+    print("=" * 80)
+
+    metric_attrs = [
+        ("mean_speed", "Mean Speed"),
+        ("speed_cv", "Speed CV"),
+        ("n_dwell_episodes", "Dwell Episodes"),
+        ("occupancy_entropy", "Occ. Entropy"),
+        ("path_tortuosity", "Tortuosity"),
+        ("explored_variance", "Explored Var"),
+    ]
+
+    for method in methods:
+        print(f"\n--- {method.upper()} ---")
+        print(f"{'Metric':<18} ", end="")
+
+        active_groups = [k for k in ["hc", "mci", "ad"] if k in all_metrics[method]]
+        for gk in active_groups:
+            gname = GROUP_NAMES.get(groups[gk][0][1], gk.upper())
+            print(f"{gname:>18} ", end="")
+        print()
+        print("-" * (18 + 19 * len(active_groups)))
+
+        for attr, label in metric_attrs:
+            print(f"{label:<18} ", end="")
+            for gk in active_groups:
+                vals = [getattr(m, attr) for m in all_metrics[method][gk]]
+                if vals:
+                    print(f"{np.mean(vals):>7.3f}±{np.std(vals):<7.3f} ", end="")
+                else:
+                    print(f"{'N/A':>18} ", end="")
+            print()
+
+    # Create combined flow metrics comparison figure
+    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
+
+    for idx, (attr, label) in enumerate(metric_attrs):
+        ax = axes.flatten()[idx]
+
+        # Collect data for each group across all methods
+        x_positions = np.arange(len(methods))
+        width = 0.25
+
+        for gi, gk in enumerate(["hc", "mci", "ad"]):
+            if gk not in group_trajectories:
+                continue
+
+            means = []
+            stds = []
+            for method in methods:
+                if gk in all_metrics[method]:
+                    vals = [getattr(m, attr) for m in all_metrics[method][gk]]
+                    means.append(np.mean(vals) if vals else 0)
+                    stds.append(np.std(vals) if vals else 0)
+                else:
+                    means.append(0)
+                    stds.append(0)
+
+            color = GROUP_COLORS.get(groups[gk][0][1] if groups.get(gk) else gi, 'gray')
+            gname = GROUP_NAMES.get(groups[gk][0][1] if groups.get(gk) else gi, gk.upper())
+            ax.bar(x_positions + gi * width, means, width, yerr=stds,
+                   label=gname, color=color, alpha=0.7, capsize=3)
+
+        ax.set_xlabel("Embedding Method")
+        ax.set_ylabel(label)
+        ax.set_title(label, fontweight='bold')
+        ax.set_xticks(x_positions + width)
+        ax.set_xticklabels([m.upper() for m in methods])
+        if idx == 0:
+            ax.legend()
+
+    plt.suptitle("Flow Metrics Across Embeddings and Groups", fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / "compare_all_flow_metrics.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"\nSaved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return all_metrics
+
+
+def list_subjects(groups: dict):
+    """List available subjects by group."""
+    print("\n" + "=" * 60)
+    print("AVAILABLE SUBJECTS")
+    print("=" * 60)
+
+    for group_key in ["hc", "mci", "ad"]:
+        subjects = groups.get(group_key, [])
+        if not subjects:
+            continue
+
+        group_name = GROUP_NAMES.get(subjects[0][1], group_key.upper())
+        print(f"\n{group_name} subjects ({len(subjects)}):")
+        for i, (fif_path, label, condition, subject_id) in enumerate(subjects):
+            print(f"  {i+1:3d}. {subject_id:10s} ({condition})")
+
+
+def main():
+    parser = argparse.ArgumentParser(
+        description="Multi-embedding trajectory analysis (Rabinovich-aligned)",
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        epilog="""
+Examples:
+  python multi_embedding.py                           # First HC subject, all embeddings
+  python multi_embedding.py --subject S001            # Specific subject
+  python multi_embedding.py --n-chunks 10             # Longer trajectory
+  python multi_embedding.py --compare-groups          # HC vs MCI comparison
+  python multi_embedding.py --compare-all             # Pooled density comparison
+  python multi_embedding.py --embedding pca           # PCA only
+  python multi_embedding.py --embedding diffusion     # Diffusion maps only
+  python multi_embedding.py --conditions MCI          # Use MCI subjects
+  python multi_embedding.py --list-subjects           # List available subjects
+
+Compare-all mode:
+  python multi_embedding.py --compare-all --conditions HID MCI AD --n-subjects 5 --n-chunks 10
+
+  Fits each embedding ONCE on pooled data from all subjects, transforms each
+  subject into shared 2D space, computes per-subject density, and averages
+  within class. Outputs density maps + difference maps (MCI-HC, AD-HC).
+
+Embedding methods:
+  pca        - Standard PCA (baseline)
+  tpca       - Time-lagged PCA (temporal structure)
+  diffusion  - Diffusion maps (metastability-aware)
+  delay      - Takens delay embedding + PCA
+  umap       - UMAP (visualization only)
+  all        - All of the above (default)
+        """
+    )
+    parser.add_argument("--subject", type=str, default=None,
+                        help="Specific subject ID")
+    parser.add_argument("--n-chunks", type=int, default=5,
+                        help="Number of consecutive chunks (default: 5)")
+    parser.add_argument("--n-subjects", type=int, default=5,
+                        help="Subjects per group for --compare-groups/--compare-all (default: 5)")
+    parser.add_argument("--conditions", type=str, nargs="+", default=["HID", "MCI"],
+                        help="Conditions to include (default: HID MCI)")
+    parser.add_argument("--embedding", type=str, default="all",
+                        choices=["pca", "tpca", "diffusion", "delay", "umap", "all"],
+                        help="Embedding method (default: all)")
+    parser.add_argument("--tau", type=int, default=5,
+                        help="Time lag for tPCA and delay embedding (default: 5)")
+    parser.add_argument("--delay-dim", type=int, default=3,
+                        help="Embedding dimension for delay embedding (default: 3)")
+    parser.add_argument("--compare-groups", action="store_true",
+                        help="Compare HC vs MCI flow metrics")
+    parser.add_argument("--compare-all", action="store_true",
+                        help="Pooled embedding with per-class density aggregation")
+    parser.add_argument("--list-subjects", action="store_true",
+                        help="List available subjects")
+    parser.add_argument("--no-show", action="store_true",
+                        help="Don't display plots interactively")
+    args = parser.parse_args()
+
+    # Create timestamped output directory
+    base_output_dir = ensure_output_dir()
+    output_dir = create_timestamped_output_dir(base_output_dir, "multi_embedding")
+    print(f"Output directory: {output_dir}")
+
+    # Save parameters for reproducibility
+    save_parameters(output_dir, {
+        "subject": args.subject,
+        "n_chunks": args.n_chunks,
+        "n_subjects": args.n_subjects,
+        "conditions": args.conditions,
+        "embedding": args.embedding,
+        "tau": args.tau,
+        "delay_dim": args.delay_dim,
+        "compare_groups": args.compare_groups,
+        "compare_all": args.compare_all,
+        "filter_low": FILTER_LOW,
+        "filter_high": FILTER_HIGH,
+        "chunk_duration": CHUNK_DURATION,
+        "sfreq": SFREQ,
+        "checkpoint_path": CHECKPOINT_PATH,
+        "data_dir": DATA_DIR,
+        "device": DEVICE,
+    })
+
+    # Get subjects
+    fif_files = get_fif_files(args.conditions)
+    groups = get_subjects_by_group(fif_files)
+
+    group_counts = []
+    for key in ["hc", "mci", "ad"]:
+        if groups.get(key):
+            group_counts.append(f"{len(groups[key])} {key.upper()}")
+    print(f"Found {', '.join(group_counts)} subjects")
+
+    if args.list_subjects:
+        list_subjects(groups)
+        return 0
+
+    # Load model
+    print("\nLoading model...")
+    model_info = load_model_from_checkpoint(CHECKPOINT_PATH, DEVICE)
+
+    # Get n_channels
+    all_subjects = []
+    for key in ["hc", "mci", "ad"]:
+        all_subjects.extend(groups.get(key, []))
+
+    if not all_subjects:
+        print("No subjects found!")
+        return 1
+
+    first_data = load_and_preprocess_fif(
+        all_subjects[0][0], FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+        verbose=False,
+    )
+    model = create_model(first_data["n_channels"], model_info, DEVICE)
+
+    # Handle --compare-all (pooled embedding with density aggregation)
+    if args.compare_all:
+        compare_all_groups(
+            model, model_info, groups,
+            args.n_subjects, args.n_chunks,
+            output_dir, not args.no_show,
+            tau=args.tau,
+            delay_dim=args.delay_dim,
+        )
+        return 0
+
+    # Handle --compare-groups
+    if args.compare_groups:
+        compare_groups(
+            model, model_info, groups,
+            args.n_subjects, args.n_chunks,
+            output_dir, not args.no_show
+        )
+        return 0
+
+    # Select subject
+    if args.subject:
+        selected = None
+        for s in all_subjects:
+            if args.subject in s[3]:
+                selected = s
+                break
+        if not selected:
+            print(f"Subject {args.subject} not found!")
+            return 1
+    else:
+        selected = all_subjects[0]
+
+    fif_path, label, condition, subject_id = selected
+    group_name = GROUP_NAMES.get(label, "Unknown")
+
+    print(f"\nSelected: {subject_id} ({condition}, {group_name})")
+
+    # Extract latent
+    latent = extract_continuous_latent(model, fif_path, model_info, args.n_chunks)
+    if latent is None:
+        print("No data!")
+        return 1
+
+    print(f"Latent shape: {latent.shape}")
+
+    # Run analysis
+    embeddings, metrics = plot_multi_embedding_comparison(
+        latent, output_dir, subject_id, group_name, not args.no_show
+    )
+
+    # Consistency analysis
+    plot_consistency_analysis(embeddings, output_dir, subject_id, not args.no_show)
+
+    print(f"\nAll plots saved to: {output_dir}")
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/scripts/local_analysis/plot_recurrence.py b/scripts/local_analysis/plot_recurrence.py
new file mode 100644
index 0000000..c5b62ba
--- /dev/null
+++ b/scripts/local_analysis/plot_recurrence.py
@@ -0,0 +1,520 @@
+#!/usr/bin/env python3
+"""
+Plot Recurrence Matrices for Local Analysis
+
+Visualize recurrence matrices from trained autoencoder latents.
+Interactive version for local analysis on M1 Mac.
+
+Usage:
+    python plot_recurrence.py                       # Default: 2 subjects
+    python plot_recurrence.py --n-subjects 5        # Plot 5 subjects
+    python plot_recurrence.py --subject S001        # Plot specific subject
+    python plot_recurrence.py --list-subjects       # List available subjects
+    python plot_recurrence.py --list-chunks S001    # List chunks for subject
+    python plot_recurrence.py --chunk 3             # Use chunk 3 (0-indexed)
+    python plot_recurrence.py --chunk all           # Plot all chunks
+    python plot_recurrence.py --theiler 0           # Disable Theiler window
+    python plot_recurrence.py --no-theiler          # Same as --theiler 0
+"""
+
+import argparse
+import sys
+import json
+from pathlib import Path
+from datetime import datetime
+
+import numpy as np
+import matplotlib.pyplot as plt
+
+# Add parent to path
+sys.path.insert(0, str(Path(__file__).parent))
+
+from config import (
+    CHECKPOINT_PATH, DATA_DIR, OUTPUT_DIR, DEVICE,
+    FILTER_LOW, FILTER_HIGH, CHUNK_DURATION, SFREQ,
+    RR_TARGETS, THEILER_WINDOW, ensure_output_dir, get_fif_files, get_subject_id,
+    get_unique_subjects
+)
+from load_model import load_model_from_checkpoint, create_model, compute_latent_trajectory
+from load_data import load_and_preprocess_fif
+
+
+def create_timestamped_output_dir(base_dir: Path, script_name: str) -> Path:
+    """Create a timestamped output directory for versioned results."""
+    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+    output_dir = base_dir / f"{script_name}_{timestamp}"
+    output_dir.mkdir(parents=True, exist_ok=True)
+    return output_dir
+
+
+def save_parameters(output_dir: Path, params: dict):
+    """Save parameters to a JSON file for reproducibility."""
+    params_path = output_dir / "parameters.json"
+
+    # Convert Path objects to strings
+    serializable_params = {}
+    for k, v in params.items():
+        if isinstance(v, Path):
+            serializable_params[k] = str(v)
+        else:
+            serializable_params[k] = v
+
+    serializable_params["timestamp"] = datetime.now().isoformat()
+
+    with open(params_path, 'w') as f:
+        json.dump(serializable_params, f, indent=2)
+    print(f"Parameters saved to: {params_path}")
+
+
+def compute_angular_distance_matrix(latent_trajectory: np.ndarray) -> np.ndarray:
+    """Compute angular distance matrix from latent trajectory."""
+    norms = np.linalg.norm(latent_trajectory, axis=1, keepdims=True)
+    norms = np.maximum(norms, 1e-8)
+    normalized = latent_trajectory / norms
+
+    cos_sim = np.dot(normalized, normalized.T)
+    cos_sim = np.clip(cos_sim, -1.0, 1.0)
+    distance_matrix = np.arccos(cos_sim)
+
+    return distance_matrix
+
+
+def compute_recurrence_matrix(
+    distance_matrix: np.ndarray,
+    target_rr: float = 0.02,
+    theiler_window: int = 0,
+) -> tuple[np.ndarray, float]:
+    """Compute binary recurrence matrix with RR-controlled threshold."""
+    N = distance_matrix.shape[0]
+
+    # Create mask for threshold computation
+    if theiler_window > 0:
+        mask = np.ones((N, N), dtype=bool)
+        for i in range(N):
+            for j in range(max(0, i - theiler_window + 1), min(N, i + theiler_window)):
+                mask[i, j] = False
+    else:
+        mask = ~np.eye(N, dtype=bool)
+
+    off_diag = distance_matrix[mask]
+    epsilon = np.percentile(off_diag, target_rr * 100)
+
+    R = (distance_matrix <= epsilon).astype(np.float64)
+
+    if theiler_window > 0:
+        for i in range(N):
+            for j in range(max(0, i - theiler_window + 1), min(N, i + theiler_window)):
+                R[i, j] = 0
+
+    return R, epsilon
+
+
+def compute_rqa_stats(R: np.ndarray, l_min: int = 2, v_min: int = 2) -> dict:
+    """
+    Compute RQA statistics from recurrence matrix.
+
+    Features computed:
+    - RR: Recurrence Rate
+    - DET: Determinism (diagonal lines)
+    - L_mean: Mean diagonal line length
+    - L_max: Max diagonal line length
+    - LAM: Laminarity (vertical lines)
+    - TT: Trapping Time (mean vertical line length)
+    - V_max: Max vertical line length
+    - DIV: Divergence (1/L_max)
+    - ENTR: Shannon entropy of diagonal line distribution
+    """
+    N = R.shape[0]
+    mask = ~np.eye(N, dtype=bool)
+
+    total_points = mask.sum()
+    recurrence_points = R[mask].sum()
+    rr = recurrence_points / total_points if total_points > 0 else 0
+
+    # Diagonal line analysis (for DET, L_mean, L_max, ENTR)
+    diagonal_lengths = []
+    for offset in range(l_min, N):
+        diag = np.diag(R, offset)
+        run_length = 0
+        for val in diag:
+            if val > 0:
+                run_length += 1
+            else:
+                if run_length >= l_min:
+                    diagonal_lengths.append(run_length)
+                run_length = 0
+        if run_length >= l_min:
+            diagonal_lengths.append(run_length)
+
+    # Also check negative diagonals (below main diagonal)
+    for offset in range(l_min, N):
+        diag = np.diag(R, -offset)
+        run_length = 0
+        for val in diag:
+            if val > 0:
+                run_length += 1
+            else:
+                if run_length >= l_min:
+                    diagonal_lengths.append(run_length)
+                run_length = 0
+        if run_length >= l_min:
+            diagonal_lengths.append(run_length)
+
+    det_points = sum(diagonal_lengths)
+    det = det_points / recurrence_points if recurrence_points > 0 else 0
+    l_mean = np.mean(diagonal_lengths) if diagonal_lengths else 0
+    l_max = max(diagonal_lengths) if diagonal_lengths else 0
+    div = 1.0 / l_max if l_max > 0 else 0
+
+    # Entropy of diagonal line length distribution
+    if diagonal_lengths:
+        hist, _ = np.histogram(diagonal_lengths, bins=range(l_min, max(diagonal_lengths) + 2))
+        hist = hist[hist > 0]
+        p = hist / hist.sum()
+        entr = -np.sum(p * np.log(p))
+    else:
+        entr = 0
+
+    # Vertical line analysis (for LAM, TT, V_max)
+    vertical_lengths = []
+    for col in range(N):
+        run_length = 0
+        for row in range(N):
+            if row == col:  # Skip main diagonal
+                if run_length >= v_min:
+                    vertical_lengths.append(run_length)
+                run_length = 0
+                continue
+            if R[row, col] > 0:
+                run_length += 1
+            else:
+                if run_length >= v_min:
+                    vertical_lengths.append(run_length)
+                run_length = 0
+        if run_length >= v_min:
+            vertical_lengths.append(run_length)
+
+    lam_points = sum(vertical_lengths)
+    lam = lam_points / recurrence_points if recurrence_points > 0 else 0
+    tt = np.mean(vertical_lengths) if vertical_lengths else 0
+    v_max = max(vertical_lengths) if vertical_lengths else 0
+
+    return {
+        "RR": rr,
+        "DET": det,
+        "L_mean": l_mean,
+        "L_max": l_max,
+        "LAM": lam,
+        "TT": tt,
+        "V_max": v_max,
+        "DIV": div,
+        "ENTR": entr,
+    }
+
+
+def list_subjects(fif_files: list):
+    """List all available unique subjects."""
+    unique = get_unique_subjects(fif_files)
+
+    print("\nAvailable unique subjects:")
+    print("-" * 60)
+
+    # Separate by group: HC (label=0), MCI (label=1), AD (label=2)
+    groups = {"HC": [], "MCI": [], "AD": []}
+    label_to_group = {0: "HC", 1: "MCI", 2: "AD"}
+
+    for subject_id, (fif_path, label, condition) in unique.items():
+        group_name = label_to_group.get(label, "Unknown")
+        groups[group_name].append((subject_id, condition))
+
+    for group_name in ["HC", "MCI", "AD"]:
+        subjects = groups[group_name]
+        if subjects:
+            print(f"\n{group_name} subjects ({len(subjects)}):")
+            for i, (subject_id, condition) in enumerate(sorted(subjects)):
+                print(f"  {i+1:3d}. {subject_id:10s} ({condition})")
+
+    total = sum(len(g) for g in groups.values())
+    counts = [f"{len(groups[g])} {g}" for g in ["HC", "MCI", "AD"] if groups[g]]
+    print(f"\nTotal: {' + '.join(counts)} = {total} unique subjects")
+
+
+def list_chunks_for_subject(fif_path: Path, model_info: dict):
+    """List available chunks for a subject."""
+    data = load_and_preprocess_fif(
+        fif_path,
+        FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+    )
+
+    subject_id = data["subject_id"]
+    n_chunks = len(data["chunks"])
+    chunk_samples = data["chunks"][0].shape[1]
+    chunk_duration = chunk_samples / data["sfreq"]
+
+    print(f"\nSubject: {subject_id}")
+    print(f"Total chunks: {n_chunks}")
+    print(f"Chunk duration: {chunk_duration:.1f}s ({chunk_samples} samples)")
+    print(f"Total duration: {n_chunks * chunk_duration:.1f}s")
+    print(f"\nChunk indices: 0 to {n_chunks - 1}")
+    print(f"Use --chunk <idx> or --chunk all")
+
+
+def plot_single_subject(
+    model,
+    fif_path: Path,
+    model_info: dict,
+    output_dir: Path,
+    rr_targets: list = [0.01, 0.02, 0.05],
+    theiler_window: int = 50,
+    chunk_idx: int | str = 0,
+    show_plot: bool = True,
+):
+    """Plot recurrence matrices for a single subject.
+
+    Args:
+        chunk_idx: Integer for specific chunk, or "all" to plot all chunks
+    """
+
+    # Load and preprocess
+    data = load_and_preprocess_fif(
+        fif_path,
+        FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+    )
+
+    subject_id = data["subject_id"]
+    n_chunks = len(data["chunks"])
+
+    # Determine which chunks to process
+    if chunk_idx == "all":
+        chunks_to_process = list(range(n_chunks))
+    else:
+        if chunk_idx >= n_chunks:
+            print(f"Warning: chunk_idx {chunk_idx} >= {n_chunks}, using 0")
+            chunk_idx = 0
+        chunks_to_process = [chunk_idx]
+
+    print(f"Subject {subject_id}: {n_chunks} chunks available, processing {len(chunks_to_process)}")
+
+    results = []
+    for cidx in chunks_to_process:
+        phase_data = data["chunks"][cidx]
+
+        # Compute latent trajectory
+        print(f"  Computing latent trajectory for chunk {cidx}...")
+        latent = compute_latent_trajectory(model, phase_data, DEVICE)
+        print(f"    Latent shape: {latent.shape} (time, hidden_size)")
+
+        # Compute distance matrix
+        distance_matrix = compute_angular_distance_matrix(latent)
+
+        # Create figure with distance + multiple RR comparisons
+        n_rr = len(rr_targets)
+        fig, axes = plt.subplots(1, n_rr + 1, figsize=(5 * (n_rr + 1), 5))
+
+        time_extent = latent.shape[0] / SFREQ
+
+        # Distance matrix
+        im0 = axes[0].imshow(
+            distance_matrix, cmap="viridis", origin="lower",
+            extent=[0, time_extent, 0, time_extent]
+        )
+        axes[0].set_title("Angular Distance Matrix")
+        axes[0].set_xlabel("Time (s)")
+        axes[0].set_ylabel("Time (s)")
+        plt.colorbar(im0, ax=axes[0], label="Distance (rad)")
+
+        # Recurrence matrices at different RR
+        for i, rr in enumerate(rr_targets):
+            R, eps = compute_recurrence_matrix(distance_matrix, rr, theiler_window)
+            stats = compute_rqa_stats(R)
+
+            axes[i + 1].imshow(
+                R, cmap="binary", origin="lower",
+                extent=[0, time_extent, 0, time_extent]
+            )
+            axes[i + 1].set_title(f"RR={rr*100:.0f}%\nDET={stats['DET']:.2f}")
+            axes[i + 1].set_xlabel("Time (s)")
+            if i == 0:
+                axes[i + 1].set_ylabel("Time (s)")
+
+        # Title with Theiler info
+        theiler_str = f"Theiler={theiler_window}" if theiler_window > 0 else "No Theiler"
+        chunk_str = f"chunk {cidx}" if len(chunks_to_process) > 1 or chunk_idx != 0 else ""
+        title_parts = [f"Subject: {subject_id}", theiler_str]
+        if chunk_str:
+            title_parts.append(chunk_str)
+        plt.suptitle(" | ".join(title_parts), fontsize=14, fontweight="bold")
+        plt.tight_layout()
+
+        # Save
+        if len(chunks_to_process) > 1:
+            save_path = output_dir / f"{subject_id}_chunk{cidx}_recurrence.png"
+        else:
+            save_path = output_dir / f"{subject_id}_recurrence_matrices.png"
+        fig.savefig(save_path, dpi=150, bbox_inches="tight")
+        print(f"    Saved: {save_path}")
+
+        if show_plot:
+            plt.show()
+        else:
+            plt.close(fig)
+
+        results.append((latent, distance_matrix))
+
+    return results
+
+
+def main():
+    parser = argparse.ArgumentParser(
+        description="Plot recurrence matrices locally",
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        epilog="""
+Examples:
+  python plot_recurrence.py                       # Default: 2 subjects, chunk 0
+  python plot_recurrence.py --n-subjects 5        # Plot 5 subjects
+  python plot_recurrence.py --subject S001        # Specific subject
+  python plot_recurrence.py --list-subjects       # List available subjects
+  python plot_recurrence.py --list-chunks S001    # Show chunks for subject
+  python plot_recurrence.py --chunk 3             # Use chunk 3
+  python plot_recurrence.py --chunk all           # Plot all chunks
+  python plot_recurrence.py --no-theiler          # Disable Theiler window
+  python plot_recurrence.py --theiler 100         # Custom Theiler window
+        """
+    )
+    parser.add_argument("--n-subjects", type=int, default=2, help="Number of subjects to plot (default: 2)")
+    parser.add_argument("--subject", type=str, default=None, help="Specific subject ID to plot")
+    parser.add_argument("--conditions", type=str, nargs="+", default=["MCI", "HID", "AD", "HC"],
+                        help="Conditions to include (default: all)")
+    parser.add_argument("--chunk", type=str, default="0",
+                        help="Chunk index (0-based) or 'all' for all chunks (default: 0)")
+    parser.add_argument("--theiler", type=int, default=THEILER_WINDOW,
+                        help=f"Theiler window in samples (default: {THEILER_WINDOW}, ~{THEILER_WINDOW/SFREQ:.2f}s)")
+    parser.add_argument("--no-theiler", action="store_true",
+                        help="Disable Theiler window (same as --theiler 0)")
+    parser.add_argument("--list-subjects", action="store_true", help="List available subjects and exit")
+    parser.add_argument("--list-chunks", type=str, metavar="SUBJECT",
+                        help="List chunks for a specific subject and exit")
+    parser.add_argument("--no-show", action="store_true", help="Don't display plots interactively")
+    args = parser.parse_args()
+
+    # Handle Theiler window
+    theiler_window = 0 if args.no_theiler else args.theiler
+
+    # Parse chunk argument
+    if args.chunk.lower() == "all":
+        chunk_idx = "all"
+    else:
+        try:
+            chunk_idx = int(args.chunk)
+        except ValueError:
+            print(f"Error: --chunk must be an integer or 'all', got '{args.chunk}'")
+            return 1
+
+    # Create timestamped output directory
+    base_output_dir = ensure_output_dir()
+    output_dir = create_timestamped_output_dir(base_output_dir, "plot_recurrence")
+    print(f"Output directory: {output_dir}")
+
+    # Save parameters for reproducibility
+    save_parameters(output_dir, {
+        "n_subjects": args.n_subjects,
+        "subject": args.subject,
+        "conditions": args.conditions,
+        "chunk": args.chunk,
+        "theiler": theiler_window,
+        "rr_targets": RR_TARGETS,
+        "filter_low": FILTER_LOW,
+        "filter_high": FILTER_HIGH,
+        "chunk_duration": CHUNK_DURATION,
+        "sfreq": SFREQ,
+        "checkpoint_path": CHECKPOINT_PATH,
+        "data_dir": DATA_DIR,
+        "device": DEVICE,
+    })
+
+    # Get files first (before loading model for list commands)
+    fif_files = get_fif_files(args.conditions)
+
+    # Get unique subjects
+    unique_subjects = get_unique_subjects(fif_files)
+    print(f"Found {len(unique_subjects)} unique subjects in conditions: {args.conditions}")
+
+    if len(unique_subjects) == 0:
+        print(f"No .fif files found in {DATA_DIR}")
+        return 1
+
+    # Handle --list-subjects
+    if args.list_subjects:
+        list_subjects(fif_files)
+        return 0
+
+    # Load model
+    print("\nLoading model...")
+    model_info = load_model_from_checkpoint(CHECKPOINT_PATH, DEVICE)
+
+    # Handle --list-chunks
+    if args.list_chunks:
+        if args.list_chunks in unique_subjects:
+            fif_path, _, _ = unique_subjects[args.list_chunks]
+            list_chunks_for_subject(fif_path, model_info)
+        else:
+            # Try partial match
+            matching = [sid for sid in unique_subjects if args.list_chunks in sid]
+            if not matching:
+                print(f"No files found for subject {args.list_chunks}")
+                return 1
+            fif_path, _, _ = unique_subjects[matching[0]]
+            list_chunks_for_subject(fif_path, model_info)
+        return 0
+
+    # Create model (need n_channels from first file)
+    first_subject = list(unique_subjects.values())[0]
+    first_data = load_and_preprocess_fif(
+        first_subject[0], FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+    )
+    n_channels = first_data["n_channels"]
+    model = create_model(n_channels, model_info, DEVICE)
+
+    # Filter by subject if specified
+    if args.subject:
+        if args.subject in unique_subjects:
+            subjects_to_plot = {args.subject: unique_subjects[args.subject]}
+        else:
+            # Try partial match
+            subjects_to_plot = {sid: data for sid, data in unique_subjects.items() if args.subject in sid}
+        if not subjects_to_plot:
+            print(f"No files found for subject {args.subject}")
+            return 1
+    else:
+        subjects_to_plot = unique_subjects
+
+    # Plot
+    subjects_plotted = 0
+    label_to_group = {0: "HC", 1: "MCI", 2: "AD"}
+    for subject_id, (fif_path, label, condition) in list(subjects_to_plot.items())[:args.n_subjects]:
+        group = label_to_group.get(label, "Unknown")
+        print(f"\n{'='*60}")
+        print(f"Processing: {subject_id} ({condition}, {group})")
+        print(f"Theiler window: {theiler_window} samples ({theiler_window/SFREQ:.2f}s)" if theiler_window > 0 else "Theiler window: DISABLED")
+        print(f"{'='*60}")
+
+        plot_single_subject(
+            model, fif_path, model_info, output_dir,
+            rr_targets=RR_TARGETS,
+            theiler_window=theiler_window,
+            chunk_idx=chunk_idx,
+            show_plot=not args.no_show,
+        )
+        subjects_plotted += 1
+
+    print(f"\n{'='*60}")
+    print(f"Done! Plotted {subjects_plotted} unique subject(s)")
+    print(f"All plots saved to: {output_dir}")
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/scripts/local_analysis/plot_trajectory.py b/scripts/local_analysis/plot_trajectory.py
new file mode 100644
index 0000000..32efce9
--- /dev/null
+++ b/scripts/local_analysis/plot_trajectory.py
@@ -0,0 +1,884 @@
+#!/usr/bin/env python3
+"""
+Rabinovich-style Trajectory Visualization in Latent Phase Space
+
+Visualize latent dynamics as trajectories (not points/clusters) to observe:
+- Flow channels and preferred directions
+- Metastable regions (slow/sticky areas)
+- Recurrent visits and looping motifs
+- Transition dynamics between states
+
+Based on Rabinovich's Information Flow Phase Space (IFPS) framework.
+
+Usage:
+    python plot_trajectory.py                           # Default: first HC subject
+    python plot_trajectory.py --subject S001            # Specific subject
+    python plot_trajectory.py --n-chunks 5              # Use 5 consecutive chunks
+    python plot_trajectory.py --mode all                # All visualizations
+    python plot_trajectory.py --mode trajectory         # Just trajectory plots
+    python plot_trajectory.py --mode speed              # Speed-colored + metastability
+    python plot_trajectory.py --mode flow               # Flow field (quiver)
+    python plot_trajectory.py --mode density            # Density heatmap
+    python plot_trajectory.py --compare-random          # Compare trained vs random
+    python plot_trajectory.py --list-subjects           # List available subjects
+"""
+
+import argparse
+import sys
+import json
+from pathlib import Path
+from datetime import datetime
+
+import numpy as np
+import matplotlib.pyplot as plt
+from matplotlib.collections import LineCollection
+from matplotlib.colors import Normalize
+from scipy.ndimage import gaussian_filter
+from sklearn.decomposition import PCA
+
+# Add parent to path
+sys.path.insert(0, str(Path(__file__).parent))
+
+from config import (
+    CHECKPOINT_PATH, DATA_DIR, OUTPUT_DIR, DEVICE,
+    FILTER_LOW, FILTER_HIGH, CHUNK_DURATION, SFREQ,
+    ensure_output_dir, get_fif_files, get_subjects_by_group
+)
+from load_model import load_model_from_checkpoint, create_model, compute_latent_trajectory
+from load_data import load_and_preprocess_fif
+
+# Color mapping
+GROUP_COLORS = {0: "blue", 1: "orange", 2: "red"}
+GROUP_NAMES = {0: "HC", 1: "MCI", 2: "AD"}
+
+
+def create_timestamped_output_dir(base_dir: Path, script_name: str) -> Path:
+    """Create a timestamped output directory for versioned results."""
+    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+    output_dir = base_dir / f"{script_name}_{timestamp}"
+    output_dir.mkdir(parents=True, exist_ok=True)
+    return output_dir
+
+
+def save_parameters(output_dir: Path, params: dict):
+    """Save parameters to a JSON file for reproducibility."""
+    params_path = output_dir / "parameters.json"
+
+    # Convert Path objects to strings
+    serializable_params = {}
+    for k, v in params.items():
+        if isinstance(v, Path):
+            serializable_params[k] = str(v)
+        else:
+            serializable_params[k] = v
+
+    serializable_params["timestamp"] = datetime.now().isoformat()
+
+    with open(params_path, 'w') as f:
+        json.dump(serializable_params, f, indent=2)
+    print(f"Parameters saved to: {params_path}")
+
+
+def extract_continuous_latent(
+    model,
+    fif_path: Path,
+    model_info: dict,
+    n_chunks: int = 5,
+    device: str = "mps",
+) -> tuple[np.ndarray, list[int]]:
+    """
+    Extract continuous latent trajectory from consecutive chunks.
+
+    Returns:
+        latent: (total_time, hidden_size) array - concatenated latent trajectory
+        chunk_boundaries: list of indices where chunks start
+    """
+    data = load_and_preprocess_fif(
+        fif_path, FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+        verbose=False,
+    )
+
+    available_chunks = len(data["chunks"])
+    if available_chunks == 0:
+        return None, []
+
+    chunks_to_use = min(n_chunks, available_chunks)
+
+    latents = []
+    chunk_boundaries = [0]
+
+    for cidx in range(chunks_to_use):
+        phase_data = data["chunks"][cidx]
+        latent = compute_latent_trajectory(model, phase_data, device)
+        latents.append(latent)
+        chunk_boundaries.append(chunk_boundaries[-1] + latent.shape[0])
+
+    # Concatenate all chunks
+    continuous_latent = np.concatenate(latents, axis=0)
+
+    return continuous_latent, chunk_boundaries[:-1]  # Remove last (total length)
+
+
+def compute_instantaneous_speed(latent: np.ndarray, dt: float = 1.0) -> np.ndarray:
+    """
+    Compute instantaneous speed in latent space.
+
+    Args:
+        latent: (T, D) trajectory
+        dt: time step (default 1 sample)
+
+    Returns:
+        speed: (T-1,) array of speeds
+    """
+    diff = np.diff(latent, axis=0)
+    speed = np.linalg.norm(diff, axis=1) / dt
+    return speed
+
+
+def detect_dwell_episodes(
+    speed: np.ndarray,
+    threshold_percentile: float = 20,
+    min_duration: int = 10,
+) -> list[tuple[int, int, float]]:
+    """
+    Detect dwell episodes (contiguous low-speed runs).
+
+    Args:
+        speed: instantaneous speed array
+        threshold_percentile: consider bottom X% as "slow"
+        min_duration: minimum samples to count as episode
+
+    Returns:
+        List of (start_idx, end_idx, mean_speed) tuples
+    """
+    threshold = np.percentile(speed, threshold_percentile)
+    is_slow = speed < threshold
+
+    episodes = []
+    in_episode = False
+    start = 0
+
+    for i, slow in enumerate(is_slow):
+        if slow and not in_episode:
+            in_episode = True
+            start = i
+        elif not slow and in_episode:
+            in_episode = False
+            if i - start >= min_duration:
+                episodes.append((start, i, speed[start:i].mean()))
+
+    # Handle episode at end
+    if in_episode and len(is_slow) - start >= min_duration:
+        episodes.append((start, len(is_slow), speed[start:].mean()))
+
+    return episodes
+
+
+def plot_trajectory_2d(
+    latent: np.ndarray,
+    output_dir: Path,
+    chunk_boundaries: list[int] = None,
+    title_suffix: str = "",
+    show_plot: bool = True,
+):
+    """
+    Plot 2D PCA trajectory colored by time.
+    """
+    # PCA to 2D
+    pca = PCA(n_components=2)
+    latent_2d = pca.fit_transform(latent)
+
+    fig, ax = plt.subplots(figsize=(10, 8))
+
+    # Create line segments for color mapping
+    points = latent_2d.reshape(-1, 1, 2)
+    segments = np.concatenate([points[:-1], points[1:]], axis=1)
+
+    # Color by time
+    norm = Normalize(vmin=0, vmax=len(latent_2d))
+    lc = LineCollection(segments, cmap='viridis', norm=norm, linewidth=0.5, alpha=0.7)
+    lc.set_array(np.arange(len(latent_2d)))
+    ax.add_collection(lc)
+
+    # Mark start and end
+    ax.scatter(latent_2d[0, 0], latent_2d[0, 1], c='green', s=100, marker='o',
+               label='Start', zorder=5, edgecolors='black')
+    ax.scatter(latent_2d[-1, 0], latent_2d[-1, 1], c='red', s=100, marker='s',
+               label='End', zorder=5, edgecolors='black')
+
+    # Mark chunk boundaries
+    if chunk_boundaries:
+        for i, boundary in enumerate(chunk_boundaries[1:], 1):
+            if boundary < len(latent_2d):
+                ax.scatter(latent_2d[boundary, 0], latent_2d[boundary, 1],
+                          c='white', s=50, marker='|', edgecolors='black', zorder=4)
+
+    ax.autoscale()
+    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
+    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
+    ax.set_title(f'Latent Trajectory (2D PCA){title_suffix}', fontsize=14, fontweight='bold')
+    ax.legend(loc='upper right')
+
+    # Colorbar
+    sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)
+    sm.set_array([])
+    cbar = plt.colorbar(sm, ax=ax, label='Time (samples)')
+
+    plt.tight_layout()
+
+    save_path = output_dir / "trajectory_2d.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return pca, latent_2d
+
+
+def plot_trajectory_3d(
+    latent: np.ndarray,
+    output_dir: Path,
+    chunk_boundaries: list[int] = None,
+    title_suffix: str = "",
+    show_plot: bool = True,
+):
+    """
+    Plot 3D PCA trajectory colored by time.
+    """
+    # PCA to 3D
+    pca = PCA(n_components=3)
+    latent_3d = pca.fit_transform(latent)
+
+    fig = plt.figure(figsize=(12, 10))
+    ax = fig.add_subplot(111, projection='3d')
+
+    # Plot trajectory colored by time
+    colors = plt.cm.viridis(np.linspace(0, 1, len(latent_3d)))
+
+    for i in range(len(latent_3d) - 1):
+        ax.plot(latent_3d[i:i+2, 0], latent_3d[i:i+2, 1], latent_3d[i:i+2, 2],
+                c=colors[i], alpha=0.7, linewidth=0.5)
+
+    # Mark start and end
+    ax.scatter(*latent_3d[0], c='green', s=100, marker='o', label='Start', edgecolors='black')
+    ax.scatter(*latent_3d[-1], c='red', s=100, marker='s', label='End', edgecolors='black')
+
+    var_explained = pca.explained_variance_ratio_
+    ax.set_xlabel(f'PC1 ({var_explained[0]*100:.1f}%)')
+    ax.set_ylabel(f'PC2 ({var_explained[1]*100:.1f}%)')
+    ax.set_zlabel(f'PC3 ({var_explained[2]*100:.1f}%)')
+    ax.set_title(f'Latent Trajectory (3D PCA){title_suffix}', fontsize=14, fontweight='bold')
+    ax.legend()
+
+    plt.tight_layout()
+
+    save_path = output_dir / "trajectory_3d.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return pca, latent_3d
+
+
+def plot_speed_trajectory(
+    latent: np.ndarray,
+    output_dir: Path,
+    speed_threshold_percentile: float = 20,
+    title_suffix: str = "",
+    show_plot: bool = True,
+):
+    """
+    Plot trajectory colored by instantaneous speed + mark metastable regions.
+    """
+    # PCA to 2D
+    pca = PCA(n_components=2)
+    latent_2d = pca.fit_transform(latent)
+
+    # Compute speed
+    speed = compute_instantaneous_speed(latent)
+    speed_threshold = np.percentile(speed, speed_threshold_percentile)
+
+    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
+
+    # Left: Speed-colored trajectory
+    ax = axes[0]
+    points = latent_2d[:-1].reshape(-1, 1, 2)
+    segments = np.concatenate([points[:-1], points[1:]], axis=1)
+
+    norm = Normalize(vmin=speed.min(), vmax=speed.max())
+    lc = LineCollection(segments, cmap='coolwarm_r', norm=norm, linewidth=1, alpha=0.8)
+    lc.set_array(speed[:-1])
+    ax.add_collection(lc)
+
+    ax.autoscale()
+    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
+    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
+    ax.set_title('Speed-colored trajectory\n(dark blue = slow/metastable)', fontsize=12)
+
+    sm = plt.cm.ScalarMappable(cmap='coolwarm_r', norm=norm)
+    sm.set_array([])
+    plt.colorbar(sm, ax=ax, label='Speed (latent units/sample)')
+
+    # Right: Metastable markers
+    ax = axes[1]
+
+    # Plot full trajectory in gray
+    ax.plot(latent_2d[:, 0], latent_2d[:, 1], 'gray', alpha=0.3, linewidth=0.5)
+
+    # Mark slow (metastable) points
+    is_slow = speed < speed_threshold
+    slow_points = latent_2d[:-1][is_slow]
+    ax.scatter(slow_points[:, 0], slow_points[:, 1], c='darkblue', s=3, alpha=0.5,
+               label=f'Slow (<{speed_threshold_percentile}%ile)')
+
+    # Mark fast points
+    fast_points = latent_2d[:-1][~is_slow]
+    ax.scatter(fast_points[:, 0], fast_points[:, 1], c='red', s=1, alpha=0.3,
+               label=f'Fast (>{speed_threshold_percentile}%ile)')
+
+    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
+    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
+    ax.set_title(f'Metastable regions (bottom {speed_threshold_percentile}% speed)', fontsize=12)
+    ax.legend(loc='upper right')
+
+    plt.suptitle(f'Metastability Analysis{title_suffix}', fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / "trajectory_speed.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return speed
+
+
+def plot_density_heatmap(
+    latent: np.ndarray,
+    output_dir: Path,
+    bins: int = 50,
+    title_suffix: str = "",
+    show_plot: bool = True,
+):
+    """
+    Plot 2D density heatmap showing where system spends time + trajectory overlay.
+    """
+    # PCA to 2D
+    pca = PCA(n_components=2)
+    latent_2d = pca.fit_transform(latent)
+
+    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
+
+    # Left: Density heatmap
+    ax = axes[0]
+    H, xedges, yedges = np.histogram2d(latent_2d[:, 0], latent_2d[:, 1], bins=bins)
+    H = gaussian_filter(H.T, sigma=1)  # Smooth
+
+    extent = [xedges[0], xedges[-1], yedges[0], yedges[-1]]
+    im = ax.imshow(H, origin='lower', extent=extent, cmap='hot', aspect='equal')
+    plt.colorbar(im, ax=ax, label='Density (time spent)', shrink=0.8)
+
+    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
+    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
+    ax.set_title('Density heatmap\n(where system spends time)', fontsize=12)
+    ax.set_aspect('equal')
+
+    # Right: Density + trajectory overlay
+    ax = axes[1]
+    im = ax.imshow(H, origin='lower', extent=extent, cmap='hot', aspect='equal', alpha=0.7)
+    ax.plot(latent_2d[:, 0], latent_2d[:, 1], 'cyan', alpha=0.4, linewidth=0.3)
+
+    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
+    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
+    ax.set_title('Density + trajectory overlay', fontsize=12)
+    ax.set_aspect('equal')
+
+    plt.suptitle(f'Density Analysis (Dwell Time Distribution){title_suffix}', fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / "trajectory_density.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def plot_flow_field(
+    latent: np.ndarray,
+    output_dir: Path,
+    grid_size: int = 20,
+    title_suffix: str = "",
+    show_plot: bool = True,
+):
+    """
+    Plot empirical flow field (quiver) showing local displacement vectors.
+    """
+    # PCA to 2D
+    pca = PCA(n_components=2)
+    latent_2d = pca.fit_transform(latent)
+
+    # Compute displacements
+    dx = np.diff(latent_2d[:, 0])
+    dy = np.diff(latent_2d[:, 1])
+
+    # Create grid
+    x_min, x_max = latent_2d[:, 0].min(), latent_2d[:, 0].max()
+    y_min, y_max = latent_2d[:, 1].min(), latent_2d[:, 1].max()
+
+    x_edges = np.linspace(x_min, x_max, grid_size + 1)
+    y_edges = np.linspace(y_min, y_max, grid_size + 1)
+
+    # Bin displacements
+    flow_x = np.zeros((grid_size, grid_size))
+    flow_y = np.zeros((grid_size, grid_size))
+    counts = np.zeros((grid_size, grid_size))
+
+    for i in range(len(dx)):
+        x_bin = np.searchsorted(x_edges[:-1], latent_2d[i, 0]) - 1
+        y_bin = np.searchsorted(y_edges[:-1], latent_2d[i, 1]) - 1
+
+        x_bin = np.clip(x_bin, 0, grid_size - 1)
+        y_bin = np.clip(y_bin, 0, grid_size - 1)
+
+        flow_x[y_bin, x_bin] += dx[i]
+        flow_y[y_bin, x_bin] += dy[i]
+        counts[y_bin, x_bin] += 1
+
+    # Average
+    mask = counts > 0
+    flow_x[mask] /= counts[mask]
+    flow_y[mask] /= counts[mask]
+
+    # Grid centers
+    x_centers = (x_edges[:-1] + x_edges[1:]) / 2
+    y_centers = (y_edges[:-1] + y_edges[1:]) / 2
+    X, Y = np.meshgrid(x_centers, y_centers)
+
+    fig, axes = plt.subplots(1, 2, figsize=(16, 7))
+
+    # Left: Flow field only
+    ax = axes[0]
+    magnitude = np.sqrt(flow_x**2 + flow_y**2)
+    ax.quiver(X, Y, flow_x, flow_y, magnitude, cmap='plasma', alpha=0.8)
+    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
+    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
+    ax.set_title('Flow field (local displacement vectors)', fontsize=12)
+    ax.set_xlim(x_min, x_max)
+    ax.set_ylim(y_min, y_max)
+    ax.set_aspect('equal')
+
+    # Right: Flow field + trajectory
+    ax = axes[1]
+    ax.plot(latent_2d[:, 0], latent_2d[:, 1], 'gray', alpha=0.3, linewidth=0.3)
+    ax.quiver(X, Y, flow_x, flow_y, magnitude, cmap='plasma', alpha=0.8)
+    ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)')
+    ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)')
+    ax.set_title('Flow field + trajectory overlay', fontsize=12)
+    ax.set_xlim(x_min, x_max)
+    ax.set_ylim(y_min, y_max)
+    ax.set_aspect('equal')
+
+    plt.suptitle(f'Empirical Flow Field (Rabinovich-style){title_suffix}', fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / "trajectory_flow.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+
+def compute_trajectory_stats(latent: np.ndarray, speed: np.ndarray = None) -> dict:
+    """
+    Compute summary statistics for trajectory dynamics.
+    """
+    if speed is None:
+        speed = compute_instantaneous_speed(latent)
+
+    # Dwell episodes
+    episodes = detect_dwell_episodes(speed)
+
+    # PCA for explored volume
+    pca = PCA(n_components=min(3, latent.shape[1]))
+    latent_pca = pca.fit_transform(latent)
+
+    # Convex hull volume proxy (variance)
+    explored_variance = np.var(latent_pca, axis=0).sum()
+
+    # Path length
+    path_length = speed.sum()
+
+    # Tortuosity (path length / displacement)
+    displacement = np.linalg.norm(latent[-1] - latent[0])
+    tortuosity = path_length / displacement if displacement > 0 else np.inf
+
+    return {
+        "mean_speed": speed.mean(),
+        "speed_std": speed.std(),
+        "speed_cv": speed.std() / speed.mean() if speed.mean() > 0 else 0,
+        "n_dwell_episodes": len(episodes),
+        "total_dwell_time": sum(e[1] - e[0] for e in episodes),
+        "mean_dwell_duration": np.mean([e[1] - e[0] for e in episodes]) if episodes else 0,
+        "explored_variance": explored_variance,
+        "path_length": path_length,
+        "displacement": displacement,
+        "tortuosity": tortuosity,
+        "pca_var_explained": pca.explained_variance_ratio_.tolist(),
+    }
+
+
+def compare_trained_vs_random(
+    fif_path: Path,
+    model_info: dict,
+    n_channels: int,
+    n_chunks: int,
+    output_dir: Path,
+    show_plot: bool = True,
+):
+    """
+    Compare trajectory properties between trained and random models.
+    """
+    print("\n" + "=" * 60)
+    print("COMPARING TRAINED vs RANDOM WEIGHTS")
+    print("=" * 60)
+
+    results = {}
+
+    for mode, load_weights in [("Trained", True), ("Random", False)]:
+        print(f"\n--- {mode} model ---")
+        model = create_model(n_channels, model_info, DEVICE, load_weights=load_weights)
+
+        latent, boundaries = extract_continuous_latent(
+            model, fif_path, model_info, n_chunks, DEVICE
+        )
+
+        if latent is None:
+            print(f"  No data available")
+            continue
+
+        speed = compute_instantaneous_speed(latent)
+        stats = compute_trajectory_stats(latent, speed)
+        results[mode] = {"latent": latent, "speed": speed, "stats": stats}
+
+        print(f"  Mean speed: {stats['mean_speed']:.4f}")
+        print(f"  Speed CV: {stats['speed_cv']:.4f}")
+        print(f"  Dwell episodes: {stats['n_dwell_episodes']}")
+        print(f"  Tortuosity: {stats['tortuosity']:.2f}")
+        print(f"  Explored variance: {stats['explored_variance']:.4f}")
+
+    if len(results) < 2:
+        print("Cannot compare - missing data")
+        return
+
+    # Create comparison plot
+    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
+
+    for col, (mode, data) in enumerate(results.items()):
+        latent = data["latent"]
+        speed = data["speed"]
+
+        # PCA
+        pca = PCA(n_components=2)
+        latent_2d = pca.fit_transform(latent)
+
+        # Row 0: Trajectory
+        ax = axes[0, col]
+        points = latent_2d.reshape(-1, 1, 2)
+        segments = np.concatenate([points[:-1], points[1:]], axis=1)
+        norm = Normalize(vmin=0, vmax=len(latent_2d))
+        lc = LineCollection(segments, cmap='viridis', norm=norm, linewidth=0.5, alpha=0.7)
+        lc.set_array(np.arange(len(latent_2d)))
+        ax.add_collection(lc)
+        ax.autoscale()
+        ax.set_title(f'{mode}: Trajectory', fontsize=12)
+        ax.set_xlabel('PC1')
+        ax.set_ylabel('PC2')
+
+        # Row 1: Speed distribution
+        ax = axes[1, col]
+        ax.hist(speed, bins=50, alpha=0.7, color='steelblue', edgecolor='black')
+        ax.axvline(np.percentile(speed, 20), color='red', linestyle='--', label='20%ile')
+        ax.set_xlabel('Speed')
+        ax.set_ylabel('Count')
+        ax.set_title(f'{mode}: Speed distribution', fontsize=12)
+        ax.legend()
+
+    # Right column: comparison stats
+    ax = axes[0, 2]
+    ax.axis('off')
+
+    stats_text = "COMPARISON SUMMARY\n" + "=" * 40 + "\n\n"
+    for metric in ["mean_speed", "speed_cv", "n_dwell_episodes", "tortuosity", "explored_variance"]:
+        trained_val = results["Trained"]["stats"][metric]
+        random_val = results["Random"]["stats"][metric]
+        ratio = trained_val / random_val if random_val != 0 else np.inf
+        stats_text += f"{metric}:\n"
+        stats_text += f"  Trained: {trained_val:.4f}\n"
+        stats_text += f"  Random:  {random_val:.4f}\n"
+        stats_text += f"  Ratio:   {ratio:.2f}\n\n"
+
+    ax.text(0.1, 0.9, stats_text, transform=ax.transAxes, fontsize=10,
+            verticalalignment='top', fontfamily='monospace',
+            bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
+
+    ax = axes[1, 2]
+    ax.axis('off')
+
+    interpretation = """
+INTERPRETATION GUIDE
+====================
+
+If Trained shows:
+- Lower speed CV → more consistent dynamics
+- More dwell episodes → richer metastable structure
+- Higher tortuosity → more complex paths
+- Lower explored variance → more constrained dynamics
+
+vs Random:
+- Similar values → learned features may not add much
+- Very different → learned representations capture structure
+"""
+    ax.text(0.1, 0.9, interpretation, transform=ax.transAxes, fontsize=9,
+            verticalalignment='top', fontfamily='monospace')
+
+    plt.suptitle('Trained vs Random Weights: Trajectory Comparison', fontsize=14, fontweight='bold')
+    plt.tight_layout()
+
+    save_path = output_dir / "trajectory_trained_vs_random.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"\nSaved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return results
+
+
+def list_subjects(groups: dict):
+    """List available subjects by group."""
+    print("\n" + "=" * 60)
+    print("AVAILABLE SUBJECTS")
+    print("=" * 60)
+
+    for group_key in ["hc", "mci", "ad"]:
+        subjects = groups.get(group_key, [])
+        if not subjects:
+            continue
+
+        group_name = GROUP_NAMES.get(subjects[0][1], group_key.upper())
+        print(f"\n{group_name} subjects ({len(subjects)}):")
+        for i, (fif_path, label, condition, subject_id) in enumerate(subjects):
+            print(f"  {i+1:3d}. {subject_id:10s} ({condition})")
+
+    total = sum(len(groups.get(k, [])) for k in ["hc", "mci", "ad"])
+    print(f"\nTotal: {total} unique subjects")
+
+
+def main():
+    parser = argparse.ArgumentParser(
+        description="Rabinovich-style trajectory visualization",
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        epilog="""
+Examples:
+  python plot_trajectory.py                           # Default: first HC subject
+  python plot_trajectory.py --subject S001            # Specific subject
+  python plot_trajectory.py --n-chunks 10             # Use 10 consecutive chunks
+  python plot_trajectory.py --mode all                # All visualizations
+  python plot_trajectory.py --mode trajectory         # Just 2D/3D trajectory
+  python plot_trajectory.py --mode speed              # Speed + metastability
+  python plot_trajectory.py --mode flow               # Flow field (quiver)
+  python plot_trajectory.py --mode density            # Density heatmap
+  python plot_trajectory.py --compare-random          # Trained vs random comparison
+  python plot_trajectory.py --conditions MCI          # Use MCI subject
+  python plot_trajectory.py --list-subjects           # List available subjects
+        """
+    )
+    parser.add_argument("--subject", type=str, default=None,
+                        help="Specific subject ID (default: first available)")
+    parser.add_argument("--n-chunks", type=int, default=5,
+                        help="Number of consecutive chunks to use (default: 5)")
+    parser.add_argument("--conditions", type=str, nargs="+", default=["HID", "MCI"],
+                        help="Conditions to include (default: HID MCI)")
+    parser.add_argument("--mode", type=str, default="all",
+                        choices=["all", "trajectory", "speed", "flow", "density"],
+                        help="Visualization mode (default: all)")
+    parser.add_argument("--compare-random", action="store_true",
+                        help="Compare trained vs random weights")
+    parser.add_argument("--list-subjects", action="store_true",
+                        help="List available subjects and exit")
+    parser.add_argument("--no-show", action="store_true",
+                        help="Don't display plots interactively")
+    parser.add_argument("--output-dir", type=str, default=None,
+                        help="Custom output directory (default: timestamped folder)")
+    args = parser.parse_args()
+
+    # Create timestamped output directory
+    base_output_dir = ensure_output_dir()
+    if args.output_dir:
+        output_dir = Path(args.output_dir)
+        output_dir.mkdir(parents=True, exist_ok=True)
+    else:
+        output_dir = create_timestamped_output_dir(base_output_dir, "plot_trajectory")
+
+    # Get subjects
+    fif_files = get_fif_files(args.conditions)
+    groups = get_subjects_by_group(fif_files)
+
+    # Print summary
+    group_counts = []
+    for key in ["hc", "mci", "ad"]:
+        if groups.get(key):
+            group_counts.append(f"{len(groups[key])} {key.upper()}")
+    print(f"Found {', '.join(group_counts)} subjects for conditions: {args.conditions}")
+
+    # Handle --list-subjects
+    if args.list_subjects:
+        list_subjects(groups)
+        return 0
+
+    # Select subject
+    all_subjects = []
+    for key in ["hc", "mci", "ad"]:
+        all_subjects.extend(groups.get(key, []))
+
+    if not all_subjects:
+        print("No subjects found!")
+        return 1
+
+    if args.subject:
+        # Find specific subject
+        selected = None
+        for s in all_subjects:
+            if args.subject in s[3]:  # subject_id is at index 3
+                selected = s
+                break
+        if not selected:
+            print(f"Subject {args.subject} not found!")
+            return 1
+    else:
+        # Use first subject
+        selected = all_subjects[0]
+
+    fif_path, label, condition, subject_id = selected
+    group_name = GROUP_NAMES.get(label, "Unknown")
+
+    print(f"\nSelected subject: {subject_id} ({condition}, {group_name})")
+
+    # Save parameters
+    params = {
+        "subject": subject_id,
+        "n_chunks": args.n_chunks,
+        "conditions": args.conditions,
+        "mode": args.mode,
+        "compare_random": args.compare_random,
+        "group": group_name,
+        "fif_path": str(fif_path),
+        "checkpoint_path": str(CHECKPOINT_PATH),
+        "filter_low": FILTER_LOW,
+        "filter_high": FILTER_HIGH,
+        "chunk_duration": CHUNK_DURATION,
+        "sfreq": SFREQ,
+    }
+    save_parameters(output_dir, params)
+
+    # Load model
+    print("\nLoading model...")
+    model_info = load_model_from_checkpoint(CHECKPOINT_PATH, DEVICE)
+
+    # Get n_channels from first file
+    first_data = load_and_preprocess_fif(
+        fif_path, FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+        verbose=False,
+    )
+    n_channels = first_data["n_channels"]
+
+    # Handle --compare-random
+    if args.compare_random:
+        compare_trained_vs_random(
+            fif_path, model_info, n_channels, args.n_chunks,
+            output_dir, not args.no_show
+        )
+        return 0
+
+    # Create model with trained weights
+    model = create_model(n_channels, model_info, DEVICE, load_weights=True)
+
+    # Extract continuous latent
+    print(f"\nExtracting latent trajectory from {args.n_chunks} chunks...")
+    latent, boundaries = extract_continuous_latent(
+        model, fif_path, model_info, args.n_chunks, DEVICE
+    )
+
+    if latent is None:
+        print("No data available!")
+        return 1
+
+    print(f"Latent trajectory shape: {latent.shape}")
+    print(f"Duration: {latent.shape[0] / SFREQ:.1f} seconds")
+
+    title_suffix = f"\n{subject_id} ({group_name})"
+
+    # Generate visualizations based on mode
+    if args.mode in ["all", "trajectory"]:
+        print("\n--- 2D Trajectory ---")
+        plot_trajectory_2d(latent, output_dir, boundaries, title_suffix, not args.no_show)
+
+        print("\n--- 3D Trajectory ---")
+        plot_trajectory_3d(latent, output_dir, boundaries, title_suffix, not args.no_show)
+
+    if args.mode in ["all", "speed"]:
+        print("\n--- Speed/Metastability Analysis ---")
+        speed = plot_speed_trajectory(latent, output_dir, 20, title_suffix, not args.no_show)
+
+    if args.mode in ["all", "density"]:
+        print("\n--- Density Heatmap ---")
+        plot_density_heatmap(latent, output_dir, 50, title_suffix, not args.no_show)
+
+    if args.mode in ["all", "flow"]:
+        print("\n--- Flow Field ---")
+        plot_flow_field(latent, output_dir, 20, title_suffix, not args.no_show)
+
+    # Compute and print summary stats
+    print("\n" + "=" * 60)
+    print("TRAJECTORY STATISTICS")
+    print("=" * 60)
+
+    stats = compute_trajectory_stats(latent)
+    print(f"Mean speed:        {stats['mean_speed']:.4f}")
+    print(f"Speed std:         {stats['speed_std']:.4f}")
+    print(f"Speed CV:          {stats['speed_cv']:.4f}")
+    print(f"Dwell episodes:    {stats['n_dwell_episodes']}")
+    print(f"Total dwell time:  {stats['total_dwell_time']} samples ({stats['total_dwell_time']/SFREQ:.1f}s)")
+    print(f"Mean dwell dur:    {stats['mean_dwell_duration']:.1f} samples")
+    print(f"Path length:       {stats['path_length']:.2f}")
+    print(f"Displacement:      {stats['displacement']:.4f}")
+    print(f"Tortuosity:        {stats['tortuosity']:.2f}")
+    print(f"Explored variance: {stats['explored_variance']:.4f}")
+    print(f"PCA var explained: {[f'{v*100:.1f}%' for v in stats['pca_var_explained']]}")
+
+    print(f"\nAll plots saved to: {output_dir}")
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/scripts/local_analysis/plot_umap_3d.py b/scripts/local_analysis/plot_umap_3d.py
new file mode 100644
index 0000000..2042b8b
--- /dev/null
+++ b/scripts/local_analysis/plot_umap_3d.py
@@ -0,0 +1,595 @@
+#!/usr/bin/env python3
+"""
+3D UMAP Visualization of Latent Trajectories
+
+Compare latent space structure between different groups (HC, MCI, AD).
+Interactive 3D plots for local analysis on M1 Mac.
+
+Usage:
+    python plot_umap_3d.py                           # Default: HC vs MCI comparison
+    python plot_umap_3d.py --conditions HID MCI      # Compare specific conditions
+    python plot_umap_3d.py --conditions HID MCI AD   # Compare all three groups
+    python plot_umap_3d.py --n-subjects 5            # 5 unique subjects per group
+    python plot_umap_3d.py --n-chunks 10             # 10 chunks per subject
+    python plot_umap_3d.py --mode mean               # Only mean latents plot
+    python plot_umap_3d.py --mode subject            # Only per-subject coloring
+    python plot_umap_3d.py --mode trajectory         # Trajectory plot (slow, many points!)
+    python plot_umap_3d.py --mode all                # All plots including trajectory
+    python plot_umap_3d.py --list-subjects           # List available subjects
+"""
+
+import argparse
+import json
+import sys
+from datetime import datetime
+from pathlib import Path
+
+import numpy as np
+import matplotlib.pyplot as plt
+from mpl_toolkits.mplot3d import Axes3D
+import umap
+
+# Add parent to path
+sys.path.insert(0, str(Path(__file__).parent))
+
+from config import (
+    CHECKPOINT_PATH, DATA_DIR, OUTPUT_DIR, DEVICE,
+    FILTER_LOW, FILTER_HIGH, CHUNK_DURATION, SFREQ,
+    UMAP_N_NEIGHBORS, UMAP_MIN_DIST, UMAP_N_COMPONENTS,
+    ensure_output_dir, get_fif_files, get_subjects_by_group, get_label_name
+)
+from load_model import load_model_from_checkpoint, create_model, compute_latent_trajectory
+from load_data import load_and_preprocess_fif
+
+# Color mapping for groups
+GROUP_COLORS = {
+    0: "blue",   # HC
+    1: "orange", # MCI
+    2: "red",    # AD
+}
+
+GROUP_NAMES = {
+    0: "HC",
+    1: "MCI",
+    2: "AD",
+}
+
+
+def create_timestamped_output_dir(base_dir: Path, script_name: str) -> Path:
+    """Create a timestamped output directory for versioned results."""
+    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
+    output_dir = base_dir / f"{script_name}_{timestamp}"
+    output_dir.mkdir(parents=True, exist_ok=True)
+    return output_dir
+
+
+def save_parameters(output_dir: Path, params: dict):
+    """Save parameters to a JSON file for reproducibility."""
+    params_path = output_dir / "parameters.json"
+
+    # Convert Path objects to strings
+    serializable_params = {}
+    for k, v in params.items():
+        if isinstance(v, Path):
+            serializable_params[k] = str(v)
+        else:
+            serializable_params[k] = v
+
+    serializable_params["timestamp"] = datetime.now().isoformat()
+
+    with open(params_path, 'w') as f:
+        json.dump(serializable_params, f, indent=2)
+    print(f"Parameters saved to: {params_path}")
+
+
+def collect_latents_for_groups(
+    model,
+    groups: dict,
+    model_info: dict,
+    n_subjects_per_group: int = 3,
+    n_chunks_per_subject: int = 5,
+):
+    """
+    Collect latent trajectories for multiple groups.
+
+    Args:
+        model: Trained autoencoder
+        groups: Dict with 'hc', 'mci', 'ad' lists of (fif_path, label, condition, subject_id)
+        model_info: Model configuration dict
+        n_subjects_per_group: Number of unique subjects per group
+        n_chunks_per_subject: Number of chunks to use per subject
+
+    Returns:
+        Dict with latents organized by group key
+    """
+    results = {}
+
+    for group_key, subjects in groups.items():
+        if not subjects:
+            continue
+
+        group_name = GROUP_NAMES.get(subjects[0][1], group_key.upper())
+        print(f"\nProcessing {group_name} group (targeting {n_subjects_per_group} subjects)...")
+
+        results[group_key] = {
+            "latents": [],
+            "subject_ids": [],
+            "chunk_ids": [],
+            "label": subjects[0][1] if subjects else 0,
+        }
+
+        # Iterate through subjects until we have enough valid ones
+        subjects_processed = 0
+        for fif_path, label, condition, subject_id in subjects:
+            if subjects_processed >= n_subjects_per_group:
+                break
+
+            print(f"  Processing {subject_id} ({condition})...")
+
+            data = load_and_preprocess_fif(
+                fif_path, FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+                include_amplitude=model_info["include_amplitude"],
+                verbose=False,
+            )
+
+            # Skip subjects with no chunks
+            if len(data["chunks"]) == 0:
+                print(f"    WARNING: No chunks available, skipping subject")
+                continue
+
+            n_chunks = min(n_chunks_per_subject, len(data["chunks"]))
+            print(f"    Using {n_chunks}/{len(data['chunks'])} chunks")
+
+            for chunk_idx in range(n_chunks):
+                phase_data = data["chunks"][chunk_idx]
+                latent = compute_latent_trajectory(model, phase_data, DEVICE)
+
+                results[group_key]["latents"].append(latent)
+                results[group_key]["subject_ids"].append(subject_id)
+                results[group_key]["chunk_ids"].append(chunk_idx)
+
+            subjects_processed += 1
+
+        if subjects_processed < n_subjects_per_group:
+            print(f"  NOTE: Only found {subjects_processed} valid subjects (requested {n_subjects_per_group})")
+
+    return results
+
+
+def plot_umap_mean_latents(results: dict, output_dir: Path, show_plot: bool = True):
+    """
+    Plot UMAP of mean latents (one point per chunk).
+
+    Each chunk is reduced to its mean latent vector, then UMAP is applied.
+    """
+    print("\nComputing mean latents for UMAP...")
+
+    # Compute mean latent per chunk
+    all_means = []
+    all_labels = []
+    all_subject_ids = []
+
+    for group_key, group_data in results.items():
+        label = group_data["label"]
+        for latent, subj_id in zip(group_data["latents"], group_data["subject_ids"]):
+            mean_latent = latent.mean(axis=0)  # (hidden_size,)
+            all_means.append(mean_latent)
+            all_labels.append(label)
+            all_subject_ids.append(subj_id)
+
+    all_means = np.array(all_means)
+    all_labels = np.array(all_labels)
+    unique_labels = sorted(set(all_labels))
+    print(f"  Shape: {all_means.shape} (n_chunks, hidden_size)")
+    print(f"  Groups: {[GROUP_NAMES.get(l, str(l)) for l in unique_labels]}")
+
+    # UMAP
+    print("  Running UMAP...")
+    reducer = umap.UMAP(
+        n_neighbors=UMAP_N_NEIGHBORS,
+        min_dist=UMAP_MIN_DIST,
+        n_components=3,
+        random_state=42,
+    )
+    embedding = reducer.fit_transform(all_means)
+
+    # Plot 3D
+    fig = plt.figure(figsize=(12, 10))
+    ax = fig.add_subplot(111, projection="3d")
+
+    for label_val in unique_labels:
+        mask = all_labels == label_val
+        ax.scatter(
+            embedding[mask, 0],
+            embedding[mask, 1],
+            embedding[mask, 2],
+            c=GROUP_COLORS.get(label_val, "gray"),
+            label=GROUP_NAMES.get(label_val, f"Label {label_val}"),
+            alpha=0.6,
+            s=50,
+        )
+
+    ax.set_xlabel("UMAP 1")
+    ax.set_ylabel("UMAP 2")
+    ax.set_zlabel("UMAP 3")
+
+    group_str = " vs ".join([GROUP_NAMES.get(l, str(l)) for l in unique_labels])
+    ax.set_title(f"3D UMAP of Mean Latents (per chunk)\n{group_str}", fontsize=14)
+    ax.legend()
+
+    save_path = output_dir / "umap_mean_latents_3d.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return embedding, all_labels
+
+
+def plot_umap_trajectories(results: dict, output_dir: Path, show_plot: bool = True):
+    """
+    Plot UMAP of all timepoints, showing temporal trajectories.
+
+    All timepoints from all chunks are projected together.
+    """
+    print("\nComputing UMAP on all timepoints...")
+
+    # Concatenate all latents
+    all_latents = []
+    all_labels = []
+    all_time_indices = []
+    trajectory_boundaries = []  # Track where each chunk starts/ends
+
+    current_idx = 0
+    for group_key, group_data in results.items():
+        label = group_data["label"]
+        for latent in group_data["latents"]:
+            n_time = latent.shape[0]
+            all_latents.append(latent)
+            all_labels.extend([label] * n_time)
+            all_time_indices.extend(range(n_time))
+            trajectory_boundaries.append((current_idx, current_idx + n_time, label))
+            current_idx += n_time
+
+    all_latents = np.vstack(all_latents)
+    all_labels = np.array(all_labels)
+    all_time_indices = np.array(all_time_indices)
+    unique_labels = sorted(set(all_labels))
+    print(f"  Shape: {all_latents.shape} (total_timepoints, hidden_size)")
+
+    # UMAP (subsample if too many points)
+    max_points = 20000
+    if len(all_latents) > max_points:
+        print(f"  Subsampling from {len(all_latents)} to {max_points} points...")
+        idx = np.random.choice(len(all_latents), max_points, replace=False)
+        idx = np.sort(idx)
+        all_latents_sub = all_latents[idx]
+        all_labels_sub = all_labels[idx]
+        all_time_sub = all_time_indices[idx]
+    else:
+        all_latents_sub = all_latents
+        all_labels_sub = all_labels
+        all_time_sub = all_time_indices
+
+    print("  Running UMAP...")
+    reducer = umap.UMAP(
+        n_neighbors=UMAP_N_NEIGHBORS,
+        min_dist=UMAP_MIN_DIST,
+        n_components=3,
+        random_state=42,
+    )
+    embedding = reducer.fit_transform(all_latents_sub)
+
+    # Plot 3D with time coloring
+    fig = plt.figure(figsize=(14, 6))
+
+    # Left: Color by group
+    ax1 = fig.add_subplot(121, projection="3d")
+    for label_val in unique_labels:
+        name = GROUP_NAMES.get(label_val, f"Label {label_val}")
+        mask = all_labels_sub == label_val
+        ax1.scatter(
+            embedding[mask, 0],
+            embedding[mask, 1],
+            embedding[mask, 2],
+            c=GROUP_COLORS.get(label_val, "gray"),
+            label=name,
+            alpha=0.3,
+            s=5,
+        )
+    ax1.set_xlabel("UMAP 1")
+    ax1.set_ylabel("UMAP 2")
+    ax1.set_zlabel("UMAP 3")
+    group_str = " vs ".join([GROUP_NAMES.get(l, str(l)) for l in unique_labels])
+    ax1.set_title(f"Colored by Group ({group_str})")
+    ax1.legend()
+
+    # Right: Color by time within chunk
+    ax2 = fig.add_subplot(122, projection="3d")
+    sc = ax2.scatter(
+        embedding[:, 0],
+        embedding[:, 1],
+        embedding[:, 2],
+        c=all_time_sub,
+        cmap="viridis",
+        alpha=0.3,
+        s=5,
+    )
+    ax2.set_xlabel("UMAP 1")
+    ax2.set_ylabel("UMAP 2")
+    ax2.set_zlabel("UMAP 3")
+    ax2.set_title("Colored by Time (within chunk)")
+    plt.colorbar(sc, ax=ax2, label="Time index")
+
+    plt.suptitle("3D UMAP of Latent Trajectories", fontsize=14)
+    plt.tight_layout()
+
+    save_path = output_dir / "umap_trajectories_3d.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return embedding, all_labels_sub
+
+
+def plot_umap_per_subject(results: dict, output_dir: Path, show_plot: bool = True):
+    """
+    Plot UMAP where each subject has a different color.
+
+    Shows how individual subjects cluster in latent space.
+    """
+    print("\nComputing UMAP colored by subject...")
+
+    all_means = []
+    all_labels = []
+    all_subject_ids = []
+
+    for group_key, group_data in results.items():
+        label = group_data["label"]
+        for latent, subj_id in zip(group_data["latents"], group_data["subject_ids"]):
+            mean_latent = latent.mean(axis=0)
+            all_means.append(mean_latent)
+            all_labels.append(label)
+            all_subject_ids.append(subj_id)
+
+    all_means = np.array(all_means)
+    all_labels = np.array(all_labels)
+    unique_subjects = list(dict.fromkeys(all_subject_ids))  # Preserve order
+    unique_labels = sorted(set(all_labels))
+
+    print("  Running UMAP...")
+    reducer = umap.UMAP(
+        n_neighbors=UMAP_N_NEIGHBORS,
+        min_dist=UMAP_MIN_DIST,
+        n_components=3,
+        random_state=42,
+    )
+    embedding = reducer.fit_transform(all_means)
+
+    # Create color map for subjects
+    n_subjects = len(unique_subjects)
+    cmap = plt.cm.get_cmap("tab20", n_subjects)
+
+    fig = plt.figure(figsize=(14, 6))
+
+    # Left: Color by subject
+    ax1 = fig.add_subplot(121, projection="3d")
+    markers = {0: "o", 1: "^", 2: "s"}  # Circle for HC, triangle for MCI, square for AD
+    for i, subj in enumerate(unique_subjects):
+        mask = np.array([s == subj for s in all_subject_ids])
+        group_label = all_labels[np.where(mask)[0][0]]
+        marker = markers.get(group_label, "o")
+        group_name = GROUP_NAMES.get(group_label, "?")
+        ax1.scatter(
+            embedding[mask, 0],
+            embedding[mask, 1],
+            embedding[mask, 2],
+            c=[cmap(i)],
+            label=f"{subj} ({group_name})",
+            marker=marker,
+            alpha=0.7,
+            s=60,
+        )
+    ax1.set_xlabel("UMAP 1")
+    ax1.set_ylabel("UMAP 2")
+    ax1.set_zlabel("UMAP 3")
+    ax1.set_title("Colored by Subject")
+    ax1.legend(loc="upper left", fontsize=8)
+
+    # Right: Color by group with subject markers
+    ax2 = fig.add_subplot(122, projection="3d")
+    for label_val in unique_labels:
+        name = GROUP_NAMES.get(label_val, f"Label {label_val}")
+        mask = np.array(all_labels) == label_val
+        ax2.scatter(
+            embedding[mask, 0],
+            embedding[mask, 1],
+            embedding[mask, 2],
+            c=GROUP_COLORS.get(label_val, "gray"),
+            label=name,
+            alpha=0.6,
+            s=50,
+        )
+    ax2.set_xlabel("UMAP 1")
+    ax2.set_ylabel("UMAP 2")
+    ax2.set_zlabel("UMAP 3")
+    ax2.set_title("Colored by Group")
+    ax2.legend()
+
+    group_str = " vs ".join([GROUP_NAMES.get(l, str(l)) for l in unique_labels])
+    plt.suptitle(f"UMAP: Subject vs Group Clustering ({group_str})", fontsize=14)
+    plt.tight_layout()
+
+    save_path = output_dir / "umap_per_subject_3d.png"
+    fig.savefig(save_path, dpi=150, bbox_inches="tight")
+    print(f"Saved: {save_path}")
+
+    if show_plot:
+        plt.show()
+    else:
+        plt.close(fig)
+
+    return embedding, unique_subjects
+
+
+def list_subjects(groups: dict):
+    """List available subjects by group."""
+    print("\n" + "=" * 60)
+    print("AVAILABLE SUBJECTS")
+    print("=" * 60)
+
+    for group_key in ["hc", "mci", "ad"]:
+        subjects = groups.get(group_key, [])
+        if not subjects:
+            continue
+
+        group_name = GROUP_NAMES.get(subjects[0][1], group_key.upper())
+        print(f"\n{group_name} subjects ({len(subjects)}):")
+        for i, (fif_path, label, condition, subject_id) in enumerate(subjects):
+            print(f"  {i+1:3d}. {subject_id:10s} ({condition})")
+
+    total = sum(len(groups.get(k, [])) for k in ["hc", "mci", "ad"])
+    print(f"\nTotal: {total} unique subjects")
+
+
+def main():
+    parser = argparse.ArgumentParser(
+        description="3D UMAP visualization of latent space",
+        formatter_class=argparse.RawDescriptionHelpFormatter,
+        epilog="""
+Examples:
+  python plot_umap_3d.py                           # Default: HC vs MCI (no AD)
+  python plot_umap_3d.py --conditions HID MCI      # HC vs MCI only
+  python plot_umap_3d.py --conditions HID MCI AD   # All three groups
+  python plot_umap_3d.py --conditions HID AD       # HC vs AD only
+  python plot_umap_3d.py --n-subjects 5            # 5 unique subjects per group
+  python plot_umap_3d.py --n-chunks 10             # 10 chunks per subject
+  python plot_umap_3d.py --mode mean               # Only mean latent UMAP
+  python plot_umap_3d.py --mode subject            # Only per-subject coloring
+  python plot_umap_3d.py --mode trajectory         # Trajectory UMAP (slow, many points!)
+  python plot_umap_3d.py --mode all                # All plots including trajectory
+  python plot_umap_3d.py --list-subjects           # Show available subjects
+
+Note: 'trajectory' mode projects all timepoints (1000s of points) and is slow.
+      Default mode excludes trajectory for faster execution.
+        """
+    )
+    parser.add_argument("--n-subjects", type=int, default=3,
+                        help="Number of unique subjects per group (default: 3)")
+    parser.add_argument("--n-chunks", type=int, default=5,
+                        help="Number of chunks per subject (default: 5)")
+    parser.add_argument("--conditions", type=str, nargs="+", default=["HID", "MCI"],
+                        help="Conditions to include: HID, MCI, AD (default: HID MCI)")
+    parser.add_argument("--mode", type=str, choices=["all", "default", "mean", "trajectory", "subject"],
+                        default="default", help="Which plots: default (mean+subject), all, mean, trajectory, subject")
+    parser.add_argument("--list-subjects", action="store_true",
+                        help="List available subjects and exit")
+    parser.add_argument("--no-show", action="store_true",
+                        help="Don't display plots interactively")
+    parser.add_argument("--output-dir", type=str, default=None,
+                        help="Custom output directory (default: timestamped folder)")
+    args = parser.parse_args()
+
+    # Create timestamped output directory
+    base_output_dir = ensure_output_dir()
+    if args.output_dir:
+        output_dir = Path(args.output_dir)
+        output_dir.mkdir(parents=True, exist_ok=True)
+    else:
+        output_dir = create_timestamped_output_dir(base_output_dir, "plot_umap_3d")
+
+    # Get subjects by group for selected conditions
+    fif_files = get_fif_files(args.conditions)
+    groups = get_subjects_by_group(fif_files)
+
+    # Print summary
+    group_counts = []
+    for key in ["hc", "mci", "ad"]:
+        if groups.get(key):
+            group_counts.append(f"{len(groups[key])} {key.upper()}")
+    print(f"Found {', '.join(group_counts)} subjects for conditions: {args.conditions}")
+
+    # Handle --list-subjects
+    if args.list_subjects:
+        list_subjects(groups)
+        return 0
+
+    # Check we have at least one group with subjects
+    active_groups = {k: v for k, v in groups.items() if v}
+    if len(active_groups) < 2:
+        print("Need at least 2 groups with subjects for comparison!")
+        print("Available groups:", list(active_groups.keys()))
+        return 1
+
+    # Save parameters
+    params = {
+        "n_subjects": args.n_subjects,
+        "n_chunks": args.n_chunks,
+        "conditions": args.conditions,
+        "mode": args.mode,
+        "checkpoint_path": str(CHECKPOINT_PATH),
+        "filter_low": FILTER_LOW,
+        "filter_high": FILTER_HIGH,
+        "chunk_duration": CHUNK_DURATION,
+        "sfreq": SFREQ,
+        "umap_n_neighbors": UMAP_N_NEIGHBORS,
+        "umap_min_dist": UMAP_MIN_DIST,
+        "umap_n_components": UMAP_N_COMPONENTS,
+        "groups_found": {k: len(v) for k, v in active_groups.items()},
+    }
+    save_parameters(output_dir, params)
+
+    # Load model
+    print("\nLoading model...")
+    model_info = load_model_from_checkpoint(CHECKPOINT_PATH, DEVICE)
+
+    # Create model using first available subject
+    first_subject = list(active_groups.values())[0][0]
+    first_data = load_and_preprocess_fif(
+        first_subject[0], FILTER_LOW, FILTER_HIGH, CHUNK_DURATION,
+        include_amplitude=model_info["include_amplitude"],
+        verbose=False,
+    )
+    model = create_model(first_data["n_channels"], model_info, DEVICE)
+
+    # Collect latents
+    print(f"\nCollecting latents for {args.n_subjects} subjects per group, {args.n_chunks} chunks each...")
+    results = collect_latents_for_groups(
+        model, active_groups, model_info,
+        n_subjects_per_group=args.n_subjects,
+        n_chunks_per_subject=args.n_chunks,
+    )
+
+    # Count unique subjects in results
+    for group_key, group_data in results.items():
+        n_unique = len(set(group_data['subject_ids']))
+        n_chunks = len(group_data['latents'])
+        group_name = GROUP_NAMES.get(group_data['label'], group_key.upper())
+        print(f"  {group_name}: {n_chunks} chunks from {n_unique} subjects")
+
+    # Generate plots
+    # "default" mode = mean + subject (fast, excludes trajectory)
+    # "all" mode = mean + trajectory + subject
+    if args.mode in ["all", "default", "mean"]:
+        plot_umap_mean_latents(results, output_dir, show_plot=not args.no_show)
+
+    if args.mode in ["all", "trajectory"]:
+        print("\nNote: Trajectory mode processes many points, this may take a while...")
+        plot_umap_trajectories(results, output_dir, show_plot=not args.no_show)
+
+    if args.mode in ["all", "default", "subject"]:
+        plot_umap_per_subject(results, output_dir, show_plot=not args.no_show)
+
+    print(f"\nAll plots saved to: {output_dir}")
+    return 0
+
+
+if __name__ == "__main__":
+    sys.exit(main())
diff --git a/src/eeg_biomarkers/data/__init__.py b/src/eeg_biomarkers/data/__init__.py
index 46ef841..67bfd2f 100644
--- a/src/eeg_biomarkers/data/__init__.py
+++ b/src/eeg_biomarkers/data/__init__.py
@@ -5,12 +5,37 @@ from eeg_biomarkers.data.preprocessing import (
     extract_phase_circular,
     chunk_signal,
     load_eeg_file,
+    preprocess_raw,
+)
+from eeg_biomarkers.data.dataset_config import (
+    DatasetConfig,
+    GroupConfig,
+    PreprocessingConfig,
+    GreekRestingConfig,
+    MeditationBIDSConfig,
+    get_dataset_config,
+    register_dataset,
+    list_datasets,
+    dataset_from_hydra_config,
 )
 
 __all__ = [
+    # Dataset classes
     "EEGDataset",
     "EEGDataModule",
+    # Preprocessing
     "extract_phase_circular",
     "chunk_signal",
     "load_eeg_file",
+    "preprocess_raw",
+    # Dataset configuration
+    "DatasetConfig",
+    "GroupConfig",
+    "PreprocessingConfig",
+    "GreekRestingConfig",
+    "MeditationBIDSConfig",
+    "get_dataset_config",
+    "register_dataset",
+    "list_datasets",
+    "dataset_from_hydra_config",
 ]
diff --git a/src/eeg_biomarkers/data/dataset.py b/src/eeg_biomarkers/data/dataset.py
index 94c377a..777f5c5 100644
--- a/src/eeg_biomarkers/data/dataset.py
+++ b/src/eeg_biomarkers/data/dataset.py
@@ -15,16 +15,22 @@ from omegaconf import DictConfig
 from tqdm import tqdm
 
 from eeg_biomarkers.data.preprocessing import (
-    load_eeg_file,
+    preprocess_raw,
     prepare_phase_chunks,
 )
+from eeg_biomarkers.data.dataset_config import (
+    DatasetConfig,
+    dataset_from_hydra_config,
+)
 
 logger = logging.getLogger(__name__)
 
 
 def get_cache_key(cfg: DictConfig) -> str:
-    """Generate a unique cache key based on preprocessing config."""
+    """Generate a unique cache key based on dataset and preprocessing config."""
+    dataset_name = cfg.data.get("dataset", "greek_resting")
     key_parts = [
+        f"ds_{dataset_name}",
         f"filter_{cfg.data.preprocessing.filter_low}_{cfg.data.preprocessing.filter_high}",
         f"ref_{cfg.data.preprocessing.reference}",
         f"notch_{cfg.data.preprocessing.notch_freq}",
@@ -110,7 +116,7 @@ class EEGDataModule:
     Data module for loading and preparing EEG datasets.
 
     Handles:
-    - Loading data from multiple groups/conditions
+    - Loading data from multiple groups/conditions (via DatasetConfig)
     - Preprocessing and phase extraction with disk caching
     - Train/test splitting with proper subject-level separation
     - DataLoader creation
@@ -118,15 +124,27 @@ class EEGDataModule:
     The preprocessing cache ensures that expensive CSD/filtering operations
     are only done once per file. Subsequent runs load fast tensor files.
 
+    Supports multiple datasets through the DatasetConfig abstraction:
+    - Greek resting-state (HC/MCI/AD)
+    - OpenNeuro meditation BIDS (expert/novice)
+    - Custom datasets via register_dataset decorator
+
     Args:
         cfg: Hydra configuration
-        data_dir: Root directory containing group folders
+        data_dir: Root directory containing data
     """
 
     def __init__(self, cfg: DictConfig, data_dir: str | Path):
         self.cfg = cfg
         self.data_dir = Path(data_dir)
 
+        # Load dataset configuration based on cfg.data.dataset
+        self.dataset_config: DatasetConfig = dataset_from_hydra_config(cfg)
+        logger.info(f"Using dataset config: {self.dataset_config.name}")
+
+        # Validate dataset exists
+        self.dataset_config.validate(self.data_dir)
+
         # Cache directory for preprocessed data
         cache_dir = cfg.data.get("caching", {}).get("cache_dir", "preprocessed_cache")
         self.cache_dir = self.data_dir / cache_dir
@@ -156,21 +174,29 @@ class EEGDataModule:
 
     def _preprocess_and_cache_file(
         self,
-        fif_file: Path,
+        data_file: Path,
         cache_path: Path
     ) -> bool:
         """
         Preprocess a single file and save to cache.
 
+        Uses the dataset config to load the raw data (supporting FIF, BDF, etc.)
+        and then applies standard preprocessing pipeline.
+
         Returns True if successful, False otherwise.
         """
         try:
-            raw = load_eeg_file(
-                fif_file,
+            # Load raw data using dataset-specific loader
+            raw = self.dataset_config.load_raw(data_file)
+
+            # Apply preprocessing (filtering, referencing)
+            raw = preprocess_raw(
+                raw,
                 filter_low=self.cfg.data.preprocessing.filter_low,
                 filter_high=self.cfg.data.preprocessing.filter_high,
                 reference=self.cfg.data.preprocessing.reference,
                 notch_freq=self.cfg.data.preprocessing.notch_freq,
+                resample_freq=self.cfg.data.preprocessing.get("resample_freq"),
             )
 
             chunks, mask, info = prepare_phase_chunks(
@@ -196,7 +222,7 @@ class EEGDataModule:
             return True
 
         except Exception as e:
-            logger.warning(f"Failed to preprocess {fif_file.name}: {e}")
+            logger.warning(f"Failed to preprocess {data_file.name}: {e}")
             return False
 
     def _setup_cached_datasets(self) -> None:
@@ -204,43 +230,62 @@ class EEGDataModule:
         Set up datasets with disk caching.
 
         Strategy:
-        1. Discover all subjects and their files
+        1. Discover all subjects and their files (via DatasetConfig)
         2. Split subjects into train/val/test
         3. For each split, preprocess files (if not cached) and create dataset
         """
-        # Step 1: Discover all subjects and their files
+        # Step 1: Discover all subjects and their files using DatasetConfig
         subject_files: dict[str, list[Path]] = {}
         subject_labels: dict[str, int] = {}
 
-        for group_idx, group in enumerate(self.cfg.data.groups):
-            group_name = group.name
-            group_path = self.data_dir / group.path
-            self._label_map[group_name] = group_idx
+        # Get label map from dataset config
+        self._label_map = self.dataset_config.get_label_map()
+
+        for group in self.dataset_config.groups:
+            # Use DatasetConfig to discover files for this group
+            group_files = self.dataset_config.get_files_for_group(self.data_dir, group)
 
-            if not group_path.exists():
-                logger.warning(f"Group path does not exist: {group_path}")
+            if not group_files:
+                logger.warning(f"No files found for group {group.name}")
                 continue
 
-            subject_dirs = sorted([d for d in group_path.iterdir() if d.is_dir()])
-
-            # Sampling
-            n_subjects = self.cfg.data.sampling.n_subjects_per_group
-            if n_subjects is not None and n_subjects < len(subject_dirs):
-                rng = np.random.RandomState(self.cfg.data.sampling.random_seed)
-                indices = rng.permutation(len(subject_dirs))[:n_subjects]
-                subject_dirs = [subject_dirs[i] for i in sorted(indices)]
-            elif n_subjects is not None:
-                subject_dirs = subject_dirs[:n_subjects]
-
-            for subject_dir in subject_dirs:
-                subject_id = subject_dir.name
-                fif_files = [
-                    f for f in subject_dir.rglob("*_eeg.fif")
-                    if not f.name.startswith("._")
+            # Group files by subject using DatasetConfig's subject ID extraction
+            for data_file in group_files:
+                subject_id = self.dataset_config.get_subject_id(data_file)
+
+                if subject_id not in subject_files:
+                    subject_files[subject_id] = []
+                    subject_labels[subject_id] = group.label
+
+                subject_files[subject_id].append(data_file)
+
+            logger.info(f"Group {group.name}: {len(group_files)} files")
+
+        # Apply sampling if requested
+        n_subjects = self.cfg.data.sampling.n_subjects_per_group
+        if n_subjects is not None:
+            # Sample within each group
+            sampled_subject_files = {}
+            sampled_subject_labels = {}
+
+            for group in self.dataset_config.groups:
+                # Get subjects for this group
+                group_subjects = [
+                    s for s, label in subject_labels.items()
+                    if label == group.label
                 ]
-                if fif_files:
-                    subject_files[subject_id] = fif_files
-                    subject_labels[subject_id] = group_idx
+
+                if n_subjects < len(group_subjects):
+                    rng = np.random.RandomState(self.cfg.data.sampling.random_seed)
+                    indices = rng.permutation(len(group_subjects))[:n_subjects]
+                    group_subjects = [group_subjects[i] for i in sorted(indices)]
+
+                for subj in group_subjects:
+                    sampled_subject_files[subj] = subject_files[subj]
+                    sampled_subject_labels[subj] = subject_labels[subj]
+
+            subject_files = sampled_subject_files
+            subject_labels = sampled_subject_labels
 
         all_subjects = list(subject_files.keys())
         total_files = sum(len(files) for files in subject_files.values())
@@ -270,10 +315,14 @@ class EEGDataModule:
         )
 
         # Step 3: Preprocess and cache files, then create datasets
-        def get_cache_path(fif_file: Path) -> Path:
-            """Get cache path for a .fif file."""
+        def get_cache_path(data_file: Path) -> Path:
+            """Get cache path for a data file (FIF, BDF, etc.)."""
             # Use relative path from data_dir to create unique cache path
-            rel_path = fif_file.relative_to(self.data_dir)
+            try:
+                rel_path = data_file.relative_to(self.data_dir)
+            except ValueError:
+                # File is not under data_dir, use absolute path hash
+                rel_path = Path(hashlib.md5(str(data_file).encode()).hexdigest()[:16])
             cache_name = f"{rel_path.stem}_{self.cache_key}.pt"
             return self.cache_dir / rel_path.parent / cache_name
 
@@ -287,12 +336,12 @@ class EEGDataModule:
             # Collect all files for these subjects
             all_files = []
             for subj in subjects:
-                for fif_file in subject_files[subj]:
-                    all_files.append((fif_file, subj, subject_labels[subj]))
+                for data_file in subject_files[subj]:
+                    all_files.append((data_file, subj, subject_labels[subj]))
 
             with tqdm(all_files, desc=f"Preparing {desc}", unit="file") as pbar:
-                for fif_file, subject_id, label in pbar:
-                    cache_path = get_cache_path(fif_file)
+                for data_file, subject_id, label in pbar:
+                    cache_path = get_cache_path(data_file)
                     pbar.set_postfix(subj=subject_id[:10])
 
                     # Check if cached
@@ -308,7 +357,7 @@ class EEGDataModule:
                                 pass  # Will be set during preprocessing if needed
                     else:
                         # Preprocess and cache
-                        if self._preprocess_and_cache_file(fif_file, cache_path):
+                        if self._preprocess_and_cache_file(data_file, cache_path):
                             n_processed += 1
                         else:
                             n_failed += 1
diff --git a/src/eeg_biomarkers/data/preprocessing.py b/src/eeg_biomarkers/data/preprocessing.py
index 364ec2e..e18f21f 100644
--- a/src/eeg_biomarkers/data/preprocessing.py
+++ b/src/eeg_biomarkers/data/preprocessing.py
@@ -10,30 +10,33 @@ import mne
 from scipy.signal import hilbert
 
 
-def load_eeg_file(
-    filepath: str | Path,
+def preprocess_raw(
+    raw: mne.io.Raw,
     filter_low: float = 3.0,
     filter_high: float = 48.0,
     reference: Literal["csd", "average"] = "csd",
     notch_freq: float | None = None,
+    resample_freq: float | None = None,
     verbose: bool = False,
 ) -> mne.io.Raw:
     """
-    Load and preprocess an EEG file.
+    Preprocess an already-loaded MNE Raw object.
 
     Args:
-        filepath: Path to .fif file
+        raw: MNE Raw object (already loaded with preload=True)
         filter_low: Low cutoff frequency (Hz)
         filter_high: High cutoff frequency (Hz)
         reference: Referencing method ("csd" or "average")
         notch_freq: Notch filter frequency for line noise (50/60 Hz), or None
+        resample_freq: Target sampling rate, or None to keep original
         verbose: Whether to print MNE output
 
     Returns:
         Preprocessed Raw object
     """
-    # Load raw data
-    raw = mne.io.read_raw_fif(filepath, preload=True, verbose=verbose)
+    # Resample if requested (do this first to speed up filtering)
+    if resample_freq is not None and raw.info["sfreq"] != resample_freq:
+        raw.resample(resample_freq, verbose=verbose)
 
     # Apply notch filter if specified
     if notch_freq is not None:
@@ -51,6 +54,44 @@ def load_eeg_file(
     return raw
 
 
+def load_eeg_file(
+    filepath: str | Path,
+    filter_low: float = 3.0,
+    filter_high: float = 48.0,
+    reference: Literal["csd", "average"] = "csd",
+    notch_freq: float | None = None,
+    verbose: bool = False,
+) -> mne.io.Raw:
+    """
+    Load and preprocess an EEG file (FIF format).
+
+    This is a convenience function for loading .fif files. For other formats
+    or more control, use preprocess_raw() directly.
+
+    Args:
+        filepath: Path to .fif file
+        filter_low: Low cutoff frequency (Hz)
+        filter_high: High cutoff frequency (Hz)
+        reference: Referencing method ("csd" or "average")
+        notch_freq: Notch filter frequency for line noise (50/60 Hz), or None
+        verbose: Whether to print MNE output
+
+    Returns:
+        Preprocessed Raw object
+    """
+    # Load raw data
+    raw = mne.io.read_raw_fif(filepath, preload=True, verbose=verbose)
+
+    return preprocess_raw(
+        raw,
+        filter_low=filter_low,
+        filter_high=filter_high,
+        reference=reference,
+        notch_freq=notch_freq,
+        verbose=verbose,
+    )
+
+
 def extract_phase_circular(
     signal: np.ndarray,
     include_amplitude: bool = False,
